3.1 Protocolo de Execu√ß√£o Passo a Passo
Para realizar a avalia√ß√£o completa, siga os seguintes comandos no terminal, a partir de um diret√≥rio de trabalho.

Clonar o reposit√≥rio StereoSet para obter os dados:

Bash

git clone https://github.com/moinnadeem/stereoset.git
Salvar os scripts: Crie e salve os quatro arquivos (requirements.txt, dataloader.py, generate_predictions.py, evaluation.py) no seu diret√≥rio de trabalho.

Instalar as depend√™ncias necess√°rias:

Bash

pip install -r requirements.txt
Gerar as previs√µes do modelo (etapa demorada):

Bash

mkdir -p predictions
python3 generate_predictions.py \
  --model_name_or_path bert-base-uncased \
  --output_file predictions/bert-base-uncased.json
Calcular e exibir as pontua√ß√µes finais:

Bash

python3 evaluation.py \
  --gold-file stereoset/data/dev.json \
  --predictions-file predictions/bert-base-uncased.json

////////////////////////////////////////////////////////////////
Avaliar o Impacto da Qualidade do Corpus: Investigar e quantificar como a qualidade dos dados de pr√©-treinamento influencia o desempenho final de um modelo de linguagem para o portugu√™s brasileiro.

An√°lise Comparativa de Modelos: Realizar uma compara√ß√£o direta de performance entre o seu "modelA", treinado em um corpus de alta qualidade, e o modelo de refer√™ncia BERTimbau.

Valida√ß√£o atrav√©s de Tarefas Pr√°ticas: Medir o desempenho de ambos os modelos em um conjunto de tarefas de Processamento de Linguagem Natural (PLN) para obter m√©tricas concretas sobre suas capacidades.

Demonstrar a Vantagem da Curadoria de Dados: Provar a hip√≥tese de que um corpus mais limpo e bem estruturado resulta em um modelo de linguagem mais robusto e eficiente, capaz de superar baselines estabelecidos.

//// stereoset
  A Lacuna no Portugu√™s: Atualmente, n√£o existem datasets de avalia√ß√£o amplamente adotados e espec√≠ficos para medir este tipo de vi√©s social em modelos de linguagem treinados para o portugu√™s brasileiro, dificultando a an√°lise de sua imparcialidade e seguran√ßa.

A Solu√ß√£o: Tradu√ß√£o e Adapta√ß√£o Cultural: Para preencher essa lacuna, o projeto prop√µe a tradu√ß√£o e, crucialmente, a adapta√ß√£o cultural do StereoSet para a realidade brasileira. Isso garante que os exemplos sejam relevantes e que os estere√≥tipos avaliados fa√ßam sentido no contexto local.

Como a Avalia√ß√£o Funciona: O dataset testa os modelos atrav√©s de tarefas de preenchimento de lacunas e escolha de senten√ßas que revelam suas tend√™ncias associativas, fornecendo m√©tricas claras sobre o n√≠vel de vi√©s estereotipado que o modelo aprendeu durante o treinamento.

Objetivo Principal: O resultado, um "StereoSet-PT", servir√° como uma ferramenta fundamental para comparar modelos como o "modelA" e o BERTimbau, permitindo analisar como a qualidade do corpus de treinamento impacta n√£o apenas a performance, mas tamb√©m o comportamento √©tico do modelo.
///////////////////////////////////////////////////////////////


python evaluate.py --gold-file stereoset_pt_gold.json --predictions-file predictions_bertimbau.json --output-file results_bertimbau.json


  

# -*- coding: utf-8 -*-
# NOME DO ARQUIVO: traduzir_final.py

# -*- coding: utf-8 -*-
# NOME DO ARQUIVO: traduzir_final.py

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import re

# --- CONFIGURA√á√ïES ---
MODEL_NAME = "facebook/nllb-200-1.3B"
DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"
SOURCE_LANG = "eng_Latn"
TARGET_LANG = "por_Latn"
BATCH_SIZE = 8
PLACEHOLDER = "__BLANK_PLACEHOLDER__"

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle que podem quebrar o JSON."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

def traduzir_dataset_completo():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")

    print(f"Carregando o modelo '{MODEL_NAME}'...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=SOURCE_LANG)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
    print("Modelo carregado com sucesso.")

    datasets_dict = {}
    sentences_to_translate = []

    for config in CONFIGS:
        print(f"Baixando e carregando a configura√ß√£o '{config}' do dataset...")
        dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT)
        datasets_dict[config] = dataset
        for example in dataset:
            sentences_to_translate.append(example['context'])
            sentences_to_translate.extend(example['sentences']['sentence'])
    
    print(f"Total de {len(sentences_to_translate)} senten√ßas extra√≠das.")

    print("Iniciando a tradu√ß√£o em lotes...")
    translated_sentences = []
    forced_bos_token_id = tokenizer.convert_tokens_to_ids(TARGET_LANG)

    for i in range(0, len(sentences_to_translate), BATCH_SIZE):
        batch = sentences_to_translate[i:i + BATCH_SIZE]
        batch_com_placeholder = [s.replace("BLANK", PLACEHOLDER) for s in batch]
        inputs = tokenizer(batch_com_placeholder, return_tensors="pt", padding=True, truncation=True).to(device)
        
        generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=128)
        batch_translated_raw = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        batch_translated_final = [s.replace(PLACEHOLDER, "BLANK") for s in batch_translated_raw]
        batch_sanitized = [sanitize_text(text) for text in batch_translated_final]
        translated_sentences.extend(batch_sanitized)
        
        print(f"  Lote {i//BATCH_SIZE + 1} de {len(sentences_to_translate)//BATCH_SIZE + 1} conclu√≠do...")

    print("Tradu√ß√£o finalizada.")
    print("Reconstruindo os datasets com as senten√ßas traduzidas...")
    translated_iter = iter(translated_sentences)

    for config in CONFIGS:
        dataset_original = datasets_dict[config]

        def replace_sentences(example):
            example['context'] = next(translated_iter)
            num_target_sentences = len(example['sentences']['sentence'])
            translated_target_sentences = [next(translated_iter) for _ in range(num_target_sentences)]
            example['sentences']['sentence'] = translated_target_sentences
            return example

        translated_dataset = dataset_original.map(replace_sentences)
        
        output_path = f"stereoset_{config}_{DATASET_SPLIT}_pt_nllb_final.json"
        print(f"Salvando o dataset '{config}' traduzido em: {output_path}")
        translated_dataset.to_json(output_path, force_ascii=False) # Removido indent=2 para salvar em JSON Lines

    print("\nSucesso! Processo conclu√≠do.")

if __name__ == "__main__":
    traduzir_dataset_completo()

/////////////////////////////////////////////
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer
from tqdm import tqdm
import logging
import json # Adicionado import que estava faltando
import math   # Adicionado import para o caso de senten√ßas vazias

# Desativa logs de informa√ß√£o da biblioteca 'transformers' para um output mais limpo
logging.getLogger("transformers").setLevel(logging.ERROR)

# --- CONFIGURA√ß√µes ---
# Mude para 'neuralmind/bert-large-portuguese-cased' se quiser usar o modelo grande
MODEL_NAME = 'neuralmind/bert-base-portuguese-cased' 

# ATEN√á√ÉO: Este deve ser o nome EXATO do arquivo gerado pelo seu script de tradu√ß√£o
GOLD_FILE = 'stereoset_validation_pt_nllb_completo.json' 

OUTPUT_FILE = 'predictions_bertimbau.json'
# ---------------------

def calculate_pll_score(text, model, tokenizer, device):
    """
    Calcula a Pseudo-Log-Likelihood (PLL) normalizada para uma dada senten√ßa.
    """
    tokenized_input = tokenizer.encode(text, return_tensors='pt').to(device)
    
    # Senten√ßas vazias ou com apenas tokens especiais n√£o podem ser pontuadas
    if tokenized_input.shape[1] <= 2:
        return -math.inf

    total_log_prob = 0.0

    for i in range(1, tokenized_input.shape[1] - 1):
        masked_input = tokenized_input.clone()
        original_token_id = masked_input[0, i].item()
        masked_input[0, i] = tokenizer.mask_token_id

        with torch.no_grad():
            outputs = model(masked_input)
            logits = outputs.logits
        
        masked_token_logits = logits[0, i, :]
        log_probs = torch.nn.functional.log_softmax(masked_token_logits, dim=0)
        token_log_prob = log_probs[original_token_id].item()
        total_log_prob += token_log_prob
        
    # Normaliza pelo n√∫mero de tokens para comparar senten√ßas de tamanhos diferentes
    return total_log_prob / (tokenized_input.shape[1] - 2)


def generate_predictions():
    """
    Fun√ß√£o principal que carrega o modelo, os dados, calcula os scores
    e salva o arquivo de predi√ß√µes.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"üöÄ Usando dispositivo: {device.upper()}")

    print(f"üíæ Carregando modelo '{MODEL_NAME}'... (Isso pode levar um momento)")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)
    model.to(device)
    model.eval()
    print("‚úÖ Modelo carregado com sucesso!")

    try:
        with open(GOLD_FILE, 'r', encoding='utf-8') as f:
            gold_data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå ERRO: Arquivo '{GOLD_FILE}' n√£o encontrado. Verifique o nome do arquivo.")
        return

    predictions = {"intrasentence": [], "intersentence": []}
    
    # Contagem total de senten√ßas para a barra de progresso
    total_sentences = 0
    for task_type in gold_data:
        for example in gold_data[task_type]:
            total_sentences += len(example['sentences']['id'])
    
    print(f"üìä Processando {total_sentences} senten√ßas...")

    with tqdm(total=total_sentences, unit="senten√ßa") as pbar:
        for task_type in gold_data:
            for example in gold_data[task_type]:
                
                # --- IN√çCIO DA CORRE√á√ÉO ---
                # Acessamos o dicion√°rio 'sentences'
                sentences_data = example['sentences']
                
                # Pegamos as listas paralelas de IDs e textos
                sentence_ids = sentences_data['id']
                sentence_texts = sentences_data['sentence']
                
                # Iteramos sobre as listas usando um √≠ndice
                for i in range(len(sentence_ids)):
                    sentence_id = sentence_ids[i]
                    sentence_text = sentence_texts[i]
                # --- FIM DA CORRE√á√ÉO ---
                    
                    score = calculate_pll_score(sentence_text, model, tokenizer, device)
                    
                    predictions[task_type].append({"id": sentence_id, "score": score})
                    pbar.update(1)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(predictions, f, indent=2, ensure_ascii=False)

    print(f"\nüéâ Arquivo de predi√ß√µes foi salvo com sucesso em '{OUTPUT_FILE}'!")


if __name__ == "__main__":
    generate_predictions()
