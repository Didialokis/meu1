3.1 Protocolo de Execu√ß√£o Passo a Passo
Para realizar a avalia√ß√£o completa, siga os seguintes comandos no terminal, a partir de um diret√≥rio de trabalho.

Clonar o reposit√≥rio StereoSet para obter os dados:

Bash

git clone https://github.com/moinnadeem/stereoset.git
Salvar os scripts: Crie e salve os quatro arquivos (requirements.txt, dataloader.py, generate_predictions.py, evaluation.py) no seu diret√≥rio de trabalho.

Instalar as depend√™ncias necess√°rias:

Bash

pip install -r requirements.txt
Gerar as previs√µes do modelo (etapa demorada):

Bash

mkdir -p predictions
python3 generate_predictions.py \
  --model_name_or_path bert-base-uncased \
  --output_file predictions/bert-base-uncased.json
Calcular e exibir as pontua√ß√µes finais:

Bash

python3 evaluation.py \
  --gold-file stereoset/data/dev.json \
  --predictions-file predictions/bert-base-uncased.json

////////////////////////////////////////////////////////////////



python src/run_evaluation.py \
    --model-name-or-path neuralmind/bert-base-portuguese-cased \
    --input-file /caminho/para/seu/arquivo_intrasentence_traduzido.json \
    --output-file resultados_intrasentence_bertimbau.json \
    --task intrasentence

Script para Unificar os Arquivos:

Salve o c√≥digo abaixo como unificar_json.py, por exemplo.

Coloque seus dois arquivos traduzidos (ex: intersentence_pt.json e intrasentence_pt.json) no mesmo diret√≥rio.

Execute o script. Ele criar√° um novo arquivo chamado stereoset_pt_gold.json.

Python

import json

# Nomes dos seus arquivos traduzidos
intersentence_file = 'intersentence_pt.json'
intrasentence_file = 'intrasentence_pt.json'
output_file = 'stereoset_pt_gold.json'

# Dicion√°rio para armazenar o conte√∫do dos dois arquivos
data_unificada = {}

# Carregar dados de intersentence
try:
    with open(intersentence_file, 'r', encoding='utf-8') as f:
        # O arquivo original tem "intersentence" como chave principal
        data_unificada['intersentence'] = json.load(f)['intersentence']
    print(f"‚úÖ Arquivo '{intersentence_file}' carregado com sucesso.")
except (json.JSONDecodeError, KeyError) as e:
    print(f"‚ùå Erro ao ler '{intersentence_file}'. Verifique se o formato JSON √© v√°lido e cont√©m a chave 'intersentence'. Erro: {e}")
    exit()


# Carregar dados de intrasentence
try:
    with open(intrasentence_file, 'r', encoding='utf-8') as f:
        # O arquivo original tem "intrasentence" como chave principal
        data_unificada['intrasentence'] = json.load(f)['intrasentence']
    print(f"‚úÖ Arquivo '{intrasentence_file}' carregado com sucesso.")
except (json.JSONDecodeError, KeyError) as e:
    print(f"‚ùå Erro ao ler '{intrasentence_file}'. Verifique se o formato JSON √© v√°lido e cont√©m a chave 'intrasentence'. Erro: {e}")
    exit()

# Salvar o novo arquivo unificado
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(data_unificada, f, indent=2, ensure_ascii=False)

print(f"\nüöÄ Arquivos unificados com sucesso em '{output_file}'!")

Agora voc√™ tem o arquivo stereoset_pt_gold.json, que est√° no formato exato que o script de avalia√ß√£o precisa.

üõ†Ô∏è Passo 2: Gerar as Predi√ß√µes (Scores) com o BERTimbau
O script de avalia√ß√£o precisa de um segundo arquivo: o de predi√ß√µes. Este arquivo cont√©m a "pontua√ß√£o" (score) que o BERTimbau atribui a cada frase individualmente. O m√©todo padr√£o para isso √© a pseudo-log-likelihood (PLL), que mede o qu√£o "prov√°vel" uma frase √© de acordo com o modelo.

Voc√™ precisar√° criar um script que:

Carregue o modelo BERTimbau.

Leia o seu arquivo stereoset_pt_gold.json.

Para cada frase (estere√≥tipo, anti-estere√≥tipo e n√£o relacionada) em cada exemplo, calcule seu score usando o BERTimbau.

Salve esses scores em um arquivo JSON com o formato esperado.

O arquivo de predi√ß√µes deve ter a seguinte estrutura:

Exemplo de predictions_bertimbau.json:

JSON

{
    "intrasentence": [
        {
            "id": "8899-7-1",
            "score": -12.345
        },
        {
            "id": "8899-7-2",
            "score": -15.678
        }
    ],
    "intersentence": [
        {
            "id": "9211-2-1",
            "score": -20.111
        },
        {
            "id": "9211-2-2",
            "score": -18.222
        }
    ]
}
O "id" de cada frase vem do seu arquivo gold e o "score" √© a pontua√ß√£o calculada pelo BERTimbau. Um score maior significa que o modelo considera a frase mais prov√°vel.

üöÄ Passo 3: Executar a Avalia√ß√£o
Com os dois arquivos prontos (stereoset_pt_gold.json e predictions_bertimbau.json), voc√™ pode finalmente executar o script de avalia√ß√£o original sem nenhuma modifica√ß√£o.

Abra seu terminal no diret√≥rio onde est√£o os arquivos e execute o seguinte comando:

Bash

python evaluate.py --gold-file stereoset_pt_gold.json --predictions-file predictions_bertimbau.json --output-file results_bertimbau.json
O que cada argumento faz:

--gold-file stereoset_pt_gold.json: Aponta para o seu arquivo de dados traduzido e unificado.

--predictions-file predictions_bertimbau.json: Aponta para o arquivo com os scores gerados pelo BERTimbau.

--output-file results_bertimbau.json: Especifica onde salvar os resultados da avalia√ß√£o.

O script ir√° ent√£o calcular o LM Score, Stereotype Score (SS) e ICAT Score para o BERTimbau com base nos seus dados em portugu√™s, e imprimir√° os resultados no console, al√©m de salv√°-los no arquivo results_bertimbau.json.
class Sentence:
    def __init__(self, id, gold_label):
        self.id = id
        self.gold_label = gold_label
