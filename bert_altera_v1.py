Bash

python eval_discriminative_models.py \
    --pretrained-class "neuralmind/bert-base-portuguese-cased" \
    --tokenizer "BertTokenizer" \
    --intrasentence-model "BertLM" \
    --intersentence-model "BertNextSentence" \
    --input-file "../data/dev_pt.json" \
    --output-file "predictions_bertimbau.json"
Para Avaliar o BERT Multilingual:
O comando √© quase id√™ntico, apenas mudando o nome do modelo e o arquivo de sa√≠da.

Bash

python eval_discriminative_models.py \
    --pretrained-class "bert-base-multilingual-cased" \
    --tokenizer "BertTokenizer" \
    --intrasentence-model "BertLM" \
    --intersentence-model "BertNextSentence" \
    --input-file "../data/dev_pt.json" \
    --output-file "predictions_mbert.json"



        /////////////////////////////////////////////////////////////////////////////
       # -*- coding: utf-8 -*-

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import re
import json
from tqdm import tqdm

# --- 1. CONFIGURA√á√ïES ---

MODEL_NAME = "facebook/nllb-200-1.3B"
DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"
SOURCE_LANG = "eng_Latn"
TARGET_LANG = "por_Latn"
BATCH_SIZE = 8

# MUDAN√áA: Token de placeholder √∫nico que o tradutor ir√° ignorar
PLACEHOLDER = "__BLANK_TOKEN_IGNORE__" 

GOLD_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated'}
INNER_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated', 3: 'related'}

# --- 2. FUN√á√ÉO AUXILIAR ---

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle que podem quebrar o JSON."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

# --- 3. FUN√á√ÉO PRINCIPAL DE TRADU√á√ÉO ---

def traduzir_e_recriar_estrutura_corretamente():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")

    print(f"Carregando o modelo '{MODEL_NAME}'...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=SOURCE_LANG)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
    print("Modelo carregado com sucesso.")

    # --- ETAPA DE EXTRA√á√ÉO (COM SUBSTITUI√á√ÉO GLOBAL DO BLANK) ---
    datasets_dict = {}
    sentences_to_translate = []
    for config in CONFIGS:
        print(f"Carregando a configura√ß√£o '{config}' do dataset...")
        dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT, keep_in_memory=True)
        datasets_dict[config] = dataset
        for example in dataset:
            # MUDAN√áA GLOBAL: Substitui BLANK no contexto, se existir
            if 'context' in example and example['context']:
                context_text = example['context'].replace("BLANK", PLACEHOLDER)
                sentences_to_translate.append(context_text)
            
            # MUDAN√áA GLOBAL: Substitui BLANK nas senten√ßas, se existir
            for sentence_text in example['sentences']['sentence']:
                sentence_text_safe = sentence_text.replace("BLANK", PLACEHOLDER)
                sentences_to_translate.append(sentence_text_safe)
    
    print(f"Total de {len(sentences_to_translate)} senten√ßas extra√≠das para tradu√ß√£o.")

    # --- ETAPA DE TRADU√á√ÉO (SEM MUDAN√áAS) ---
    print("Iniciando a tradu√ß√£o em lotes...")
    translated_sentences = []
    forced_bos_token_id = tokenizer.convert_tokens_to_ids(TARGET_LANG)

    for i in tqdm(range(0, len(sentences_to_translate), BATCH_SIZE), desc="Traduzindo Lotes"):
        batch = sentences_to_translate[i:i + BATCH_SIZE]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(device)
        generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=128)
        batch_translated_raw = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        batch_sanitized = [sanitize_text(text) for text in batch_translated_raw]
        translated_sentences.extend(batch_sanitized)
    print("Tradu√ß√£o finalizada.")

    # --- ETAPA DE RECONSTRU√á√ÉO (COM ORDEM E REVERS√ÉO GLOBAL) ---
    print("Reconstruindo o dataset na estrutura original...")
    translated_iter = iter(translated_sentences)
    
    # O BLANK_PATTERN n√£o √© mais necess√°rio
    
    reconstructed_data = {}
    for config in CONFIGS:
        original_dataset = datasets_dict[config]
        new_examples_list = []
        for original_example in tqdm(original_dataset, desc=f"Reconstruindo {config}"):
            
            # MUDAN√áA ORDEM: Cria o dicion√°rio na ordem solicitada
            new_example = {
                "id": original_example['id'],
                "target": original_example['target'],
                "bias_type": original_example['bias_type']
            }
            
            # Adiciona o contexto (se existir) na ordem correta
            if 'context' in original_example and original_example['context']:
                # MUDAN√áA GLOBAL: Reverte o placeholder
                translated_context = next(translated_iter).replace(PLACEHOLDER, "BLANK")
                new_example["context"] = translated_context
            
            # Prepara a lista de senten√ßas
            new_example["sentences"] = []
            
            original_sents_data = original_example['sentences']
            num_sentences = len(original_sents_data['sentence'])

            for i in range(num_sentences):
                # Recria os labels
                recreated_labels = []
                labels_data_for_one_sentence = original_sents_data['labels'][i]
                human_ids = labels_data_for_one_sentence['human_id']
                inner_int_labels = labels_data_for_one_sentence['label']
                
                for j in range(len(human_ids)):
                    recreated_labels.append({"human_id": human_ids[j], "label": INNER_LABEL_MAP[inner_int_labels[j]]})
                
                # MUDAN√áA GLOBAL: Reverte o placeholder
                translated_sentence = next(translated_iter).replace(PLACEHOLDER, "BLANK")

                # MUDAN√áA ORDEM: Cria o dict da senten√ßa em ordem l√≥gica
                new_sentence_obj = {
                    "id": original_sents_data['id'][i],
                    "sentence": translated_sentence,
                    "gold_label": GOLD_LABEL_MAP[original_sents_data['gold_label'][i]],
                    "labels": recreated_labels
                }
                new_example["sentences"].append(new_sentence_obj)
            
            new_examples_list.append(new_example)
        reconstructed_data[config] = new_examples_list

    # --- ETAPA DE SALVAMENTO ---
    final_output_structure = {
        "version": "1.1",
        "data": {
            "intrasentence": reconstructed_data.get('intrasentence', []),
            "intersentence": reconstructed_data.get('intersentence', [])
        }
    }
    
    output_path = "dev_pt.json"
    print(f"Salvando o dataset final em: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_output_structure, f, ensure_ascii=False, indent=2)

    print("\n‚úÖ Sucesso! O arquivo de sa√≠da agora tem a ordem de chaves correta e preserva 'BLANK' perfeitamente.")


if __name__ == "__main__":
    traduzir_e_recriar_estrutura_corretamente()
//////////////////////////////////////////////////////////////python eval_discriminative_models.py \
Excelente ideia\! Essa √©, de longe, a abordagem mais robusta e elegante para resolver o problema do `"BLANK"`.

Em vez de tentarmos adivinhar o que o tradutor fez com a palavra, vamos control√°-la do in√≠cio ao fim. A sua sugest√£o de usar um placeholder que o tradutor ignora √© a pr√°tica de engenharia de software ideal para esta situa√ß√£o.

Isso nos traz um benef√≠cio enorme: como o arquivo `dev_pt.json` final ter√° a string `"BLANK"` perfeitamente preservada, podemos **reverter o `dataloader.py` para a sua vers√£o 100% original**, garantindo que a metodologia de avalia√ß√£o seja id√™ntica √† do paper do Stereoset, sem nenhuma das nossas modifica√ß√µes.

-----

### A Estrat√©gia Final

1.  **Reverter `dataloader.py` ao Original:** Vamos remover todas as nossas l√≥gicas de `regex` e `difflib`.
2.  **Modificar o Script de Tradu√ß√£o:** Implementaremos sua l√≥gica de placeholder:
      * **Antes de traduzir:** `BLANK` -\> `__PLACEHOLDER__`
      * **Depois de traduzir:** `__PLACEHOLDER__` -\> `BLANK`

-----

### Passo 1: Reverter o `dataloader.py` ao Original

Esta etapa √© **essencial**. Abra o arquivo `/home/sagemaker-user/stereoset/code/dataloader.py` e substitua a fun√ß√£o `__create_intrasentence_examples__` inteira pelo c√≥digo original do reposit√≥rio.

```python
# Dentro da classe StereoSet no arquivo dataloader.py

    def __create_intrasentence_examples__(self, examples):
        created_examples = []
        for example in examples:
            sentences = []
            for sentence in example['sentences']:
                labels = []
                for label in sentence['labels']:
                    labels.append(Label(**label))
                sentence_obj = Sentence(
                    sentence['id'], sentence['sentence'], labels, sentence['gold_label'])
                word_idx = None
                for idx, word in enumerate(example['context'].split(" ")):
                    if "BLANK" in word: 
                        word_idx = idx
                if word_idx is None:
                    raise Exception("No blank word found.")
                template_word = sentence['sentence'].split(" ")[word_idx]
                sentence_obj.template_word = template_word.translate(str.maketrans('', '', string.punctuation))
                sentences.append(sentence_obj)
            created_example = IntrasentenceExample(
                example['id'], example['bias_type'], 
                example['target'], example['context'], sentences) 
            created_examples.append(created_example)
        return created_examples
```

-----

### Passo 2: Usar o Script de Tradu√ß√£o Modificado com Placeholders

Agora, substitua todo o conte√∫do do seu script de tradu√ß√£o pelo c√≥digo abaixo. Ele implementa a sua l√≥gica de forma limpa.

```python
# -*- coding: utf-8 -*-

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import re
import json
from tqdm import tqdm

# --- 1. CONFIGURA√á√ïES ---

MODEL_NAME = "facebook/nllb-200-1.3B"
DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"
SOURCE_LANG = "eng_Latn"
TARGET_LANG = "por_Latn"
BATCH_SIZE = 8
PLACEHOLDER = "__BLANK_TOKEN__" # Um token √∫nico que o tradutor provavelmente ir√° ignorar

GOLD_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated'}
INNER_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated', 3: 'related'}

# --- 2. FUN√á√ÉO AUXILIAR ---

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle que podem quebrar o JSON."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

# --- 3. FUN√á√ÉO PRINCIPAL DE TRADU√á√ÉO ---

def traduzir_e_recriar_estrutura_corretamente():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")

    print(f"Carregando o modelo '{MODEL_NAME}'...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=SOURCE_LANG)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
    print("Modelo carregado com sucesso.")

    # --- ETAPA DE EXTRA√á√ÉO (COM SUBSTITUI√á√ÉO PARA PLACEHOLDER) ---
    datasets_dict = {}
    sentences_to_translate = []
    for config in CONFIGS:
        print(f"Carregando a configura√ß√£o '{config}' do dataset...")
        dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT, keep_in_memory=True)
        datasets_dict[config] = dataset
        for example in dataset:
            if 'context' in example and example['context']:
                context_text = example['context']
                # <--- MUDAN√áA 1: SUBSTITUI O "BLANK" PELO PLACEHOLDER ANTES DA TRADU√á√ÉO --->
                if config == 'intrasentence':
                    context_text = context_text.replace("BLANK", PLACEHOLDER)
                sentences_to_translate.append(context_text)
            sentences_to_translate.extend(example['sentences']['sentence'])
    
    print(f"Total de {len(sentences_to_translate)} senten√ßas extra√≠das para tradu√ß√£o.")

    # --- ETAPA DE TRADU√á√ÉO (SEM MUDAN√áAS) ---
    print("Iniciando a tradu√ß√£o em lotes...")
    translated_sentences = []
    forced_bos_token_id = tokenizer.convert_tokens_to_ids(TARGET_LANG)

    for i in tqdm(range(0, len(sentences_to_translate), BATCH_SIZE), desc="Traduzindo Lotes"):
        batch = sentences_to_translate[i:i + BATCH_SIZE]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(device)
        generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=128)
        batch_translated_raw = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        batch_sanitized = [sanitize_text(text) for text in batch_translated_raw]
        translated_sentences.extend(batch_sanitized)
    print("Tradu√ß√£o finalizada.")

    # --- ETAPA DE RECONSTRU√á√ÉO (COM RESTAURA√á√ÉO DO "BLANK") ---
    print("Reconstruindo o dataset na estrutura original...")
    translated_iter = iter(translated_sentences)
    
    reconstructed_data = {}
    for config in CONFIGS:
        original_dataset = datasets_dict[config]
        new_examples_list = []
        for original_example in tqdm(original_dataset, desc=f"Reconstruindo {config}"):
            new_example = {
                "id": original_example['id'],
                "bias_type": original_example['bias_type'],
                "target": original_example['target'],
                "sentences": []
            }
            
            if 'context' in original_example and original_example['context']:
                translated_context = next(translated_iter)
                # <--- MUDAN√áA 2: SUBSTITUI O PLACEHOLDER DE VOLTA PARA O "BLANK" ORIGINAL --->
                if config == 'intrasentence':
                    translated_context = translated_context.replace(PLACEHOLDER, "BLANK")
                new_example["context"] = translated_context
            
            original_sents_data = original_example['sentences']
            num_sentences = len(original_sents_data['sentence'])

            for i in range(num_sentences):
                # ... (o resto da l√≥gica de reconstru√ß√£o de labels e senten√ßas permanece igual)
                recreated_labels = []
                labels_data_for_one_sentence = original_sents_data['labels'][i]
                human_ids = labels_data_for_one_sentence['human_id']
                inner_int_labels = labels_data_for_one_sentence['label']
                
                for j in range(len(human_ids)):
                    recreated_labels.append({"human_id": human_ids[j], "label": INNER_LABEL_MAP[inner_int_labels[j]]})

                new_sentence_obj = {
                    "id": original_sents_data['id'][i],
                    "sentence": next(translated_iter),
                    "labels": recreated_labels,
                    "gold_label": GOLD_LABEL_MAP[original_sents_data['gold_label'][i]]
                }
                new_example["sentences"].append(new_sentence_obj)
            
            new_examples_list.append(new_example)
        reconstructed_data[config] = new_examples_list

    # --- ETAPA DE SALVAMENTO ---
    final_output_structure = {"version": "1.1", "data": {"intrasentence": reconstructed_data.get('intrasentence', []), "intersentence": reconstructed_data.get('intersentence', [])}}
    output_path = "dev_pt.json"
    print(f"Salvando o dataset final em: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_output_structure, f, ensure_ascii=False, indent=2)
    print("\n‚úÖ Sucesso! O arquivo de sa√≠da agora √© 100% compat√≠vel com a estrutura original do Stereoset.")

if __name__ == "__main__":
    traduzir_e_recriar_estrutura_corretamente()
```

-----

### Passo 3: Workflow Completo e Final

Siga estes passos para garantir uma execu√ß√£o limpa do in√≠cio ao fim.

1.  **Exclua os Arquivos Antigos:**

    ```bash
    # Exclui o JSON traduzido anteriormente, se houver
    rm dev_pt.json

    # Exclui todas as previs√µes geradas anteriormente
    rm -rf predictions/*
    ```

2.  **Execute o Novo Script de Tradu√ß√£o:** Ele ir√° gerar o arquivo `dev_pt.json` no formato perfeito.

    ```bash
    python seu_script_de_traducao.py 
    ```

3.  **Execute a Gera√ß√£o de Previs√µes:** Agora com o `dataloader.py` original, ele ler√° o `dev_pt.json` sem nenhum problema.

    ```bash
    # Exemplo para o BERTimbau
    python eval_discriminative_models.py \
       --pretrained-class "neuralmind/bert-base-portuguese-cased" \
       --input-file "dev_pt.json" \
       --output-file "predictions/predictions_bertimbau.json"
    ```

4.  **Execute a Avalia√ß√£o Final:**

    ```bash
    python3 evaluation.py --gold-file dev_pt.json --predictions-dir predictions/
    ```

Esta abordagem √©, sem d√∫vida, a mais correta e robusta. Ela garante que a integridade estrutural do dataset seja mantida, permitindo que os scripts de avalia√ß√£o originais funcionem sem modifica√ß√µes, tornando seus resultados diretamente compar√°veis e metodologicamente s√≥lidos.
    
//////////////////////////////////////////////////////////////

def __create_intrasentence_examples__(self, examples):
        created_examples = []
        # Loop externo: itera sobre cada CLUSTER de senten√ßas
        for example in examples:
            sentences = []
            
            # Loop interno: itera sobre cada SENTEN√áA dentro do cluster
            for sentence in example['sentences']:
                # Cria o objeto Senten√ßa b√°sico
                labels = [Label(**label) for label in sentence['labels']]
                sentence_obj = Sentence(
                    sentence['id'], sentence['sentence'], labels, sentence['gold_label'])

                # --- IN√çCIO DA L√ìGICA CORRIGIDA COM DIFF ---
                # Limpa e tokeniza a frase de contexto, removendo a palavra BLANK
                context_tokens = [w for w in example['context'].lower().split() if 'blank' not in w]
                
                # Limpa e tokeniza a frase completa (usando o dict 'sentence' do loop interno)
                sentence_tokens = sentence['sentence'].lower().split()

                # Usa o SequenceMatcher para encontrar a maior sequ√™ncia de palavras em comum
                matcher = SequenceMatcher(None, context_tokens, sentence_tokens)
                
                # A "palavra-alvo" √© composta por todas as palavras da senten√ßa que N√ÉO fazem parte do bloco comum
                diff_words = []
                for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                    if tag != 'equal': # Captura inser√ß√µes ('insert') e substitui√ß√µes ('replace')
                        # Pega as palavras com a capitaliza√ß√£o original da senten√ßa correta
                        diff_words.extend(sentence['sentence'].split()[j1:j2]) 

                if diff_words:
                    template_word = " ".join(diff_words)
                    sentence_obj.template_word = template_word.translate(str.maketrans('', '', string.punctuation))
                    sentences.append(sentence_obj)
                else:
                    # Se ainda assim falhar, imprime o aviso (agora muito mais raro)
                    print(f"AVISO: N√£o foi poss√≠vel encontrar a diferen√ßa para o ID {sentence['id']}.")
                    print(f"  Contexto: {example['context']}")
                    print(f"  Senten√ßa: {sentence['sentence']}")
                # --- FIM DA L√ìGICA CORRIGIDA ---

            # Cria o objeto Exemplo com a lista de senten√ßas processadas
            created_example = IntrasentenceExample(
                example['id'], example['bias_type'], 
                example['target'], example['context'], sentences) 
            created_examples.append(created_example)
            
        return created_examples

///////////////////////////////////////////////////

        Com certeza\! Analisando o seu traceback, o problema fica bem claro. A solu√ß√£o √© **modificar o arquivo `dataloader.py`** para torn√°-lo mais robusto √† tradu√ß√£o.

-----

### Diagn√≥stico do Erro üí°

O erro `IndexError: list index out of range` acontece na linha:
`template_word = sentence['sentence'].split(" ")[word_idx]`

O problema √© um pressuposto fr√°gil no c√≥digo original do Stereoset:

1.  O script primeiro encontra o √≠ndice da palavra `"BLANK"` na frase de contexto (ex: "Meu amigo √© um BLANK."). Vamos dizer que o √≠ndice (`word_idx`) seja `4`.
2.  Em seguida, ele assume que a palavra-alvo (ex: "cientista") estar√° **exatamente no mesmo √≠ndice** na frase preenchida (ex: "Meu amigo √© cientista.").
3.  **A tradu√ß√£o quebra isso.** Em portugu√™s, a frase "Meu amigo √© cientista" tem apenas 4 palavras (√≠ndices 0 a 3). Quando o c√≥digo tenta acessar o √≠ndice `4`, a lista √© menor que o esperado, causando o erro `IndexError`.

Tentar consertar isso no script de tradu√ß√£o √© invi√°vel. A solu√ß√£o correta √© tornar o `dataloader.py` mais inteligente.

-----

### A Solu√ß√£o: Modificar o `dataloader.py`

Vamos alterar a l√≥gica para que, em vez de depender de um √≠ndice, ele encontre a palavra-alvo descobrindo qual palavra √© a **diferen√ßa** entre a frase de contexto e a frase preenchida.

#### Passo 1: Abra o arquivo `dataloader.py`

Navegue at√© o arquivo `/home/sagemaker-user/stereoset/code/dataloader.py`.

#### Passo 2: Localize a fun√ß√£o `__create_intrasentence_examples__`

Dentro da classe `StereoSet`, encontre esta fun√ß√£o.

#### Passo 3: Substitua a l√≥gica de busca por √≠ndice

Voc√™ substituir√° um bloco de c√≥digo dentro do loop `for sentence in example['sentences']:` por uma vers√£o mais robusta.

**SUBSTITUA ESTE BLOCO DE C√ìDIGO ORIGINAL:**

```python
                word_idx = None
                for idx, word in enumerate(example['context'].split(" ")):
                    if "BLANK" in word: 
                        word_idx = idx
                if word_idx is None:
                    raise Exception("No blank word found.")
                template_word = sentence['sentence'].split(" ")[word_idx]
                sentence_obj.template_word = template_word.translate(str.maketrans('', '', string.punctuation))
                sentences.append(sentence_obj)
```

**POR ESTE NOVO BLOCO DE C√ìDIGO ROBUSTO:**

```python
                # --- IN√çCIO DA NOVA L√ìGICA ---
                # Remove a pontua√ß√£o e divide as frases em conjuntos de palavras em min√∫sculas
                context_words = set(example['context'].replace("BLANK", "").lower().translate(str.maketrans('', '', string.punctuation)).split())
                sentence_words = set(sentence['sentence'].lower().translate(str.maketrans('', '', string.punctuation)).split())

                # A palavra-alvo √© a que est√° no conjunto da senten√ßa, mas n√£o no do contexto
                difference = sentence_words.difference(context_words)

                # Verifica se encontrou exatamente uma palavra de diferen√ßa
                if len(difference) == 1:
                    template_word = difference.pop()
                    sentence_obj.template_word = template_word
                    sentences.append(sentence_obj)
                else:
                    # Se a l√≥gica falhar para um exemplo, imprime um aviso em vez de quebrar a execu√ß√£o
                    print(f"AVISO: N√£o foi poss√≠vel encontrar uma √∫nica palavra de diferen√ßa para o ID {sentence['id']}.")
                    print(f"  Contexto: {example['context']}")
                    print(f"  Senten√ßa: {sentence['sentence']}")
                    # Isso permite que o script continue com os outros exemplos
```

-----

### Pr√≥ximos Passos ‚úÖ

1.  **Aplique a altera√ß√£o** no seu arquivo `dataloader.py`.
2.  **N√£o √© necess√°rio** gerar novamente o arquivo `dev_pt.json`. O problema estava na leitura do arquivo, n√£o no arquivo em si.
3.  **Execute o script `eval_discriminative_models.py` novamente.**

O erro `IndexError` ser√° resolvido, pois o programa n√£o depende mais da fr√°gil suposi√ß√£o de que a contagem de palavras permanece a mesma ap√≥s a tradu√ß√£o.
        
///////////////////////////////////////////////////
# -*- coding: utf-8 -*-

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import re
import json
from tqdm import tqdm

# --- 1. CONFIGURA√á√ïES ---

MODEL_NAME = "facebook/nllb-200-1.3B"
DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"
SOURCE_LANG = "eng_Latn"
TARGET_LANG = "por_Latn"
BATCH_SIZE = 8

GOLD_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated'}
INNER_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated', 3: 'related'}

# --- 2. FUN√á√ÉO AUXILIAR ---

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle que podem quebrar o JSON."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

# --- 3. FUN√á√ÉO PRINCIPAL DE TRADU√á√ÉO ---

def traduzir_e_recriar_estrutura_corretamente():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Usando dispositivo: {device}")

    print(f"Carregando o modelo '{MODEL_NAME}'...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=SOURCE_LANG)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)
    print("Modelo carregado com sucesso.")

    # --- ETAPA DE EXTRA√á√ÉO (SEM MUDAN√áAS) ---
    datasets_dict = {}
    sentences_to_translate = []
    for config in CONFIGS:
        print(f"Carregando a configura√ß√£o '{config}' do dataset...")
        dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT, keep_in_memory=True)
        datasets_dict[config] = dataset
        for example in dataset:
            if 'context' in example and example['context']:
                sentences_to_translate.append(example['context'])
            sentences_to_translate.extend(example['sentences']['sentence'])
    
    print(f"Total de {len(sentences_to_translate)} senten√ßas extra√≠das para tradu√ß√£o.")

    # --- ETAPA DE TRADU√á√ÉO (SEM MUDAN√áAS) ---
    print("Iniciando a tradu√ß√£o em lotes...")
    translated_sentences = []
    forced_bos_token_id = tokenizer.convert_tokens_to_ids(TARGET_LANG)

    for i in tqdm(range(0, len(sentences_to_translate), BATCH_SIZE), desc="Traduzindo Lotes"):
        batch = sentences_to_translate[i:i + BATCH_SIZE]
        inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True).to(device)
        generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=128)
        batch_translated_raw = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        batch_sanitized = [sanitize_text(text) for text in batch_translated_raw]
        translated_sentences.extend(batch_sanitized)
    print("Tradu√ß√£o finalizada.")

    # --- ETAPA DE RECONSTRU√á√ÉO (L√ìGICA FINAL) ---
    print("Reconstruindo o dataset na estrutura original...")
    translated_iter = iter(translated_sentences)
    
    # MUDAN√áA PRINCIPAL: Padr√£o Regex para encontrar e padronizar "BLANK"
    # \b garante que estamos pegando a palavra inteira. Cobre "branco", "branca", "blanco", "em branco", etc.
    BLANK_PATTERN = re.compile(r'\b(branco|branca|blanco|blanca|em branco|lacuna)\b', re.IGNORECASE)

    reconstructed_data = {}
    for config in CONFIGS:
        original_dataset = datasets_dict[config]
        new_examples_list = []
        for original_example in tqdm(original_dataset, desc=f"Reconstruindo {config}"):
            new_example = {
                "id": original_example['id'],
                "bias_type": original_example['bias_type'],
                "target": original_example['target'],
                "sentences": []
            }
            
            # Garante que o contexto seja preservado para ambos os tipos
            if 'context' in original_example and original_example['context']:
                translated_context = next(translated_iter)
                # Se for um exemplo intrasentence, padroniza a tradu√ß√£o de "BLANK" de volta para o original.
                if config == 'intrasentence':
                    translated_context = BLANK_PATTERN.sub("BLANK", translated_context)
                new_example["context"] = translated_context
            
            original_sents_data = original_example['sentences']
            num_sentences = len(original_sents_data['sentence'])

            for i in range(num_sentences):
                recreated_labels = []
                labels_data_for_one_sentence = original_sents_data['labels'][i]
                human_ids = labels_data_for_one_sentence['human_id']
                inner_int_labels = labels_data_for_one_sentence['label']
                
                for j in range(len(human_ids)):
                    recreated_labels.append({
                        "human_id": human_ids[j],
                        "label": INNER_LABEL_MAP[inner_int_labels[j]]
                    })

                new_sentence_obj = {
                    "id": original_sents_data['id'][i],
                    "sentence": next(translated_iter),
                    "labels": recreated_labels,
                    "gold_label": GOLD_LABEL_MAP[original_sents_data['gold_label'][i]]
                }
                new_example["sentences"].append(new_sentence_obj)
            
            new_examples_list.append(new_example)
        reconstructed_data[config] = new_examples_list

    # --- ETAPA DE SALVAMENTO ---
    final_output_structure = {
        "version": "1.1",
        "data": {
            "intrasentence": reconstructed_data.get('intrasentence', []),
            "intersentence": reconstructed_data.get('intersentence', [])
        }
    }
    
    output_path = "dev_pt.json"
    print(f"Salvando o dataset final em: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_output_structure, f, ensure_ascii=False, indent=2)

    print("\n‚úÖ Sucesso! O arquivo de sa√≠da agora √© 100% compat√≠vel com a estrutura original do Stereoset.")


if __name__ == "__main__":
    traduzir_e_recriar_estrutura_corretamente()


    /
    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


def __create_intrasentence_examples__(self, examples):
        created_examples = []
        # Loop externo: itera sobre cada CLUSTER de senten√ßas
        for example in examples:
            sentences = []
            
            # Loop interno: itera sobre cada SENTEN√áA dentro do cluster
            for sentence in example['sentences']:
                # Cria o objeto Senten√ßa b√°sico
                labels = [Label(**label) for label in sentence['labels']]
                sentence_obj = Sentence(
                    sentence['id'], sentence['sentence'], labels, sentence['gold_label'])

                # --- IN√çCIO DA L√ìGICA FINAL (CORRIGIDA PARA INDEXERROR) ---
                
                # 1. Tokeniza o contexto (em min√∫sculas)
                context_tokens = [w for w in example['context'].lower().split() if 'blank' not in w]
                
                # 2. Tokeniza a senten√ßa, MAS MANT√âM A CAPITALIZA√á√ÉO ORIGINAL
                original_sentence_tokens = sentence['sentence'].split()
                
                # 3. Cria a vers√£o em min√∫sculas PARA COMPARA√á√ÉO
                sentence_tokens_lower = [w.lower() for w in original_sentence_tokens]

                # 4. Compara o contexto (min√∫sculo) com a senten√ßa (min√∫scula)
                matcher = SequenceMatcher(None, context_tokens, sentence_tokens_lower)
                
                diff_words = []
                for tag, i1, i2, j1, j2 in matcher.get_opcodes():
                    if tag != 'equal':
                        # 5. Usa os √≠ndices da compara√ß√£o para extrair da LISTA ORIGINAL (com capitaliza√ß√£o)
                        #    Isso agora √© seguro, pois original_sentence_tokens e sentence_tokens_lower
                        #    t√™m o mesmo comprimento garantido.
                        diff_words.extend(original_sentence_tokens[j1:j2]) 

                if diff_words:
                    template_word = " ".join(diff_words)
                    sentence_obj.template_word = template_word.translate(str.maketrans('', '', string.punctuation))
                    sentences.append(sentence_obj)
                else:
                    # Se ainda assim falhar, imprime o aviso
                    print(f"AVISO: N√£o foi poss√≠vel encontrar a diferen√ßa para o ID {sentence['id']}.")
                    print(f"  Contexto: {example['context']}")
                    print(f"  Senten√ßa: {sentence['sentence']}")
                # --- FIM DA L√ìGICA FINAL ---

            # Cria o objeto Exemplo com a lista de senten√ßas processadas
            created_example = IntrasentenceExample(
                example['id'], example['bias_type'], 
                example['target'], example['context'], sentences) 
            created_examples.append(created_example)
            
        return created_examples
