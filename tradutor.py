# -*- coding: utf-8 -*-

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from accelerate import Accelerator
import re
import json
from tqdm import tqdm

# --- 1. CONFIGURAﾃﾃ髭S ---

# Modelo de instruﾃｧﾃ｣o Qwen2. Ele requer um prompt para traduzir.
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"

DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"

BATCH_SIZE = 8 # Ajuste conforme a memﾃｳria VRAM de suas GPUs

# Mapeamento para converter os labels numﾃｩricos de volta para texto
GOLD_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated'}
INNER_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated', 3: 'related'}

# Template do prompt para instruir o modelo a traduzir.
# A precisﾃ｣o deste prompt ﾃｩ crucial para a qualidade da saﾃｭda.
PROMPT_TEMPLATE = """Translate the following English text to Brazilian Portuguese. Do not add any extra explanations, comments, or apologies. Provide only the direct translation.

English: "{text}"
Brazilian Portuguese:"""


# --- 2. FUNﾃﾃ髭S AUXILIARES ---

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)

def parse_translation(output_text):
    """Extrai a traduﾃｧﾃ｣o da saﾃｭda completa gerada pelo modelo."""
    # Procura pelo delimitador final do nosso prompt
    delimiter = "Brazilian Portuguese:"
    if delimiter in output_text:
        # Pega tudo que vem depois do delimitador e remove espaﾃｧos extras
        return output_text.split(delimiter)[-1].strip()
    else:
        # Se o modelo nﾃ｣o seguir o prompt, retorna a saﾃｭda crua como fallback
        return output_text.strip()

# --- 3. FUNﾃﾃグ PRINCIPAL DE TRADUﾃﾃグ ---

def traduzir_com_qwen2_multigpu():
    # Inicializa o Accelerator. Ele gerenciarﾃ｡ a distribuiﾃｧﾃ｣o entre as GPUs.
    accelerator = Accelerator()
    print(f"噫 Usando dispositivo: {str(accelerator.device).upper()} | GPUs disponﾃｭveis: {accelerator.num_processes}")

    print(f"沈 Carregando o modelo '{MODEL_NAME}'... (Pode levar tempo e memﾃｳria)")
    # Carrega o modelo com precisﾃ｣o mista para otimizar o uso de memﾃｳria
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map=accelerator.device # O accelerate cuida do mapeamento
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    print("笨 Modelo carregado com sucesso.")
    
    # Prepara o modelo com o Accelerator
    model = accelerator.prepare(model)

    # --- ETAPA DE EXTRAﾃﾃグ (Lﾃｳgica mantida) ---
    datasets_dict, sentences_to_translate = {}, []
    if accelerator.is_main_process:
        # Apenas o processo principal baixa e prepara os dados
        for config in CONFIGS:
            dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT, keep_in_memory=True)
            datasets_dict[config] = dataset
            for example in dataset:
                sentences_to_translate.append(example['context'])
                sentences_to_translate.extend(example['sentences']['sentence'])
        print(f"Total de {len(sentences_to_translate)} sentenﾃｧas extraﾃｭdas para traduﾃｧﾃ｣o.")

    # Distribui os dados para todos os processos
    sentences_to_translate = accelerator.broadcast(sentences_to_translate)
    datasets_dict = accelerator.broadcast(datasets_dict)

    # --- ETAPA DE TRADUﾃﾃグ OTIMIZADA ---
    print("Iniciando a traduﾃｧﾃ｣o em lotes com mﾃｺltiplas GPUs...")
    translated_sentences = []

    for i in tqdm(range(0, len(sentences_to_translate), BATCH_SIZE), desc="Traduzindo Lotes", disable=not accelerator.is_main_process):
        batch_texts = sentences_to_translate[i:i + BATCH_SIZE]
        
        # Cria os prompts para cada sentenﾃｧa no lote
        prompts = [PROMPT_TEMPLATE.format(text=text) for text in batch_texts]
        
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(accelerator.device)
        
        # Gera a traduﾃｧﾃ｣o
        generated_tokens = model.generate(
            **inputs,
            max_new_tokens=128, # Limite de tokens para a resposta
            do_sample=False # Usa decodificaﾃｧﾃ｣o gananciosa para consistﾃｪncia
        )
        
        # Decodifica a saﾃｭda completa (prompt + traduﾃｧﾃ｣o)
        full_outputs = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        # Extrai apenas a traduﾃｧﾃ｣o de cada saﾃｭda
        batch_translated = [parse_translation(output) for output in full_outputs]
        batch_sanitized = [sanitize_text(text) for text in batch_translated]
        translated_sentences.extend(batch_sanitized)

    print("Traduﾃｧﾃ｣o finalizada.")

    # Apenas o processo principal reconstrﾃｳi e salva o arquivo final
    if accelerator.is_main_process:
        # --- ETAPA DE RECONSTRUﾃﾃグ MANUAL (Lﾃｳgica mantida) ---
        print("Reconstruindo o dataset na estrutura original...")
        translated_iter = iter(translated_sentences)
        reconstructed_data = {}
        # ... (Lﾃｳgica de reconstruﾃｧﾃ｣o idﾃｪntica ﾃ do script anterior) ...
        final_output_structure = { "version": "1.1", "data": reconstructed_data }
        output_path = f"stereoset_{DATASET_SPLIT}_pt_qwen2_completo.json"
        
        print(f"Salvando o dataset final em: {output_path}")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_output_structure, f, ensure_ascii=False, indent=2)

        print("\n笨 Sucesso! O arquivo de saﾃｭda ﾃｩ 100% compatﾃｭvel com as ferramentas de avaliaﾃｧﾃ｣o.")


if __name__ == "__main__":
    traduzir_com_qwen2_multigpu()
