# ... (início do script de setup do SageMaker) ...

hyperparameters = {
    # ... (outras configs gerais e de dados como max_len, vocab_size, etc.) ...
    'sagemaker_input_data_dir': f'/opt/ml/input/data/{INPUT_DATA_CHANNEL_NAME}/', # Se usando S3 input
    'input_data_filename': os.path.basename(train_data_s3_path),           # Se usando S3 input
    # 'aroeira_subset_size': 10000, # Se baixando do Hub e usando subset

    # Configs Pré-treinamento MLM com novos controles
    'max_epochs_pretrain': 40,      # <<< NÚMERO MÁXIMO DE ÉPOCAS >>>
    'target_loss_pretrain': 2.5,    # <<< LOSS OBJETIVO (exemplo, None para desabilitar) >>>
    'batch_size_pretrain': 8,
    'lr_pretrain': 5e-5,

    # ... (configs de arquitetura do modelo, nomes de arquivos, flags do_...) ...
    # Certifique-se que 'do_pretrain': "True" e outras flags do_* estão como desejado
}

# ... (resto do script de setup com HuggingFaceEstimator e .fit()) ...

/////////////////////////////////////////////////////////////////////////////////////////////////

# train_pipeline.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import datasets
import tokenizers
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
from tqdm.auto import tqdm
import random
from pathlib import Path
import sys
import math
import argparse
import os

print(f"PyTorch: {torch.__version__}")
print(f"Datasets: {datasets.__version__}")
print(f"Tokenizers: {tokenizers.__version__}")

# --- Definições de Classes ---

class BERTMLMDataset(Dataset):
    """Dataset para Masked Language Modeling."""
    def __init__(self, sentences_or_hf_dataset, tokenizer_instance, max_len_config, pad_token_id_config):
        self.tokenizer = tokenizer_instance
        self.max_len = max_len_config
        self.vocab_size = self.tokenizer.get_vocab_size()
        self.mask_id = self.tokenizer.token_to_id("<mask>")
        self.cls_id = self.tokenizer.token_to_id("<s>")
        self.sep_id = self.tokenizer.token_to_id("</s>")
        self.pad_id = pad_token_id_config

        if isinstance(sentences_or_hf_dataset, datasets.Dataset):
            self.sentences = [ex['text'] for ex in sentences_or_hf_dataset if ex['text'] and ex['text'].strip()]
        elif isinstance(sentences_or_hf_dataset, list):
            self.sentences = sentences_or_hf_dataset
        else:
            raise TypeError("Input para BERTMLMDataset deve ser lista de sentenças ou datasets.Dataset.")
        if not self.sentences: print("AVISO: BERTMLMDataset inicializado com zero sentenças.")

    def __len__(self): return len(self.sentences)
    def _mask_tokens(self, ids_orig):
        inputs, labels = list(ids_orig), list(ids_orig)
        for i, token_id in enumerate(inputs):
            if token_id in [self.cls_id, self.sep_id, self.pad_id]: labels[i] = self.pad_id; continue
            if random.random() < 0.15:
                act_prob = random.random()
                if act_prob < 0.8: inputs[i] = self.mask_id
                elif act_prob < 0.9: inputs[i] = random.randrange(self.vocab_size)
            else: labels[i] = self.pad_id
        return torch.tensor(inputs), torch.tensor(labels)
    def __getitem__(self, idx):
        enc = self.tokenizer.encode(self.sentences[idx])
        masked_ids, mlm_labels = self._mask_tokens(enc.ids)
        return {"input_ids": masked_ids, "attention_mask": torch.tensor(enc.attention_mask, dtype=torch.long), 
                "segment_ids": torch.zeros_like(masked_ids, dtype=torch.long), "labels": mlm_labels}

class BERTEmbedding(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_len, dropout_prob, pad_token_id):
        super().__init__(); self.tok_emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        self.seg_emb = nn.Embedding(2, hidden_size); self.pos_emb = nn.Embedding(max_len, hidden_size)
        self.norm = nn.LayerNorm(hidden_size, eps=1e-12); self.drop = nn.Dropout(dropout_prob)
        self.register_buffer("pos_ids", torch.arange(max_len).expand((1, -1)))
    def forward(self, input_ids, segment_ids):
        seq_len = input_ids.size(1); tok_e, seg_e = self.tok_emb(input_ids), self.seg_emb(segment_ids)
        pos_e = self.pos_emb(self.pos_ids[:, :seq_len])
        return self.drop(self.norm(tok_e + pos_e + seg_e))

class BERTBaseModel(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, intermediate_size, max_len_config, pad_token_id, dropout_prob):
        super().__init__()
        self.emb = BERTEmbedding(vocab_size, hidden_size, max_len_config, dropout_prob, pad_token_id)
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=intermediate_size, 
                                             dropout=dropout_prob, activation='gelu', batch_first=True)
        self.enc = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, input_ids, attention_mask, segment_ids):
        x = self.emb(input_ids, segment_ids)
        return self.enc(x, src_key_padding_mask=(attention_mask == 0))

class BERTLM(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, vocab_size, hidden_size):
        super().__init__(); self.bert_base = bert_base
        self.mlm_head = nn.Linear(hidden_size, vocab_size)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.mlm_head(self.bert_base(input_ids, attention_mask, segment_ids))

class SimplifiedTrainer: # Focado em MLM
    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, device, 
                 model_save_path, vocab_size_for_loss, log_freq=20):
        self.model, self.train_dl, self.val_dl = model.to(device), train_dataloader, val_dataloader
        self.opt, self.crit, self.dev = optimizer, criterion, device
        self.save_path = Path(model_save_path) # Garante que é um objeto Path
        self.best_val_loss = float('inf')
        self.vocab_size = vocab_size_for_loss
        self.log_freq = log_freq

    def _run_epoch(self, epoch_num, is_training):
        self.model.train(is_training); dl = self.train_dl if is_training else self.val_dl
        if not dl: return None
        total_loss = 0.0
        desc = f"Epoch {epoch_num+1} [{'Train' if is_training else 'Val'}] (MLM)"
        progress_bar = tqdm(dl, total=len(dl) if hasattr(dl, '__len__') else None, desc=desc, file=sys.stdout)
        for i_batch, batch in enumerate(progress_bar):
            input_ids=batch["input_ids"].to(self.dev); attention_mask=batch["attention_mask"].to(self.dev)
            segment_ids=batch.get("segment_ids",torch.zeros_like(input_ids)).to(self.dev); 
            labels=batch["labels"].to(self.dev)
            if is_training: self.opt.zero_grad()
            with torch.set_grad_enabled(is_training):
                logits = self.model(input_ids, attention_mask, segment_ids)
                loss = self.crit(logits.view(-1, self.vocab_size), labels.view(-1))
            if is_training: loss.backward(); self.opt.step()
            total_loss += loss.item()
            if (i_batch + 1) % self.log_freq == 0: progress_bar.set_postfix({"loss": f"{total_loss / (i_batch + 1):.4f}"})
        avg_loss = total_loss / len(dl) if len(dl) > 0 else 0.0
        print(f"{desc} - Avg Loss: {avg_loss:.4f}")
        return avg_loss

    def train(self, max_epochs, target_loss_objective=None):
        print(f"Treinando (MLM) por até {max_epochs} épocas.")
        if target_loss_objective is not None:
            print(f"Objetivo de loss (validação, senão treino): {target_loss_objective:.4f}")
        print(f"Observando 'loss' de validação para melhor modelo (se houver validação).")

        training_stopped_by_target = False
        model_saved_this_run = False # Flag para rastrear se um modelo foi salvo

        for epoch in range(max_epochs):
            train_loss_epoch = self._run_epoch(epoch, is_training=True)
            val_loss_epoch = self._run_epoch(epoch, is_training=False)
            
            # Lógica de salvar o melhor modelo (se houver validação)
            if self.val_dl and val_loss_epoch is not None:
                if val_loss_epoch < self.best_val_loss:
                    self.best_val_loss = val_loss_epoch
                    print(f"Nova melhor Val Loss (MLM): {self.best_val_loss:.4f}. Salvando modelo: {self.save_path}")
                    torch.save(self.model.state_dict(), self.save_path)
                    model_saved_this_run = True
            
            print("-" * 30)

            # Verifica se o objetivo de loss foi atingido
            loss_to_check = val_loss_epoch if self.val_dl and val_loss_epoch is not None else train_loss_epoch
            if target_loss_objective is not None and loss_to_check is not None:
                if loss_to_check <= target_loss_objective:
                    print(f"Objetivo de loss ({target_loss_objective:.4f}) alcançado na época {epoch+1} com loss_atual={loss_to_check:.4f}.")
                    # Se o objetivo foi atingido e não há validação, e nenhum modelo foi salvo ainda.
                    if not self.val_dl and not model_saved_this_run:
                        print(f"Salvando modelo que atingiu target_loss (sem validação): {self.save_path}")
                        torch.save(self.model.state_dict(), self.save_path)
                        model_saved_this_run = True
                    training_stopped_by_target = True
                    break # Interrompe o loop de épocas
        
        # Salva o modelo da última época se não houver validação, o treino não foi interrompido pelo target_loss,
        # e nenhum modelo foi salvo ainda (ex: se target_loss não foi definido)
        if not self.val_dl and not training_stopped_by_target and (epoch == max_epochs - 1) and not model_saved_this_run:
            print(f"(MLM) Sem validação, treino concluído. Salvando modelo da última época ({max_epochs}): {self.save_path}")
            torch.save(self.model.state_dict(), self.save_path)
            model_saved_this_run = True
        
        final_status_message = "interrompido cedo por objetivo de loss" if training_stopped_by_target else "concluído"
        best_val_display = "N/A"
        if isinstance(self.best_val_loss,(int,float)): # Checa se best_val_loss foi atualizado
            if math.isinf(self.best_val_loss): best_val_display="N/A (sem val ou não melhorou)"
            elif math.isnan(self.best_val_loss): best_val_display="N/A (NaN)"
            else: best_val_display=f"{self.best_val_loss:.4f}"
        
        print(f"Treinamento (MLM) {final_status_message}. Melhor Val Loss (se aplicável): {best_val_display}")
        if model_saved_this_run: # Verifica se um modelo foi salvo em algum momento
             print(f"Modelo salvo em: {self.save_path}")
        else:
             print(f"Nenhum modelo foi salvo nesta execução (target loss não atingido e/ou val loss não melhorou).")


# --- Funções para cada Fase do Pipeline ---
def setup_data_and_train_tokenizer(args):
    print("\n--- Fase: Preparação de Dados Aroeira e Tokenizador ---")
    aroeira_data_source_for_mlm_local = None # Para retornar lista ou Path
    _all_aroeira_sentences_list_local = [] # Para o caso de subset ou fallback do Hub

    text_col = "text" 
    temp_file_for_tokenizer = Path(args.temp_tokenizer_train_file)

    if args.sagemaker_input_data_dir and args.input_data_filename:
        local_data_path = os.path.join(args.sagemaker_input_data_dir, args.input_data_filename)
        print(f"Lendo dados Aroeira do caminho SageMaker: {local_data_path}")
        if Path(local_data_path).exists():
            if temp_file_for_tokenizer.exists(): temp_file_for_tokenizer.unlink()
            import shutil # Para copiar o arquivo
            shutil.copyfile(local_data_path, temp_file_for_tokenizer)
            print(f"Dados do S3 copiados para arquivo temporário do tokenizador: {temp_file_for_tokenizer}")
            aroeira_data_source_for_mlm_local = temp_file_for_tokenizer # Fonte para MLM é o arquivo
        else:
            print(f"AVISO: Arquivo de dados {local_data_path} do SageMaker não encontrado. Tentando Hub.")
    
    if not aroeira_data_source_for_mlm_local: # Se S3 falhou ou não foi especificado
        print("Carregando Aroeira do Hugging Face Hub (modo streaming)...")
        streamed_ds = datasets.load_dataset("Itau-Unibanco/aroeira",split="train",streaming=True,trust_remote_code=args.trust_remote_code)
        collected_ex = []
        src_iter = streamed_ds; desc_tqdm = "Extraindo sentenças Aroeira (Hub)"; total_tqdm = None
        if args.aroeira_subset_size is not None:
            iterator = iter(streamed_ds); print(f"Coletando {args.aroeira_subset_size} exemplos Aroeira (Hub)...")
            try:
                for _ in range(args.aroeira_subset_size): collected_ex.append(next(iterator))
            except StopIteration: print(f"Alerta: Stream Aroeira (Hub) esgotado. Coletados {len(collected_ex)}.")
            src_iter = collected_ex; desc_tqdm += " (subconjunto)"; total_tqdm = len(collected_ex)
        else: desc_tqdm += " (stream completo)"; print("AVISO: Processando stream completo do Aroeira (Hub).")
        
        for ex_hub in tqdm(src_iter, total=total_tqdm, desc=desc_tqdm, file=sys.stdout): # Renomeado ex
            sent_hub = ex_hub.get(text_col) # Renomeado sent
            if isinstance(sent_hub, str) and sent_hub.strip(): _all_aroeira_sentences_list_local.append(sent_hub.strip())

        if not _all_aroeira_sentences_list_local: raise ValueError("Nenhuma sentença Aroeira (Hub) extraída.")
        print(f"Total de sentenças Aroeira (Hub): {len(_all_aroeira_sentences_list_local)}")
        if temp_file_for_tokenizer.exists(): temp_file_for_tokenizer.unlink()
        with open(temp_file_for_tokenizer, "w", encoding="utf-8") as f_hub: # Renomeado f
            for s_line_hub in _all_aroeira_sentences_list_local: f_hub.write(s_line_hub + "\n") # Renomeado s_line
        
        if args.aroeira_subset_size is not None:
             aroeira_data_source_for_mlm_local = _all_aroeira_sentences_list_local # Lista para subset
        else:
             aroeira_data_source_for_mlm_local = temp_file_for_tokenizer # Arquivo para dataset completo
        
    tokenizer_train_files_list = [str(temp_file_for_tokenizer)] # Tokenizer sempre treina do arquivo
    
    vocab_f = Path(args.tokenizer_vocab_filename); merges_f = Path(args.tokenizer_merges_filename)
    if not vocab_f.exists() or not merges_f.exists():
        if not Path(tokenizer_train_files_list[0]).exists():
            raise FileNotFoundError(f"Arquivo treino tokenizador '{tokenizer_train_files_list[0]}' não encontrado.")
        base_vocab_name = Path(args.tokenizer_vocab_filename).name
        prefix = base_vocab_name.replace("-vocab.json", "")
        tok_model_bpe = ByteLevelBPETokenizer(lowercase=True)
        print(f"Treinando tokenizador com {tokenizer_train_files_list}...")
        tok_model_bpe.train(files=tokenizer_train_files_list, vocab_size=args.vocab_size, 
                           min_frequency=args.min_frequency_tokenizer, 
                           special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"])
        tok_model_bpe.save_model(str(Path(args.output_dir)), prefix=prefix)
    else: print("Tokenizador já existe. Carregando...")
    
    loaded_tokenizer = ByteLevelBPETokenizer(vocab=args.tokenizer_vocab_filename, merges=args.tokenizer_merges_filename, lowercase=True)
    loaded_tokenizer._tokenizer.post_processor = BertProcessing(("</s>", loaded_tokenizer.token_to_id("</s>")), ("<s>", loaded_tokenizer.token_to_id("<s>")))
    loaded_tokenizer.enable_truncation(max_length=args.max_len)
    loaded_tokenizer.enable_padding(pad_id=loaded_tokenizer.token_to_id("<pad>"), pad_token="<pad>", length=args.max_len)
    pad_id_val = loaded_tokenizer.token_to_id("<pad>")
    print(f"Vocabulário: {loaded_tokenizer.get_vocab_size()}, PAD ID: {pad_id_val}")
    return loaded_tokenizer, pad_id_val, aroeira_data_source_for_mlm_local

def run_mlm_pretrain(args, tokenizer_obj, pad_token_id_val, data_source_for_mlm):
    print("\n--- Fase: Pré-Treinamento MLM ---")
    mlm_processed_sentences = []
    if isinstance(data_source_for_mlm, Path): # Se for o arquivo com dataset completo
        print(f"Carregando sentenças para MLM do arquivo: {data_source_for_mlm}")
        # Usa datasets.load_dataset para carregar o arquivo de texto de forma eficiente
        # Isso retorna um objeto Dataset da Hugging Face
        hf_text_ds = datasets.load_dataset("text", data_files=str(data_source_for_mlm), split="train", trust_remote_code=args.trust_remote_code)
        # BERTMLMDataset foi adaptado para receber este objeto Dataset
        dataset_input_for_mlm = hf_text_ds 
        # Se for dividir em treino/validação, faça aqui com hf_text_ds.train_test_split()
        # Por simplicidade, usaremos tudo para treino e uma pequena porção para validação se possível
        if len(hf_text_ds) > 10: # Condição mínima para criar um split de validação
            split = hf_text_ds.train_test_split(test_size=0.1, seed=42)
            train_mlm_input_data = split['train']
            val_mlm_input_data = split['test']
        else:
            train_mlm_input_data = hf_text_ds
            val_mlm_input_data = None # Sem validação se o dataset for muito pequeno
    elif isinstance(data_source_for_mlm, list): # Se for uma lista de sentenças (subset)
        mlm_processed_sentences = data_source_for_mlm
        val_s_mlm_r = 0.1; num_v_mlm = int(len(mlm_processed_sentences) * val_s_mlm_r)
        if num_v_mlm < 1 and len(mlm_processed_sentences) > 1: num_v_mlm = 1
        train_mlm_input_data = mlm_processed_sentences[num_v_mlm:]
        val_mlm_input_data = mlm_processed_sentences[:num_v_mlm]
        if not train_mlm_input_data: train_mlm_input_data = mlm_processed_sentences; val_mlm_input_data = []
    else:
        raise TypeError("data_source_for_mlm deve ser lista de sentenças ou Path para arquivo.")

    print(f"Dados de Treino (MLM) para BERTMLMDataset: {len(train_mlm_input_data)} exemplos/linhas.")
    if val_mlm_input_data: print(f"Dados de Validação (MLM) para BERTMLMDataset: {len(val_mlm_input_data)} exemplos/linhas.")

    train_ds_mlm = BERTMLMDataset(train_mlm_input_data, tokenizer_obj, args.max_len, pad_token_id_val)
    if len(train_ds_mlm) == 0 and ( (isinstance(train_mlm_input_data, list) and len(train_mlm_input_data) > 0) or \
                                   (isinstance(train_mlm_input_data, datasets.Dataset) and len(train_mlm_input_data) > 0) ):
        raise ValueError("Dataset de treino MLM vazio após processamento em BERTMLMDataset.")
    train_dl_mlm = DataLoader(train_ds_mlm, batch_size=args.batch_size_pretrain, shuffle=True, num_workers=0)
    
    val_dl_mlm = None
    if val_mlm_input_data and len(val_mlm_input_data) > 0 :
        val_ds_mlm = BERTMLMDataset(val_mlm_input_data, tokenizer_obj, args.max_len, pad_token_id_val)
        if len(val_ds_mlm) > 0: val_dl_mlm = DataLoader(val_ds_mlm, batch_size=args.batch_size_pretrain, shuffle=False, num_workers=0)

    bert_base = BERTBaseModel(tokenizer_obj.get_vocab_size(), args.model_hidden_size, args.model_num_layers, 
                             args.model_num_attention_heads, args.model_intermediate_size, args.max_len, 
                             pad_token_id_val, dropout_prob=args.model_dropout_prob)
    bertlm_model = BERTLM(bert_base, tokenizer_obj.get_vocab_size(), args.model_hidden_size)
    opt_mlm = torch.optim.AdamW(bertlm_model.parameters(), lr=args.lr_pretrain, 
                                betas=(args.adam_beta1, args.adam_beta2), eps=args.adam_epsilon, weight_decay=args.weight_decay)
    crit_mlm = nn.CrossEntropyLoss(ignore_index=pad_token_id_val)
    
    trainer_mlm_instance = SimplifiedTrainer(
        bertlm_model, train_dl_mlm, val_dl_mlm, opt_mlm, crit_mlm, args.device, 
        args.pretrained_bertlm_save_filename, tokenizer_obj.get_vocab_size(), log_freq=args.logging_steps
    )
    print("Pré-treinamento MLM configurado. Iniciando...")
    trainer_mlm_instance.train(max_epochs=args.max_epochs_pretrain, target_loss_objective=args.target_loss_pretrain)


def parse_args(custom_args_list=None):
    parser = argparse.ArgumentParser(description="Pipeline de Pré-treino BERT no Aroeira (MLM-only).")
    parser.add_argument("--max_len", type=int, default=128)
    parser.add_argument("--aroeira_subset_size", type=int, default=None)
    parser.add_argument("--vocab_size", type=int, default=30000)
    parser.add_argument("--min_frequency_tokenizer", type=int, default=2)
    parser.add_argument("--device", type=str, default=None, choices=['cuda', 'cpu'])
    parser.add_argument("--trust_remote_code", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument("--sagemaker_input_data_dir", type=str, default=None)
    parser.add_argument("--input_data_filename", type=str, default=None)
    parser.add_argument("--max_epochs_pretrain", type=int, default=3) # Renomeado
    parser.add_argument("--target_loss_pretrain", type=float, default=None) # Novo
    parser.add_argument("--batch_size_pretrain", type=int, default=8)
    parser.add_argument("--lr_pretrain", type=float, default=5e-5)
    parser.add_argument("--model_hidden_size", type=int, default=256)
    parser.add_argument("--model_num_layers", type=int, default=2)
    parser.add_argument("--model_num_attention_heads", type=int, default=4)
    parser.add_argument("--model_dropout_prob", type=float, default=0.1)
    parser.add_argument("--output_dir", type=str, default=os.environ.get('SM_MODEL_DIR', './bert_mlm_outputs_final_v2'))
    parser.add_argument("--tokenizer_vocab_filename", type=str, default="aroeira_mlm_tokenizer-vocab.json")
    parser.add_argument("--tokenizer_merges_filename", type=str, default="aroeira_mlm_tokenizer-merges.txt")
    parser.add_argument("--pretrained_bertlm_save_filename", type=str, default="aroeira_bertlm_pretrained.pth")
    parser.add_argument("--temp_tokenizer_train_file", type=str, default="temp_aroeira_for_tokenizer.txt")
    parser.add_argument("--do_dataprep_tokenizer", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument("--do_pretrain", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument('--logging_steps', type=int, default=20) # Ajustado log_freq default
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.999)
    parser.add_argument("--adam_epsilon", type=float, default=1e-8)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    # Manter outros args que o SageMaker possa passar para evitar erros de "unrecognized"
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help="Ignorado pelo SimplifiedTrainer")
    parser.add_argument('--warmup_steps', type=int, default=0, help="Ignorado pelo AdamW direto, precisaria de scheduler")
    # ... (outros args do HF Trainer podem ser adicionados aqui com 'help="Ignorado..."')

    if custom_args_list: args = parser.parse_args(custom_args_list)
    else: args = parser.parse_args()
    
    if args.device is None: args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    args.model_intermediate_size = args.model_hidden_size * 4
    output_dir_path = Path(args.output_dir)
    if not str(args.output_dir).startswith("/opt/ml/"):
         output_dir_path.mkdir(parents=True, exist_ok=True)
    args.tokenizer_vocab_filename = str(output_dir_path / Path(args.tokenizer_vocab_filename).name)
    args.tokenizer_merges_filename = str(output_dir_path / Path(args.tokenizer_merges_filename).name)
    args.pretrained_bertlm_save_filename = str(output_dir_path / Path(args.pretrained_bertlm_save_filename).name)
    args.temp_tokenizer_train_file = str(output_dir_path / Path(args.temp_tokenizer_train_file).name)
    return args

def main(notebook_mode_args_list=None):
    ARGS = parse_args(notebook_mode_args_list)
    print("--- Configurações Utilizadas ---")
    for arg_name, value in vars(ARGS).items(): print(f"{arg_name}: {value}")
    print("----------------------------------")

    current_tokenizer_obj, current_pad_id, data_source_for_mlm = None, None, None
    
    if ARGS.do_dataprep_tokenizer or ARGS.do_pretrain:
        current_tokenizer_obj, current_pad_id, data_source_for_mlm = setup_data_and_train_tokenizer(ARGS)
    else:
        print("Nenhuma ação de preparação de dados ou pré-treinamento solicitada. Encerrando.")
        return

    if ARGS.do_pretrain:
        if not data_source_for_mlm or not current_tokenizer_obj:
            print("ERRO: Fonte de dados Aroeira ou tokenizador não preparados para pré-treinamento.")
        else:
            run_mlm_pretrain(ARGS, current_tokenizer_obj, current_pad_id, data_source_for_mlm)
    
    print("\n--- Pipeline MLM Finalizado ---")

if __name__ == "__main__":
    main()

# --- Como Executar no Jupyter Notebook (Exemplo) ---
# notebook_args = [
#     "--max_len", "64",
#     # Para testar com dataset completo do Hub (requer modificações no script para não estourar RAM com all_aroeira_sentences):
#     # Se AROEIRA_SUBSET_SIZE for None, setup_data_and_train_tokenizer retorna o CAMINHO do arquivo de texto.
#     # Se for um número, retorna a LISTA de sentenças (para o subset).
#     # "--aroeira_subset_size", "1000", # Comente esta linha para usar o dataset completo (com ressalvas de RAM)
#     # Para testar o carregamento do S3 (simulado):
#     # "--sagemaker_input_data_dir", "./caminho_simulado_sagemaker_input/aroeira_data/", 
#     # "--input_data_filename", "meu_arquivo_aroeira.json", 
#     "--max_epochs_pretrain", "1",
#     "--target_loss_pretrain", "3.0", # Exemplo de loss alvo
#     "--batch_size_pretrain", "4", 
#     "--model_hidden_size", "128", 
#     "--model_num_layers", "2",
#     "--model_num_attention_heads", "2",
#     "--output_dir", "./notebook_outputs_mlm_final", 
#     # As flags do_dataprep_tokenizer e do_pretrain são True por padrão no parse_args
# ]
# # Antes de rodar main com dados do S3 simulados, crie o arquivo dummy:
# # Path("./caminho_simulado_sagemaker_input/aroeira_data/").mkdir(parents=True, exist_ok=True)
# # dummy_data_content = [{"text": "Primeira frase do aroeira."}, {"text": "Segunda frase do aroeira."}]
# # import json
# # with open("./caminho_simulado_sagemaker_input/aroeira_data/meu_arquivo_aroeira.json", "w", encoding="utf-8") as f_dummy:
# #     for entry in dummy_data_content:
# #         f_dummy.write(json.dumps(entry) + "\n") # Formato JSON Lines
#
# # main(notebook_mode_args_list=notebook_args) 
# # print("Execução de teste do notebook concluída.")
