# --- Bloco 1: Imports Essenciais ---
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import datasets
import tokenizers # Para tokenizers.__version__
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
from tqdm.auto import tqdm # Recomendado para auto-seleção do backend tqdm
import random
from pathlib import Path
import sys
import math # Para math.isnan

# Métricas para benchmarks
from sklearn.metrics import accuracy_score, f1_score, classification_report
from seqeval.metrics import classification_report as seqeval_classification_report
from seqeval.scheme import IOB2

print(f"PyTorch: {torch.__version__}")
print(f"Datasets: {datasets.__version__}")
print(f"Tokenizers: {tokenizers.__version__}")

# --- Bloco 2: Constantes e Configurações Iniciais ---
# Configurações Gerais
MAX_LEN = 128
AROEIRA_SUBSET_SIZE = 10000 # None para dataset completo (CUIDADO: RAM)
VOCAB_SIZE = 30000
MIN_FREQUENCY_TOKENIZER = 2
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# Configurações de Pré-treinamento MLM
EPOCHS_PRETRAIN = 1 # Ajuste para treino mais longo
BATCH_SIZE_PRETRAIN = 8
LEARNING_RATE_PRETRAIN = 5e-5

# Arquitetura do Modelo BERT (base)
MODEL_HIDDEN_SIZE = 256
MODEL_NUM_LAYERS = 2
MODEL_NUM_ATTENTION_HEADS = 4
MODEL_INTERMEDIATE_SIZE = MODEL_HIDDEN_SIZE * 4

# Nomes de Arquivos
TOKENIZER_VOCAB_FILENAME = "aroeira_mlm_tokenizer-vocab.json"
TOKENIZER_MERGES_FILENAME = "aroeira_mlm_tokenizer-merges.txt"
PRETRAINED_BERTLM_SAVE_FILENAME = "aroeira_bertlm_pretrained.pth"
TEMP_TOKENIZER_TRAIN_FILE = Path("temp_aroeira_for_tokenizer.txt")

# Constantes para Fine-tuning dos Benchmarks
FINETUNE_EPOCHS = 2
FINETUNE_BATCH_SIZE = 8
FINETUNE_LR = 3e-5

ASSIN_NLI_MODEL_SAVE_FILENAME = "aroeira_assin_nli_model.pth"
ASSIN_SUBSET_SIZE_TRAIN = 500
ASSIN_SUBSET_SIZE_VAL = 100

HAREM_MODEL_SAVE_FILENAME = "aroeira_harem_ner_model.pth"
HAREM_SUBSET_SIZE = 250
PAD_TOKEN_LABEL_ID_NER = -100 # ID para ignorar no loss de NER

print(f"Dispositivo selecionado: {DEVICE}")
print(f"Subconjunto Aroeira (Pré-Treino): {AROEIRA_SUBSET_SIZE if AROEIRA_SUBSET_SIZE is not None else 'Completo'}")

# --- Bloco 3: Carregando e Pré-processando o Corpus Aroeira ---
print("\n--- Bloco 3: Carregando Dados do Aroeira ---")
all_aroeira_sentences = []
try:
    streamed_aroeira_dataset = datasets.load_dataset("Itau-Unibanco/aroeira",split="train",streaming=True,trust_remote_code=True)
    text_column_name = "text"
    
    aroeira_examples_collected = []
    source_iterable = streamed_aroeira_dataset
    desc_tqdm = "Extraindo sentenças Aroeira"
    tqdm_total = None # Para stream completo, total é desconhecido

    if AROEIRA_SUBSET_SIZE is not None:
        stream_iterator = iter(streamed_aroeira_dataset)
        print(f"Coletando {AROEIRA_SUBSET_SIZE} exemplos do Aroeira stream...")
        try:
            for _ in range(AROEIRA_SUBSET_SIZE): aroeira_examples_collected.append(next(stream_iterator))
        except StopIteration: print(f"Alerta: Stream Aroeira esgotado. Coletados {len(aroeira_examples_collected)}.")
        source_iterable = aroeira_examples_collected
        desc_tqdm += " (subconjunto)"
        tqdm_total = len(aroeira_examples_collected)
    else:
        desc_tqdm += " (stream completo)"
        print("AVISO: Processando stream completo do Aroeira. Pode consumir muita RAM para 'all_aroeira_sentences'.")

    for example in tqdm(source_iterable, total=tqdm_total, desc=desc_tqdm, file=sys.stdout):
        sentence = example.get(text_column_name)
        if isinstance(sentence, str) and sentence.strip(): all_aroeira_sentences.append(sentence.strip())

    if not all_aroeira_sentences: raise ValueError("Nenhuma sentença Aroeira foi extraída.")
    print(f"Total de sentenças Aroeira extraídas: {len(all_aroeira_sentences)}")

    if TEMP_TOKENIZER_TRAIN_FILE.exists(): TEMP_TOKENIZER_TRAIN_FILE.unlink()
    with open(TEMP_TOKENIZER_TRAIN_FILE, "w", encoding="utf-8") as fp:
        for sentence in all_aroeira_sentences: fp.write(sentence + "\n")
    tokenizer_train_files_list = [str(TEMP_TOKENIZER_TRAIN_FILE)]
    print(f"Arquivo temporário para tokenizador: {tokenizer_train_files_list[0]}")
except Exception as e: print(f"Erro Bloco 3: {e}"); import traceback; traceback.print_exc(); raise

# --- Bloco 4: Treinando o Tokenizador ByteLevelBPE ---
print("\n--- Bloco 4: Treinando o Tokenizador ---")
if not Path(TOKENIZER_VOCAB_FILENAME).exists() or not Path(TOKENIZER_MERGES_FILENAME).exists():
    if not tokenizer_train_files_list or not Path(tokenizer_train_files_list[0]).exists():
        raise FileNotFoundError("Arquivo de treino para tokenizador não encontrado. Execute o Bloco 3.")
    tokenizer_save_prefix = TOKENIZER_VOCAB_FILENAME.replace("-vocab.json", "")
    custom_tokenizer = ByteLevelBPETokenizer()
    print(f"Treinando tokenizador com dados de: {tokenizer_train_files_list[0]}...")
    custom_tokenizer.train(files=tokenizer_train_files_list, vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQUENCY_TOKENIZER, special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"])
    custom_tokenizer.save_model(".", prefix=tokenizer_save_prefix) # Salva no diretório atual
    print(f"Tokenizador treinado e salvo.")
else: print(f"Tokenizador já existe. Carregando...")

tokenizer = ByteLevelBPETokenizer(vocab=TOKENIZER_VOCAB_FILENAME, merges=TOKENIZER_MERGES_FILENAME)
tokenizer._tokenizer.post_processor = BertProcessing(("</s>", tokenizer.token_to_id("</s>")), ("<s>", tokenizer.token_to_id("<s>")))
tokenizer.enable_truncation(max_length=MAX_LEN)
tokenizer.enable_padding(pad_id=tokenizer.token_to_id("<pad>"), pad_token="<pad>", length=MAX_LEN)
PAD_TOKEN_ID = tokenizer.token_to_id("<pad>")
print(f"Vocabulário do Tokenizador: {tokenizer.get_vocab_size()}")

# --- Bloco 5: Definição do BERTMLMDataset ---
print("\n--- Bloco 5: Definindo Dataset para MLM ---")
class BERTMLMDataset(Dataset): # Dataset para Masked Language Modeling
    def __init__(self, sentences, tokenizer, max_len):
        self.sentences, self.tokenizer, self.max_len = sentences, tokenizer, max_len
        self.vocab_size = tokenizer.get_vocab_size()
        self.mask_id, self.cls_id, self.sep_id, self.pad_id = tokenizer.token_to_id("<mask>"), tokenizer.token_to_id("<s>"), tokenizer.token_to_id("</s>"), PAD_TOKEN_ID
    def __len__(self): return len(self.sentences)
    def _mask_tokens(self, ids_orig): # Lógica de mascaramento do BERT
        inputs, labels = list(ids_orig), list(ids_orig)
        for i, token_id in enumerate(inputs):
            if token_id in [self.cls_id, self.sep_id, self.pad_id]: labels[i] = self.pad_id; continue
            if random.random() < 0.15: # 15% dos tokens
                act_prob = random.random()
                if act_prob < 0.8: inputs[i] = self.mask_id
                elif act_prob < 0.9: inputs[i] = random.randrange(self.vocab_size)
            else: labels[i] = self.pad_id
        return torch.tensor(inputs), torch.tensor(labels)
    def __getitem__(self, idx):
        enc = self.tokenizer.encode(self.sentences[idx])
        masked_ids, mlm_labels = self._mask_tokens(enc.ids)
        return {"input_ids": masked_ids, "attention_mask": torch.tensor(enc.attention_mask, dtype=torch.long), 
                "segment_ids": torch.zeros_like(masked_ids, dtype=torch.long), "labels": mlm_labels}

# --- Bloco 6: Definição do Modelo BERT ---
print("\n--- Bloco 6: Definindo Arquitetura BERT ---")
class BERTEmbedding(nn.Module): # Camada de Embedding do BERT
    def __init__(self, vocab_size, hidden_size, max_len, dropout_prob, pad_token_id):
        super().__init__(); self.tok_emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        self.seg_emb = nn.Embedding(2, hidden_size); self.pos_emb = nn.Embedding(max_len, hidden_size)
        self.norm = nn.LayerNorm(hidden_size, eps=1e-12); self.drop = nn.Dropout(dropout_prob)
        self.register_buffer("pos_ids", torch.arange(max_len).expand((1, -1)))
    def forward(self, input_ids, segment_ids):
        seq_len = input_ids.size(1); tok_e, seg_e = self.tok_emb(input_ids), self.seg_emb(segment_ids)
        pos_e = self.pos_emb(self.pos_ids[:, :seq_len])
        return self.drop(self.norm(tok_e + pos_e + seg_e))

class BERTBaseModel(nn.Module): # Backbone BERT (camadas Transformer)
    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, intermediate_size, max_len_config, pad_token_id, dropout_prob=0.1):
        super().__init__()
        self.emb = BERTEmbedding(vocab_size, hidden_size, max_len_config, dropout_prob, pad_token_id)
        enc_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=intermediate_size, 
                                             dropout=dropout_prob, activation='gelu', batch_first=True)
        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
    def forward(self, input_ids, attention_mask, segment_ids):
        x = self.emb(input_ids, segment_ids)
        return self.enc(x, src_key_padding_mask=(attention_mask == 0))

class BERTLM(nn.Module): # Modelo BERT com cabeça para Masked Language Modeling
    def __init__(self, bert_base: BERTBaseModel, vocab_size, hidden_size):
        super().__init__(); self.bert_base = bert_base
        self.mlm_head = nn.Linear(hidden_size, vocab_size)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.mlm_head(self.bert_base(input_ids, attention_mask, segment_ids))

# --- Bloco 7: Trainer Genérico ---
print("\n--- Bloco 7: Definindo Trainer Genérico ---")
class GenericTrainer: # Trainer para MLM, NLI, NER
    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, device, 
                 model_save_path, task_type="mlm", vocab_size_for_loss=None, id2label=None, log_freq=20):
        self.model, self.train_dl, self.val_dl = model.to(device), train_dataloader, val_dataloader
        self.opt, self.crit, self.dev = optimizer, criterion, device
        self.save_path = model_save_path
        self.best_val_metric = float('inf') if task_type in ["mlm", "nli"] else float('-inf')
        self.task_type, self.vocab_size, self.id2label = task_type, vocab_size_for_loss, id2label
        self.log_freq = log_freq
        if task_type == "ner" and id2label is None: raise ValueError("id2label é necessário para task_type='ner'")

    def _run_epoch(self, epoch_num, is_training):
        self.model.train(is_training); dl = self.train_dl if is_training else self.val_dl
        if not dl: return None, {}
        total_loss, all_preds_epoch, all_labels_epoch = 0.0, [], []
        desc = f"Epoch {epoch_num+1} [{'Train' if is_training else 'Val'}] ({self.task_type})"
        progress_bar = tqdm(dl, total=len(dl), desc=desc, file=sys.stdout)

        for i_batch, batch in enumerate(progress_bar):
            input_ids = batch["input_ids"].to(self.dev); attention_mask = batch["attention_mask"].to(self.dev)
            segment_ids = batch.get("segment_ids", torch.zeros_like(input_ids)).to(self.dev)
            labels = batch["labels"].to(self.dev)

            if is_training: self.opt.zero_grad()
            with torch.set_grad_enabled(is_training):
                logits = self.model(input_ids, attention_mask, segment_ids)
                if self.task_type in ["mlm", "ner"]:
                    loss = self.crit(logits.view(-1, logits.size(-1)), labels.view(-1))
                elif self.task_type == "nli":
                    loss = self.crit(logits, labels)
                else: loss = torch.tensor(0.0, device=self.dev) 
            
            if is_training: loss.backward(); self.opt.step()
            total_loss += loss.item()
            if (i_batch + 1) % self.log_freq == 0:
                progress_bar.set_postfix({"loss": f"{total_loss / (i_batch + 1):.4f}"})

            if not is_training:
                if self.task_type == "nli":
                    all_preds_epoch.extend(torch.argmax(logits, dim=-1).cpu().numpy())
                    all_labels_epoch.extend(labels.cpu().numpy())
                elif self.task_type == "ner":
                    preds_ids = torch.argmax(logits, dim=-1).cpu().numpy()
                    labels_ids = labels.cpu().numpy()
                    for p_seq, l_seq, m_seq in zip(preds_ids, labels_ids, attention_mask.cpu().numpy()):
                        all_preds_epoch.append([self.id2label[p] for p,l,m in zip(p_seq,l_seq,m_seq) if m==1 and l!=self.crit.ignore_index])
                        all_labels_epoch.append([self.id2label[l] for l,m in zip(l_seq,m_seq) if m==1 and l!=self.crit.ignore_index])
        
        avg_loss = total_loss / len(dl)
        metrics = {"loss": avg_loss}
        if not is_training and all_labels_epoch:
            if self.task_type == "nli" and all_preds_epoch:
                metrics["accuracy"] = accuracy_score(all_labels_epoch, all_preds_epoch)
                metrics["f1_w"] = f1_score(all_labels_epoch, all_preds_epoch, average='weighted', zero_division=0)
            elif self.task_type == "ner":
                preds_filt = [p for p in all_preds_epoch if p]; labels_filt = [l for l in all_labels_epoch if l]
                if preds_filt and labels_filt:
                    try:
                        report = seqeval_classification_report(labels_filt, preds_filt, output_dict=True, zero_division=0, mode='strict', scheme=IOB2)
                        metrics["f1_ner"] = report['micro avg']['f1-score']
                    except Exception as e_seqeval: print(f"Erro no cálculo de métricas NER: {e_seqeval}")

        print_line = f"{desc} - Avg Loss: {avg_loss:.4f}"
        for k, v_metric in metrics.items(): # Renomeado v para v_metric
            if k != "loss": print_line += f", {k}: {v_metric:.4f}"
        print(print_line)
        return avg_loss, metrics

    def train(self, num_epochs):
        metric_to_watch = "loss"; higher_is_better = False
        if self.task_type == "ner" and self.val_dl: metric_to_watch = "f1_ner"; higher_is_better = True
        elif self.task_type == "nli" and self.val_dl: metric_to_watch = "f1_w"; higher_is_better = True # Pode ser f1_w ou accuracy

        print(f"Treinando ({self.task_type}) por {num_epochs} épocas. Observando '{metric_to_watch}'.")
        for epoch in range(num_epochs):
            self._run_epoch(epoch, is_training=True)
            _, val_metrics = self._run_epoch(epoch, is_training=False)
            
            current_val = val_metrics.get(metric_to_watch) if val_metrics else None
            if current_val is not None:
                improved = (current_val < self.best_val_metric) if not higher_is_better else (current_val > self.best_val_metric)
                if improved:
                    self.best_val_metric = current_val
                    print(f"Melhor métrica ({metric_to_watch}): {self.best_val_metric:.4f}. Salvando modelo: {self.save_path}")
                    torch.save(self.model.state_dict(), self.save_path)
            elif epoch == num_epochs - 1 and not self.val_dl: # Sem validação, salva na última época
                print(f"({self.task_type}) Sem validação. Salvando modelo da última época: {self.save_path}")
                torch.save(self.model.state_dict(), self.save_path)
            print("-" * 30)
        
        best_val_display = "N/A"
        if isinstance(self.best_val_metric, (int, float)):
            if math.isinf(self.best_val_metric): best_val_display = "N/A (inf)"
            elif math.isnan(self.best_val_metric): best_val_display = "N/A (NaN)"
            else: best_val_display = f"{self.best_val_metric:.4f}"
        
        print(f"Treinamento ({self.task_type}) concluído. Melhor métrica ({metric_to_watch}): {best_val_display}")
        if not self.val_dl and num_epochs > 0 and Path(self.save_path).exists():
             print(f"Modelo final salvo em: {self.save_path}")

# --- Bloco 8: Preparando e Executando o Pré-Treinamento MLM ---
print("\n--- Bloco 8: Pré-Treinamento MLM ---")
val_split_mlm = 0.1; num_val_mlm = int(len(all_aroeira_sentences) * val_split_mlm)
if num_val_mlm < 1 and len(all_aroeira_sentences) > 1: num_val_mlm = 1
train_sents_mlm = all_aroeira_sentences[num_val_mlm:]; val_sents_mlm = all_aroeira_sentences[:num_val_mlm]
if not train_sents_mlm: train_sents_mlm = all_aroeira_sentences; val_sents_mlm = []

train_ds_mlm = BERTMLMDataset(train_sents_mlm, tokenizer, max_len=MAX_LEN)
train_dl_mlm = DataLoader(train_ds_mlm, batch_size=BATCH_SIZE_PRETRAIN, shuffle=True, num_workers=0)
val_dl_mlm = None
if val_sents_mlm and len(val_sents_mlm) > 0: # Adicionado len > 0
    val_ds_mlm = BERTMLMDataset(val_sents_mlm, tokenizer, max_len=MAX_LEN)
    if len(val_ds_mlm) > 0: val_dl_mlm = DataLoader(val_ds_mlm, batch_size=BATCH_SIZE_PRETRAIN, shuffle=False, num_workers=0)

bert_base_pt = BERTBaseModel(tokenizer.get_vocab_size(),MODEL_HIDDEN_SIZE,MODEL_NUM_LAYERS,MODEL_NUM_ATTENTION_HEADS,MODEL_INTERMEDIATE_SIZE,MAX_LEN,PAD_TOKEN_ID)
bertlm_pt_model = BERTLM(bert_base_pt, tokenizer.get_vocab_size(), MODEL_HIDDEN_SIZE)
opt_mlm = torch.optim.AdamW(bertlm_pt_model.parameters(), lr=LEARNING_RATE_PRETRAIN)
crit_mlm = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)
trainer_mlm = GenericTrainer(bertlm_pt_model, train_dl_mlm, val_dl_mlm, opt_mlm, crit_mlm, DEVICE, PRETRAINED_BERTLM_SAVE_FILENAME, "mlm", tokenizer.get_vocab_size())
print("Componentes para pré-treinamento MLM prontos.")

# --- Bloco 9: Executando o Pré-Treinamento MLM ---
print("\n--- Bloco 9: Iniciando Pré-Treinamento MLM ---")
try:
    trainer_mlm.train(num_epochs=EPOCHS_PRETRAIN)
except Exception as e: print(f"Erro pré-treinamento MLM: {e}"); import traceback; traceback.print_exc(); raise

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning ASSIN (NLI/RTE) ---")
ASSIN_NLI_MODEL_SAVE_FILENAME = "aroeira_assin_nli_model.pth"
ASSIN_SUBSET_TRAIN = 500; ASSIN_SUBSET_VAL = 100

class BERTForSequencePairClassification(nn.Module): # Modelo para NLI
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__(); self.bert_base = bert_base; self.drop = nn.Dropout(0.1)
        self.clf = nn.Linear(hidden_size, num_classes)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.clf(self.drop(self.bert_base(input_ids, attention_mask, segment_ids)[:, 0, :])) # Usa output do [CLS]

class ASSINNLIDataset(Dataset): # Dataset para ASSIN NLI
    def __init__(self, hf_ds, tokenizer, max_len):
        self.tok, self.max_len = tokenizer, max_len; self.data = []
        self.label_map = {"ENTAILMENT": 0, "NONE": 1, "PARAPHRASE": 2}; self.num_classes = 3
        proc_count, skip_count = 0,0
        feature_ej = hf_ds.features.get('entailment_judgment') if hasattr(hf_ds, 'features') else None
        for ex in tqdm(hf_ds, desc="Processando ASSIN NLI", file=sys.stdout, mininterval=1.0):
            p, h, j_val = ex.get("premise"), ex.get("hypothesis"), ex.get("entailment_judgment")
            j_str = None
            if isinstance(j_val, str): j_str = j_val
            elif isinstance(j_val, int) and feature_ej:
                try: j_str = feature_ej.int2str(j_val)
                except: pass
            if isinstance(p,str) and p.strip() and isinstance(h,str) and h.strip() and j_str in self.label_map:
                self.data.append({"p": p.strip(), "h": h.strip(), "lbl": self.label_map[j_str]}); proc_count+=1
            else: skip_count+=1
        print(f"ASSIN NLI Dataset: {proc_count} processados, {skip_count} ignorados.")
        if proc_count == 0 and len(hf_ds) > 0: print("AVISO: Nenhum dado ASSIN NLI processado.")
    def __len__(self): return len(self.data)
    def __getitem__(self, idx):
        item = self.data[idx]; enc = self.tok.encode(item["p"], item["h"])
        ids, seg_ids_list = enc.ids, [0]*len(enc.ids)
        try:
            sep_idx = ids.index(self.tok.token_to_id("</s>"),1)
            for i in range(sep_idx + 1, len(ids)): 
                if ids[i] != self.tok.token_to_id("<pad>"): seg_ids_list[i] = 1
                else: break
        except ValueError: pass # Ocorre se não houver segundo "</s>" (ex: truncamento)
        return {"input_ids":torch.tensor(ids),"attention_mask":torch.tensor(enc.attention_mask,dtype=torch.long),
                "segment_ids":torch.tensor(seg_ids_list,dtype=torch.long),"labels":torch.tensor(item["lbl"],dtype=torch.long)}
try:
    assin_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    raw_train_assin = assin_full["train"]; raw_val_assin = assin_full["validation"]
    def filter_ej(ex): return ex.get('entailment_judgment') is not None
    train_assin_ej = raw_train_assin.filter(filter_ej); val_assin_ej = raw_val_assin.filter(filter_ej)
    
    train_assin_sub = train_assin_ej.select(range(min(ASSIN_SUBSET_TRAIN, len(train_assin_ej)))) if len(train_assin_ej) > 0 else train_assin_ej
    val_assin_sub = val_assin_ej.select(range(min(ASSIN_SUBSET_VAL, len(val_assin_ej)))) if len(val_assin_ej) > 0 else val_assin_ej
    print(f"ASSIN NLI Treino (subset): {len(train_assin_sub)}, Val (subset): {len(val_assin_sub)}")

    if len(train_assin_sub) > 0:
        ft_train_ds_nli = ASSINNLIDataset(train_assin_sub, tokenizer, MAX_LEN)
        if len(ft_train_ds_nli)==0: raise ValueError("Dataset de treino ASSIN NLI vazio.")
        ft_train_dl_nli = DataLoader(ft_train_ds_nli, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        ft_val_dl_nli = None
        if len(val_assin_sub) > 0:
            ft_val_ds_nli = ASSINNLIDataset(val_assin_sub, tokenizer, MAX_LEN)
            if len(ft_val_ds_nli) > 0: ft_val_dl_nli = DataLoader(ft_val_ds_nli, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)
        
        base_nli = BERTBaseModel(tokenizer.get_vocab_size(),MODEL_HIDDEN_SIZE,MODEL_NUM_LAYERS,MODEL_NUM_ATTENTION_HEADS,MODEL_INTERMEDIATE_SIZE,MAX_LEN,PAD_TOKEN_ID)
        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            state = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            base_state = {k.replace("bert_base.",""):v for k,v in state.items() if k.startswith("bert_base.")}
            if base_state: base_nli.load_state_dict(base_state); print("Backbone BERT carregado para NLI.")
        
        model_nli = BERTForSequencePairClassification(base_nli, MODEL_HIDDEN_SIZE, ft_train_ds_nli.num_classes).to(DEVICE)
        opt_nli = torch.optim.AdamW(model_nli.parameters(), lr=FINETUNE_LR, betas=OPTIMIZER_BETAS, eps=OPTIMIZER_EPS, weight_decay=WEIGHT_DECAY)
        crit_nli = nn.CrossEntropyLoss()
        trainer_nli = GenericTrainer(model_nli, ft_train_dl_nli, ft_val_dl_nli, opt_nli, crit_nli, DEVICE, ASSIN_NLI_MODEL_SAVE_FILENAME, "nli")
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS)
    else: print("Nenhum dado de treino para ASSIN NLI. Pulando.")
except Exception as e: print(f"Erro Bloco 10 (ASSIN NLI): {e}"); import traceback; traceback.print_exc()

# --- Bloco 11: Fine-tuning e Avaliação no HAREM (NER) ---
print("\n--- Bloco 11: Fine-tuning HAREM (NER) ---")
HAREM_MODEL_SAVE_FILENAME = "aroeira_harem_ner_model.pth"
HAREM_SUBSET_SIZE = 250

class BERTForTokenClassification(nn.Module): # Modelo para NER
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_labels):
        super().__init__(); self.bert_base = bert_base; self.drop = nn.Dropout(0.1)
        self.clf = nn.Linear(hidden_size, num_labels)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.clf(self.drop(self.bert_base(input_ids, attention_mask, segment_ids)))

class HAREMNERDataset(Dataset): # Dataset para HAREM NER
    def __init__(self, hf_ds, tokenizer, id2label_map, max_len, pad_label_id):
        self.tok, self.id2lbl, self.max_len, self.pad_lbl_id = tokenizer, id2label_map, max_len, pad_label_id
        self.encs, self.lbls_aligned = [], []
        proc_count, skip_count = 0,0
        for i, ex in enumerate(tqdm(hf_ds, desc="Processando HAREM", file=sys.stdout, mininterval=1.0)):
            orig_toks, orig_ner_ids = ex.get("tokens"), ex.get("ner_tags")
            if not orig_toks or orig_ner_ids is None or len(orig_toks)!=len(orig_ner_ids): skip_count+=1; continue
            enc = self.tok.encode(" ".join(orig_toks)); word_ids = enc.word_ids; aligned_lbls = []
            prev_word_idx = None; err_align = False
            for word_idx in word_ids:
                if word_idx is None: aligned_lbls.append(self.pad_lbl_id)
                elif word_idx != prev_word_idx:
                    if 0 <= word_idx < len(orig_ner_ids): aligned_lbls.append(orig_ner_ids[word_idx])
                    else: aligned_lbls.append(self.pad_lbl_id); err_align=True; print(f"HAREM Alinhamento: idx {word_idx} fora dos limites para ex {i}."); break
                else: aligned_lbls.append(self.pad_lbl_id)
                prev_word_idx = word_idx
            if err_align or len(enc.ids) != len(aligned_lbls): skip_count+=1; continue
            self.encs.append(enc); self.lbls_aligned.append(aligned_lbls); proc_count+=1
        print(f"HAREM Dataset: {proc_count} processados, {skip_count} ignorados.")
        if proc_count == 0 and len(hf_ds) > 0: print("AVISO SÉRIO: Nenhum dado HAREM processado.")
    def __len__(self): return len(self.encs)
    def __getitem__(self, idx):
        enc, lbls = self.encs[idx], self.lbls_aligned[idx]
        return {"input_ids":torch.tensor(enc.ids),"attention_mask":torch.tensor(enc.attention_mask,dtype=torch.long),
                "segment_ids":torch.zeros(len(enc.ids),dtype=torch.long),"labels":torch.tensor(lbls,dtype=torch.long)}
try:
    harem_raw = datasets.load_dataset("harem", "selective", trust_remote_code=True)
    ner_feat = harem_raw["train"].features["ner_tags"].feature
    id2lbl_harem = {i:n for i,n in enumerate(ner_feat.names)}; NUM_NER_TAGS = len(id2lbl_harem)
    print(f"HAREM Tags (primeiras 5 de {NUM_NER_TAGS}): {list(id2lbl_harem.items())[:5]}")
    train_harem_full = harem_raw["train"]
    train_harem_sub = train_harem_full
    if HAREM_SUBSET_SIZE and HAREM_SUBSET_SIZE < len(train_harem_full): # Adicionada checagem
        train_harem_sub = train_harem_full.select(range(HAREM_SUBSET_SIZE))
    print(f"HAREM Treino (subset): {len(train_harem_sub)}")
    
    ft_train_ds_ner = HAREMNERDataset(train_harem_sub, tokenizer, id2lbl_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
    if len(ft_train_ds_ner) == 0: raise ValueError("Dataset HAREM vazio após processamento.")
    ft_train_dl_ner = DataLoader(ft_train_ds_ner, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
    ft_val_dl_ner = None # HAREM selective não tem val_split. Poderia criar um do train.

    base_ner = BERTBaseModel(tokenizer.get_vocab_size(),MODEL_HIDDEN_SIZE,MODEL_NUM_LAYERS,MODEL_NUM_ATTENTION_HEADS,MODEL_INTERMEDIATE_SIZE,MAX_LEN,PAD_TOKEN_ID)
    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        state = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        base_state = {k.replace("bert_base.",""):v for k,v in state.items() if k.startswith("bert_base.")}
        if base_state: base_ner.load_state_dict(base_state); print("Backbone BERT carregado para NER.")
            
    model_ner = BERTForTokenClassification(base_ner, MODEL_HIDDEN_SIZE, NUM_NER_TAGS).to(DEVICE)
    opt_ner = torch.optim.AdamW(model_ner.parameters(), lr=FINETUNE_LR, betas=OPTIMIZER_BETAS, eps=OPTIMIZER_EPS, weight_decay=WEIGHT_DECAY)
    crit_ner = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_LABEL_ID_NER)
    trainer_ner = GenericTrainer(model_ner, ft_train_dl_ner, ft_val_dl_ner, opt_ner, crit_ner, DEVICE, HAREM_MODEL_SAVE_FILENAME, "ner", NUM_NER_TAGS, id2lbl_harem)
    trainer_ner.train(num_epochs=FINETUNE_EPOCHS)
except Exception as e: print(f"Erro Bloco 11 (HAREM NER): {e}"); import traceback; traceback.print_exc(); raise

print("\n--- Script Finalizado ---")
