# -*- coding: utf-8 -*-

import boto3
import botocore.exceptions
import json
import re
import time
import logging
from datasets import load_dataset
from tqdm import tqdm

# --- 1. CONFIGURAÇÕES ---

# Configurações do Amazon Bedrock
MODEL_ID = 'meta.llama3-8b-instruct-v1:0'
REGION_NAME = 'us-east-1' # Ajuste para sua região, se necessário
client = boto3.client(service_name='bedrock-runtime', region_name=REGION_NAME)

# Configurações do Dataset (como antes)
DATASET_NAME = "McGill-NLP/stereoset"
CONFIGS = ['intersentence', 'intrasentence']
DATASET_SPLIT = "validation"

# Mapeamentos de labels (como antes)
GOLD_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated'}
INNER_LABEL_MAP = {0: 'stereotype', 1: 'anti-stereotype', 2: 'unrelated', 3: 'related'}

# Configura o logging para ver avisos de throttling
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# --- 2. FUNÇÃO DE CHAMADA DO BEDROCK (NOVA) ---

def invoke_llama_translation(text_to_translate):
    """
    Chama o modelo Llama 3 8B no Bedrock para traduzir um único texto.
    Inclui lógica de retentativa para throttling.
    """
    # Prompt de sistema otimizado para tradução direta
    system_prompt = (
        "Você é um tradutor especialista. Traduza o texto de Inglês para Português do Brasil. "
        "Responda *apenas* com o texto traduzido, sem frases introdutórias, explicações, "
        "aspas ou qualquer outro texto."
    )
    
    # Formato do prompt para Llama 3 Instruct
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
Texto em Inglês: "{text_to_translate}"<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""
    
    body = json.dumps({
        "prompt": prompt,
        "max_gen_len": 512, # Suficiente para uma sentença
        "temperature": 0.1,  # Baixa temperatura para tradução mais precisa/determinística
        "top_p": 0.9
    })

    retries = 3
    delay = 5
    for i in range(retries):
        try:
            response = client.invoke_model(modelId=MODEL_ID, body=body)
            response_body = json.loads(response['body'].read().decode('utf-8'))
            
            # Limpa a saída do LLM para pegar apenas a tradução
            translated_text = response_body['generation'].strip().strip('"')
            return translated_text
        
        except botocore.exceptions.ClientError as e:
            if "ThrottlingException" in str(e):
                logging.warning(f"ThrottlingException... retentativa em {delay}s. Texto: {text_to_translate[:50]}...")
                time.sleep(delay)
                delay *= 2 # Backoff exponencial
            else:
                logging.error(f"Erro do cliente Bedrock: {e}")
                return f"ERRO_DE_TRADUCAO: {e}" # Retorna um erro
        except Exception as e:
            logging.error(f"Erro ao invocar o modelo: {e}")
            return f"ERRO_DE_TRADUCAO: {e}"
    
    return "ERRO_DE_TRADUCAO: Excedeu retentativas de throttling."


# --- 3. FUNÇÃO AUXILIAR (COMO ANTES) ---

def sanitize_text(text):
    """Limpa o texto, removendo caracteres de controle que podem quebrar o JSON."""
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', text)


# --- 4. FUNÇÃO PRINCIPAL DE TRADUÇÃO (MODIFICADA) ---

def traduzir_e_recriar_estrutura_com_llm():
    """
    Executa o pipeline completo de tradução usando o Bedrock
    e recria a estrutura original do Stereoset.
    """
    
    # --- ETAPA DE EXTRAÇÃO (COMO ANTES) ---
    datasets_dict = {}
    sentences_to_translate = []
    for config in CONFIGS:
        print(f"Carregando a configuração '{config}' do dataset...")
        dataset = load_dataset(DATASET_NAME, config, split=DATASET_SPLIT, keep_in_memory=True)
        datasets_dict[config] = dataset
        for example in dataset:
            sentences_to_translate.append(example['context'])
            sentences_to_translate.extend(example['sentences']['sentence'])
    
    print(f"Total de {len(sentences_to_translate)} sentenças extraídas para tradução.")

    # --- ETAPA DE TRADUÇÃO (MODIFICADA PARA USAR BEDROCK) ---
    print(f"Iniciando a tradução via Bedrock (Modelo: {MODEL_ID})...")
    translated_sentences = []

    # Itera sentença por sentença para chamar a API
    # Isso será significativamente mais lento que o NLLB local.
    for sentence_to_translate in tqdm(sentences_to_translate, desc="Traduzindo sentenças"):
        translated_text_raw = invoke_llama_translation(sentence_to_translate)
        
        # Sanitiza a tradução recebida da API
        batch_sanitized = [sanitize_text(translated_text_raw)]
        translated_sentences.extend(batch_sanitized)

    print("Tradução finalizada.")

    # --- ETAPA DE RECONSTRUÇÃO MANUAL (COMO ANTES) ---
    print("Reconstruindo o dataset na estrutura original...")
    translated_iter = iter(translated_sentences)
    
    reconstructed_data = {}
    for config in CONFIGS:
        original_dataset = datasets_dict[config]
        new_examples_list = []
        for original_example in tqdm(original_dataset, desc=f"Reconstruindo {config}"):
            new_example = {
                "id": original_example['id'],
                "bias_type": original_example['bias_type'],
                "target": original_example['target'],
                "context": next(translated_iter),
                "sentences": []
            }
            
            original_sents_data = original_example['sentences']
            num_sentences = len(original_sents_data['sentence'])

            for i in range(num_sentences):
                recreated_labels = []
                labels_data_for_one_sentence = original_sents_data['labels'][i]
                human_ids = labels_data_for_one_sentence['human_id']
                inner_int_labels = labels_data_for_one_sentence['label']
                
                for j in range(len(human_ids)):
                    recreated_labels.append({
                        "human_id": human_ids[j],
                        "label": INNER_LABEL_MAP[inner_int_labels[j]]
                    })

                new_sentence_obj = {
                    "id": original_sents_data['id'][i],
                    "sentence": next(translated_iter),
                    "labels": recreated_labels,
                    "gold_label": GOLD_LABEL_MAP[original_sents_data['gold_label'][i]]
                }
                new_example["sentences"].append(new_sentence_obj)
            
            new_examples_list.append(new_example)
        reconstructed_data[config] = new_examples_list

    # --- ETAPA DE SALVAMENTO (COMO ANTES) ---
    final_output_structure = {
        "version": "1.1",
        "data": reconstructed_data
    }
    
    output_path = f"stereoset_{DATASET_SPLIT}_pt_llama3_formato_original.json" # Nome de arquivo atualizado
    print(f"Salvando o dataset final em: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_output_structure, f, ensure_ascii=False, indent=2)

    print("\n✅ Sucesso! O arquivo de saída (traduzido via LLM) é compatível com o dataloader.py.")


# --- 5. EXECUÇÃO ---
if __name__ == "__main__":
    traduzir_e_recriar_estrutura_com_llm()
