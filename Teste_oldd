# --- Bloco 12: Teste Interativo do Modelo MLM Pré-treinado ---
# Este bloco deve ser executado APÓS o Bloco 9 ter treinado e salvo o modelo.

print("\n--- Bloco 12: Teste Interativo do Modelo MLM (Estilo Artigo) ---")

# Verifica se as definições necessárias e variáveis existem
if 'ArticleBERT' not in globals() or 'ArticleBERTLMWithHeads' not in globals():
    print("ERRO: As classes de modelo ArticleBERT ou ArticleBERTLMWithHeads não estão definidas. Execute o Bloco 6.")
elif 'tokenizer_pt' not in globals() or tokenizer_pt is None:
    print("ERRO: O 'tokenizer_pt' não está definido. Execute o Bloco 4.")
elif 'PRETRAINED_BERT_SAVE_FILENAME' not in globals() or \
     'MODEL_D_MODEL' not in globals() or \
     'MODEL_N_LAYERS' not in globals() or \
     'MODEL_HEADS' not in globals() or \
     'MAX_LEN' not in globals() or \
     'PAD_TOKEN_ID_FOR_PRETRAIN' not in globals() or \
     'MODEL_DROPOUT' not in globals() or \
     'MODEL_FF_HIDDEN_SIZE' not in globals() or \
     'DEVICE' not in globals():
    print("ERRO: Uma ou mais constantes globais (MODEL_*, MAX_LEN, PAD_TOKEN_ID_FOR_PRETRAIN, DEVICE) não estão definidas. Execute o Bloco 2.")
else:
    try:
        print(f"Carregando modelo pré-treinado de: {PRETRAINED_BERT_SAVE_FILENAME}")
        # Recriar a arquitetura do modelo BERT base (backbone)
        loaded_bert_article_backbone = ArticleBERT(
            vocab_sz=tokenizer_pt.get_vocab_size(),
            d_model=MODEL_D_MODEL,
            n_layers=MODEL_N_LAYERS,
            heads=MODEL_HEADS,
            seq_len=MAX_LEN,
            pad_idx=PAD_TOKEN_ID_FOR_PRETRAIN, 
            dropout_rate=MODEL_DROPOUT,
            ff_h_size=MODEL_FF_HIDDEN_SIZE
        )
        # Recriar o modelo completo com as cabeças MLM e NSP
        interactive_model = ArticleBERTLMWithHeads(
            loaded_bert_article_backbone,
            tokenizer_pt.get_vocab_size()
        )
        
        if not Path(PRETRAINED_BERT_SAVE_FILENAME).exists():
            print(f"ERRO: Arquivo do modelo pré-treinado '{PRETRAINED_BERT_SAVE_FILENAME}' não encontrado. Execute o Bloco 9 (treinamento) primeiro.")
        else:
            interactive_model.load_state_dict(torch.load(PRETRAINED_BERT_SAVE_FILENAME, map_location=DEVICE))
            interactive_model.to(DEVICE)
            interactive_model.eval() # Colocar em modo de avaliação (desativa dropout, etc.)
            print("Modelo pré-treinado carregado com sucesso e em modo de avaliação.")

            def predict_mlm_pytorch_style(text_with_mask: str, model_to_test: ArticleBERTLMWithHeads, tokenizer_to_use, device_to_use: str, top_k: int = 5):
                # Tokenizar a entrada. O tokenizer_pt já está configurado com BertProcessing, padding e truncation.
                encoded = tokenizer_to_use.encode(text_with_mask)
                input_ids = torch.tensor([encoded.ids]).to(device_to_use)
                
                # Criar segment_ids para uma única sentença (ou o primeiro segmento de um par)
                # O ArticleBERTEmbedding espera 0 para padding, 1 para SentA, 2 para SentB.
                # Para MLM de sentença única, usamos segmento 1 para tokens reais, 0 para padding.
                segment_ids_list = []
                for id_val in encoded.ids:
                    if id_val == tokenizer_to_use.token_to_id("[PAD]"):
                        segment_ids_list.append(0) # Segmento 0 para PAD
                    else:
                        segment_ids_list.append(1) # Segmento 1 para tokens reais de Sentença A
                segment_ids = torch.tensor([segment_ids_list]).to(device_to_use)

                # O modelo ArticleBERT cria a attention_mask internamente a partir do input_ids e pad_idx.
                # Portanto, não precisamos passar attention_mask explicitamente para model_to_test.forward().
                
                with torch.no_grad():
                    # O modelo ArticleBERTLMWithHeads retorna (nsp_output, mlm_output)
                    _, mlm_logits = model_to_test(input_ids, segment_ids)
                    # mlm_logits tem shape (batch_size, seq_len, vocab_size)

                mask_token_id_val = tokenizer_to_use.token_to_id("[MASK]")
                mask_indices = (input_ids[0] == mask_token_id_val).nonzero(as_tuple=True)[0]

                results = []
                if mask_indices.numel() == 0:
                    print("   Nenhum token [MASK] encontrado na entrada tokenizada.")
                    return [], encoded.tokens

                for mask_idx_tensor in mask_indices:
                    mask_idx = mask_idx_tensor.item()
                    token_logits = mlm_logits[0, mask_idx, :] # Logits para esta posição [MASK]
                    
                    # Aplicar Softmax para obter probabilidades (opcional, mas bom para interpretação)
                    # Como os heads do modelo já usam LogSoftmax, podemos usar .exp() para obter probs.
                    token_probs = torch.exp(token_logits) 
                    
                    top_k_probs, top_k_ids = torch.topk(token_probs, top_k)
                    
                    top_k_tokens = [tokenizer_to_use.id_to_token(tid.item()) for tid in top_k_ids]
                    results.append({
                        "mask_original_index": mask_idx, # Posição do MASK nos input_ids
                        "predictions": list(zip(top_k_tokens, top_k_probs.cpu().tolist()))
                    })
                return results, encoded.tokens

            # Loop interativo
            print("\nDigite uma sentença com '[MASK]' para ver as previsões do MLM.")
            print("Exemplo: 'O Aroeira é um [MASK] de PLN para o português.'")
            print("Digite 'sair' para terminar.")
            
            while True:
                user_text = input("Sua sentença com [MASK]: ")
                if user_text.lower() == 'sair':
                    break
                if "[MASK]" not in user_text:
                    print("Por favor, inclua pelo menos um token [MASK] na sua sentença.")
                    continue
                
                mlm_predictions, tokenized_input_display = predict_mlm_pytorch_style(user_text, interactive_model, tokenizer_pt, DEVICE)
                
                print(f"   Entrada Tokenizada: {' '.join(tokenized_input_display)}")
                if mlm_predictions:
                    for pred_info in mlm_predictions:
                        print(f"   Previsões para [MASK] na posição (tokenizada) {pred_info['mask_original_index']}:")
                        for token, prob in pred_info['predictions']:
                            print(f"      - '{token}' (prob: {prob:.4f})")
                else:
                    print("   Não foi possível obter predições (verifique se há [MASK]s e se o modelo carregou).")
                print("-" * 30)
                
    except Exception as e_load:
        print(f"Erro ao carregar ou testar o modelo: {e_load}")
        import traceback
        traceback.print_exc()
