# --- Bloco 8: Preparando Componentes para Pré-Treinamento (Estilo Artigo) ---
print("\n--- Bloco 8: Preparando Componentes para Pré-Treinamento (Estilo Artigo) ---")

# Verifica se as sentenças do Aroeira foram carregadas
if not all_aroeira_individual_sentences:
    raise SystemExit("Lista 'all_aroeira_individual_sentences' está vazia. Verifique a execução do Bloco 3.")

# Divide as sentenças do Aroeira em conjuntos de treino e validação para o pré-treinamento
val_split_ratio_pt_art = 0.1 # Proporção para validação
num_val_pt_art = int(len(all_aroeira_individual_sentences) * val_split_ratio_pt_art)
if num_val_pt_art < 1 and len(all_aroeira_individual_sentences) > 1: # Garante ao menos 1 exemplo de validação se possível
    num_val_pt_art = 1

train_sents_pt_art = all_aroeira_individual_sentences[num_val_pt_art:]
val_sents_pt_art = all_aroeira_individual_sentences[:num_val_pt_art]

# Garante que há dados de treino, mesmo que o dataset seja muito pequeno
if not train_sents_pt_art:
    print("AVISO: Dataset de Aroeira muito pequeno, usando todos os exemplos para treino (sem validação para pré-treino).")
    train_sents_pt_art = all_aroeira_individual_sentences
    val_sents_pt_art = []

print(f"Sentenças para Pré-Treino (Estilo Artigo): Treino={len(train_sents_pt_art)}, Validação={len(val_sents_pt_art)}")

# Cria instâncias do ArticleStyleBERTDataset e DataLoaders
# Usa tokenizer_pt (definido e configurado no Bloco 4)
train_ds_pt_art = ArticleStyleBERTDataset(train_sents_pt_art, tokenizer_pt, seq_len=MAX_LEN)
train_dl_pt_art = DataLoader(train_ds_pt_art, batch_size=BATCH_SIZE_PRETRAIN, shuffle=True, num_workers=0)

val_dl_pt_art = None
if val_sents_pt_art and len(val_sents_pt_art) > 0:
    val_ds_pt_art = ArticleStyleBERTDataset(val_sents_pt_art, tokenizer_pt, seq_len=MAX_LEN)
    if len(val_ds_pt_art) > 0: # Checagem adicional se o processamento do dataset resultou em exemplos
        val_dl_pt_art = DataLoader(val_ds_pt_art, batch_size=BATCH_SIZE_PRETRAIN, shuffle=False, num_workers=0)
else:
    print("Nenhum conjunto de validação será usado para o pré-treinamento.")

# Instanciação do Modelo BERT (Estilo Artigo)
# O backbone ArticleBERT é definido no Bloco 6
article_bert_model = ArticleBERT(
    vocab_sz=tokenizer_pt.get_vocab_size(),
    d_model=MODEL_D_MODEL,
    n_layers=MODEL_N_LAYERS,
    num_heads=MODEL_HEADS, # Usa a constante MODEL_HEADS definida no Bloco 2
    seq_len=MAX_LEN,
    pad_idx_for_bert=PAD_TOKEN_ID_FOR_PRETRAIN, # PAD_TOKEN_ID do tokenizer_pt
    dropout_rate=MODEL_DROPOUT,
    ff_h_size=MODEL_FF_HIDDEN_SIZE
)

# Modelo Combinado com Cabeças MLM e NSP (definido no Bloco 6)
article_bertlm_with_heads = ArticleBERTLMWithHeads(
    article_bert_model,
    tokenizer_pt.get_vocab_size()
)

# Trainer (Estilo Artigo, definido no Bloco 7)
# Inclui o otimizador Adam e o ScheduledOptim internamente.
trainer_article = ArticleStyleBERTTrainer(
    model=article_bertlm_with_heads,
    train_dataloader=train_dl_pt_art,
    val_dataloader=val_dl_pt_art, # Pode ser None
    d_model_for_optim=MODEL_D_MODEL, # Usado pelo ScheduledOptim
    lr=LEARNING_RATE_ADAM,         # Taxa de aprendizado base para Adam
    betas=ADAM_BETAS,
    weight_decay=ADAM_WEIGHT_DECAY,
    warmup_steps=N_WARMUP_STEPS,   # Para ScheduledOptim
    device=DEVICE,
    model_save_path=PRETRAINED_BERT_SAVE_FILENAME,
    pad_idx_mlm_loss=PAD_TOKEN_ID_FOR_PRETRAIN # Para o ignore_index da NLLLoss do MLM
)
print("Componentes (estilo artigo) para pré-treinamento MLM+NSP prontos.")
