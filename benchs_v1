# --- Bloco 1: Imports Essenciais ---
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import datasets
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
from tqdm.auto import tqdm # Para barras de progresso
import random
from pathlib import Path
import sys
from sklearn.metrics import accuracy_score, f1_score, classification_report # Para NLI
from seqeval.metrics import classification_report as seqeval_classification_report # Para NER
from seqeval.scheme import IOB2 # Para NER

print(f"Versão do PyTorch: {torch.__version__}")
print(f"Versão do Datasets: {datasets.__version__}")
print(f"Versão do Tokenizers: {tokenizers.__version__}")
print(f"Versão do Seqeval: (instale com !pip install seqeval)")


# --- Bloco 2: Constantes e Configurações Iniciais ---
MAX_LEN = 128
AROEIRA_SUBSET_SIZE = 5000 # Reduzido para exemplo completo mais rápido
VOCAB_SIZE = 30000
MIN_FREQUENCY_TOKENIZER = 2 # Reduzido para datasets menores

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
EPOCHS_PRETRAIN = 1 # Reduzido para exemplo
BATCH_SIZE_PRETRAIN = 8 # Reduzido

MODEL_HIDDEN_SIZE = 256
MODEL_NUM_LAYERS = 2
MODEL_NUM_ATTENTION_HEADS = 4
MODEL_INTERMEDIATE_SIZE = MODEL_HIDDEN_SIZE * 4

TOKENIZER_VOCAB_FILENAME = "aroeira_mlm_tokenizer-vocab.json"
TOKENIZER_MERGES_FILENAME = "aroeira_mlm_tokenizer-merges.txt"
PRETRAINED_BERTLM_SAVE_FILENAME = "aroeira_bertlm_pretrained.pth"
TEMP_TOKENIZER_TRAIN_FILE = Path("temp_aroeira_for_tokenizer.txt")

# Constantes para Fine-tuning
FINETUNE_EPOCHS = 2 # Poucas épocas para demonstração
FINETUNE_BATCH_SIZE = 8 # Batch size menor para fine-tuning
FINETUNE_LR = 3e-5

print(f"Dispositivo: {DEVICE}")
print(f"Subconjunto Aroeira para Pré-Treino: {AROEIRA_SUBSET_SIZE if AROEIRA_SUBSET_SIZE is not None else 'Completo'}")

# --- Bloco 3: Carregando e Pré-processando o Corpus Aroeira (Streaming) ---
print("\n--- Bloco 3: Carregando e Pré-processando Dados do Aroeira ---")
all_aroeira_sentences = []
try:
    streamed_aroeira_dataset = datasets.load_dataset("Itau-Unibanco/aroeira",split="train",streaming=True,trust_remote_code=True)
    text_column_name = "text"
    if AROEIRA_SUBSET_SIZE is not None:
        stream_iterator = iter(streamed_aroeira_dataset)
        temp_collected_examples = []
        for _ in range(AROEIRA_SUBSET_SIZE): temp_collected_examples.append(next(stream_iterator))
        for example in tqdm(temp_collected_examples, desc="Extraindo sentenças Aroeira", file=sys.stdout, total=len(temp_collected_examples)):
            sentence = example.get(text_column_name); all_aroeira_sentences.append(sentence.strip()) if isinstance(sentence, str) and sentence.strip() else None
    else:
        for example in tqdm(streamed_aroeira_dataset, desc="Extraindo sentenças Aroeira (stream completo)", file=sys.stdout):
            sentence = example.get(text_column_name); all_aroeira_sentences.append(sentence.strip()) if isinstance(sentence, str) and sentence.strip() else None
    if not all_aroeira_sentences: raise ValueError("Nenhuma sentença Aroeira extraída.")
    print(f"Total de sentenças Aroeira extraídas: {len(all_aroeira_sentences)}")
    if TEMP_TOKENIZER_TRAIN_FILE.exists(): TEMP_TOKENIZER_TRAIN_FILE.unlink()
    with open(TEMP_TOKENIZER_TRAIN_FILE, "w", encoding="utf-8") as fp:
        for sentence in all_aroeira_sentences: fp.write(sentence + "\n")
    tokenizer_train_files_list = [str(TEMP_TOKENIZER_TRAIN_FILE)]
except Exception as e: print(f"Erro Bloco 3: {e}"); raise

# --- Bloco 4: Treinando o Tokenizador ByteLevelBPE ---
print("\n--- Bloco 4: Treinando o Tokenizador ---")
if not Path(TOKENIZER_VOCAB_FILENAME).exists() or not Path(TOKENIZER_MERGES_FILENAME).exists():
    tokenizer_save_prefix = TOKENIZER_VOCAB_FILENAME.replace("-vocab.json", "")
    custom_tokenizer = ByteLevelBPETokenizer()
    custom_tokenizer.train(files=tokenizer_train_files_list, vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQUENCY_TOKENIZER, special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"])
    custom_tokenizer.save_model(".", prefix=tokenizer_save_prefix)
else: print(f"Tokenizador já existe. Carregando...")
tokenizer = ByteLevelBPETokenizer(vocab=TOKENIZER_VOCAB_FILENAME, merges=TOKENIZER_MERGES_FILENAME)
tokenizer._tokenizer.post_processor = BertProcessing(("</s>", tokenizer.token_to_id("</s>")), ("<s>", tokenizer.token_to_id("<s>")))
tokenizer.enable_truncation(max_length=MAX_LEN); tokenizer.enable_padding(pad_id=tokenizer.token_to_id("<pad>"), pad_token="<pad>", length=MAX_LEN)
PAD_TOKEN_ID = tokenizer.token_to_id("<pad>")
print(f"Vocabulário do Tokenizador: {tokenizer.get_vocab_size()}")

# --- Bloco 5: Definição do BERTMLMDataset ---
print("\n--- Bloco 5: Definindo o Dataset para MLM ---")
class BERTMLMDataset(Dataset):
    def __init__(self, sentences, tokenizer, max_len):
        self.sentences, self.tokenizer, self.max_len = sentences, tokenizer, max_len
        self.vocab_size, self.mask_id, self.cls_id, self.sep_id, self.pad_id = tokenizer.get_vocab_size(), tokenizer.token_to_id("<mask>"), tokenizer.token_to_id("<s>"), tokenizer.token_to_id("</s>"), PAD_TOKEN_ID
    def __len__(self): return len(self.sentences)
    def _mask_tokens(self, ids_orig):
        ids, lbls = list(ids_orig), list(ids_orig)
        for i, token_id in enumerate(ids):
            if token_id in [self.cls_id, self.sep_id, self.pad_id]: lbls[i] = self.pad_id; continue
            if random.random() < 0.15:
                act_prob = random.random()
                if act_prob < 0.8: ids[i] = self.mask_id
                elif act_prob < 0.9: ids[i] = random.randrange(self.vocab_size)
            else: lbls[i] = self.pad_id
        return torch.tensor(ids), torch.tensor(lbls)
    def __getitem__(self, idx):
        enc = self.tokenizer.encode(self.sentences[idx])
        masked_ids, lbls = self._mask_tokens(enc.ids)
        return {"input_ids": masked_ids, "attention_mask": torch.tensor(enc.attention_mask, dtype=torch.long), 
                "segment_ids": torch.zeros_like(masked_ids, dtype=torch.long), "mlm_labels": lbls}

# --- Bloco 6: Definição do Modelo BERT (Backbone e Modelo MLM) ---
print("\n--- Bloco 6: Definindo o Modelo BERT ---")
class BERTEmbedding(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_len, dropout_prob, pad_token_id):
        super().__init__(); self.tok_emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        self.seg_emb = nn.Embedding(2, hidden_size); self.pos_emb = nn.Embedding(max_len, hidden_size)
        self.norm = nn.LayerNorm(hidden_size, eps=1e-12); self.drop = nn.Dropout(dropout_prob)
        self.register_buffer("pos_ids", torch.arange(max_len).expand((1, -1)))
    def forward(self, input_ids, segment_ids):
        seq_len = input_ids.size(1)
        tok_e, seg_e = self.tok_emb(input_ids), self.seg_emb(segment_ids)
        pos_e = self.pos_emb(self.pos_ids[:, :seq_len])
        return self.drop(self.norm(tok_e + pos_e + seg_e))

class BERTBaseModel(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, int_size, max_len, pad_id, drop_prob=0.1):
        super().__init__()
        self.emb = BERTEmbedding(vocab_size, hidden_size, max_len, drop_prob, pad_id)
        enc_layer = nn.TransformerEncoderLayer(hidden_size, num_heads, int_size, drop_prob, 'gelu', True)
        self.enc = nn.TransformerEncoder(enc_layer, num_layers)
    def forward(self, input_ids, attention_mask, segment_ids):
        x = self.emb(input_ids, segment_ids)
        return self.enc(x, src_key_padding_mask=(attention_mask == 0))

class BERTLM(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, vocab_size, hidden_size):
        super().__init__(); self.bert_base = bert_base
        self.mlm_head = nn.Linear(hidden_size, vocab_size)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.mlm_head(self.bert_base(input_ids, attention_mask, segment_ids))

# --- Bloco 7: Trainer Simplificado (Para Pré-Treino MLM e adaptável para Fine-tuning) ---
print("\n--- Bloco 7: Definindo o Trainer ---")
class GenericTrainer: # Renomeado para ser mais genérico
    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, device, 
                 model_save_path, task_type="mlm", vocab_size=None, id2label=None):
        self.model, self.train_dl, self.val_dl = model.to(device), train_dataloader, val_dataloader
        self.opt, self.crit, self.dev = optimizer, criterion, device
        self.save_path, self.best_val_metric = model_save_path, float('inf') if task_type != "ner_f1" else float('-inf')
        self.task_type, self.vocab_size, self.id2label = task_type, vocab_size, id2label
        if task_type == "ner_f1" and id2label is None: raise ValueError("id2label é necessário para ner_f1")

    def _run_epoch(self, epoch_num, is_training):
        self.model.train(is_training)
        dl = self.train_dl if is_training else self.val_dl
        if not dl: return None, {}
        total_loss, all_preds_epoch, all_labels_epoch = 0, [], []
        desc = f"Epoch {epoch_num+1} [{'Train' if is_training else 'Val'}]"
        progress_bar = tqdm(dl, total=len(dl), desc=desc, file=sys.stdout)

        for batch in progress_bar:
            input_ids = batch["input_ids"].to(self.dev); attention_mask = batch["attention_mask"].to(self.dev)
            segment_ids = batch.get("segment_ids", torch.zeros_like(input_ids)).to(self.dev) # Default para NER
            labels = batch["labels"].to(self.dev) # Nome genérico para labels

            if is_training: self.opt.zero_grad()
            with torch.set_grad_enabled(is_training):
                logits = self.model(input_ids, attention_mask, segment_ids)
                if self.task_type == "mlm" or self.task_type == "ner":
                    loss = self.crit(logits.view(-1, logits.size(-1)), labels.view(-1))
                elif self.task_type == "nli": # Classificação de sequência
                    loss = self.crit(logits, labels)
            if is_training: loss.backward(); self.opt.step()
            total_loss += loss.item()
            progress_bar.set_postfix({"loss": f"{total_loss / (progress_bar.n + 1):.4f}"})

            if not is_training: # Coleta predições e labels para métricas de validação
                if self.task_type == "nli":
                    all_preds_epoch.extend(torch.argmax(logits, dim=-1).cpu().numpy())
                    all_labels_epoch.extend(labels.cpu().numpy())
                elif self.task_type == "ner":
                    preds_ner = torch.argmax(logits, dim=-1).cpu().numpy()
                    labels_ner = labels.cpu().numpy()
                    # Converter IDs para labels de string, ignorando padding/special tokens para seqeval
                    for p_seq, l_seq, mask_seq in zip(preds_ner, labels_ner, attention_mask.cpu().numpy()):
                        all_preds_epoch.append([self.id2label[p] for p, l, m in zip(p_seq, l_seq, mask_seq) if m == 1 and l != self.crit.ignore_index])
                        all_labels_epoch.append([self.id2label[l] for l, m in zip(l_seq, mask_seq) if m == 1 and l != self.crit.ignore_index])
        
        avg_loss = total_loss / len(dl)
        metrics = {"loss": avg_loss}
        if not is_training:
            if self.task_type == "nli":
                metrics["accuracy"] = accuracy_score(all_labels_epoch, all_preds_epoch)
                metrics["f1_weighted"] = f1_score(all_labels_epoch, all_preds_epoch, average='weighted', zero_division=0)
            elif self.task_type == "ner" and all_labels_epoch: # Garante que há o que avaliar
                 # Remove listas vazias que podem surgir se uma sentença inteira for padding
                all_preds_epoch_filt = [p for p in all_preds_epoch if p]
                all_labels_epoch_filt = [l for l in all_labels_epoch if l]
                if all_preds_epoch_filt and all_labels_epoch_filt : # Só calcula se houver o que calcular
                    report = seqeval_classification_report(all_labels_epoch_filt, all_preds_epoch_filt, output_dict=True, zero_division=0, mode='strict', scheme=IOB2)
                    metrics["ner_f1_micro"] = report['micro avg']['f1-score'] # Usar micro para NER é comum

        print_metrics = f"{desc} - Avg Loss: {avg_loss:.4f}"
        for k, v in metrics.items():
            if k != "loss": print_metrics += f", {k}: {v:.4f}"
        print(print_metrics)
        return avg_loss, metrics

    def train(self, num_epochs):
        action_metric = "ner_f1_micro" if self.task_type == "ner" else "loss" # O que otimizar
        print(f"Iniciando treinamento ({self.task_type}) por {num_epochs} épocas. Otimizando para {action_metric}.")
        for epoch in range(num_epochs):
            self._run_epoch(epoch, is_training=True)
            val_loss, val_metrics = self._run_epoch(epoch, is_training=False)
            
            current_metric_val = val_metrics.get(action_metric) if val_metrics else None
            if current_metric_val is not None:
                improved = (current_metric_val < self.best_val_metric) if action_metric == "loss" else \
                           (current_metric_val > self.best_val_metric)
                if improved:
                    self.best_val_metric = current_metric_val
                    print(f"Nova melhor métrica de validação ({action_metric}): {self.best_val_metric:.4f}. Salvando modelo em {self.save_path}")
                    torch.save(self.model.state_dict(), self.save_path)
            elif epoch == num_epochs - 1: # Sem validação, salva na última época
                print(f"Treinamento ({self.task_type}) sem validação ou métrica alvo. Salvando modelo da última época em {self.save_path}")
                torch.save(self.model.state_dict(), self.save_path)
            print("-" * 30)
        print(f"Treinamento ({self.task_type}) concluído. Melhor métrica ({action_metric}): {self.best_val_metric:.4f}")

# --- Bloco 8: Preparando e Executando o Pré-Treinamento MLM ---
print("\n--- Bloco 8: Pré-Treinamento MLM ---")
# ... (código para criar train_loader_mlm, val_loader_mlm, bert_base_model, bertlm_model_for_pretrain, optimizer_mlm, criterion_mlm - mantido igual ao anterior)
val_split_ratio = 0.1; num_val_samples = int(len(all_aroeira_sentences) * val_split_ratio)
if num_val_samples < 1 and len(all_aroeira_sentences) > 1: num_val_samples = 1
train_sentences_mlm = all_aroeira_sentences[num_val_samples:]; val_sentences_mlm = all_aroeira_sentences[:num_val_samples]
if not train_sentences_mlm: train_sentences_mlm = all_aroeira_sentences; val_sentences_mlm = []
train_dataset_mlm = BERTMLMDataset(train_sentences_mlm, tokenizer, max_len=MAX_LEN)
train_loader_mlm = DataLoader(train_dataset_mlm, batch_size=BATCH_SIZE_PRETRAIN, shuffle=True, num_workers=0)
val_loader_mlm = None
if val_sentences_mlm:
    val_dataset_mlm = BERTMLMDataset(val_sentences_mlm, tokenizer, max_len=MAX_LEN)
    val_loader_mlm = DataLoader(val_dataset_mlm, batch_size=BATCH_SIZE_PRETRAIN, shuffle=False, num_workers=0)

bert_base_model_pt = BERTBaseModel(
    vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS, 
    num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE, 
    max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID
)
bertlm_model_for_pretrain = BERTLM(bert_base_model_pt, tokenizer.get_vocab_size(), MODEL_HIDDEN_SIZE)
optimizer_mlm = torch.optim.AdamW(bertlm_model_for_pretrain.parameters(), lr=LEARNING_RATE_PRETRAIN)
criterion_mlm = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)

trainer_mlm = GenericTrainer(
    model=bertlm_model_for_pretrain, train_dataloader=train_loader_mlm, val_dataloader=val_loader_mlm,
    optimizer=optimizer_mlm, criterion=criterion_mlm, device=DEVICE,
    model_save_path=PRETRAINED_BERTLM_SAVE_FILENAME, task_type="mlm", vocab_size=tokenizer.get_vocab_size()
)
try:
    trainer_mlm.train(num_epochs=EPOCHS_PRETRAIN)
except Exception as e: print(f"Erro pré-treinamento MLM: {e}"); raise


# --- Bloco 10: Fine-tuning e Avaliação no ASSIN 2 (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN 2 (NLI/RTE) ---")
ASSIN2_NLI_MODEL_SAVE_FILENAME = "aroeira_assin2_nli_model.pth"
ASSIN2_SUBSET_SIZE = 500 # Usar subset pequeno para demonstração

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes)
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :] # Output do token [CLS] (<s>)
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN 2 NLI
class ASSIN2NLIDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, max_len):
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        label_map = {"Entailment": 0, "Contradiction": 1, "None": 2} # Mapeamento para ASSIN2 RTE
        for example in hf_dataset_split:
            self.premises.append(example["premise"])
            self.hypotheses.append(example["hypothesis"])
            self.labels.append(label_map[example["entailment_judgment"]])
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx]) # Tokeniza par
        # Segment IDs precisam ser criados manualmente para pares
        # 0 para premissa (incluindo <s> inicial e </s> da premissa)
        # 1 para hipótese (incluindo </s> da hipótese)
        sep_id = self.tokenizer.token_to_id("</s>")
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list)
        try:
            # Encontra o primeiro SEP (após premissa)
            sep_idx = input_ids_list.index(sep_id, 1) # Procura SEP a partir do segundo token
            for i in range(sep_idx + 1, len(input_ids_list)):
                if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"): # Não marca PAD como segmento 1
                    segment_ids_list[i] = 1
        except ValueError: # Caso raro de não encontrar SEP se truncado antes, ou só 1 sentença
            pass

        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

try:
    print("Carregando ASSIN 2 (RTE)...") # RTE é NLI
    assin2_nli_raw = datasets.load_dataset("assin", "assin2-rte", trust_remote_code=True) # Confiar no código remoto se necessário
    
    # Usar subsets pequenos para demonstração
    assin2_nli_train_hf = datasets.Dataset.from_dict(assin2_nli_raw['train'][:ASSIN2_SUBSET_SIZE])
    assin2_nli_val_hf = datasets.Dataset.from_dict(assin2_nli_raw['validation'][:max(10, int(ASSIN2_SUBSET_SIZE*0.2))]) # Pelo menos 10 para val

    ft_assin2_nli_train_dataset = ASSIN2NLIDataset(assin2_nli_train_hf, tokenizer, max_len=MAX_LEN)
    ft_assin2_nli_train_loader = DataLoader(ft_assin2_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
    ft_assin2_nli_val_loader = DataLoader(ASSIN2NLIDataset(assin2_nli_val_hf, tokenizer, max_len=MAX_LEN), batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)

    # Carregar backbone pré-treinado e criar modelo NLI
    bert_base_for_nli = BERTBaseModel(
        vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
        num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
        max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)
    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        bert_base_state_dict = {k.replace("bert_base.", ""): v for k, v in bertlm_state_dict.items() if k.startswith("bert_base.")}
        if bert_base_state_dict: bert_base_for_nli.load_state_dict(bert_base_state_dict)
        else: print("AVISO (NLI): Pesos do backbone não encontrados no arquivo salvo.")
    else: print(f"AVISO (NLI): {PRETRAINED_BERTLM_SAVE_FILENAME} não encontrado. Usando backbone aleatório.")

    assin2_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=3).to(DEVICE) # 3 classes para NLI
    optimizer_nli = torch.optim.AdamW(assin2_nli_model.parameters(), lr=FINETUNE_LR)
    criterion_nli = nn.CrossEntropyLoss()

    trainer_nli = GenericTrainer(
        model=assin2_nli_model, train_dataloader=ft_assin2_nli_train_loader, val_dataloader=ft_assin2_nli_val_loader,
        optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
        model_save_path=ASSIN2_NLI_MODEL_SAVE_FILENAME, task_type="nli"
    )
    trainer_nli.train(num_epochs=FINETUNE_EPOCHS)
except Exception as e: print(f"Erro no Bloco 10 (ASSIN 2 NLI): {e}"); import traceback; traceback.print_exc()


# --- Bloco 11: Fine-tuning e Avaliação no HAREM (NER) ---
print("\n--- Bloco 11: Fine-tuning e Avaliação HAREM (NER) ---")
HAREM_MODEL_SAVE_FILENAME = "aroeira_harem_ner_model.pth"
HAREM_SUBSET_SIZE = 200 # Mini HAREM 'selective' já é pequeno

# 11.1. Modelo para Classificação de Tokens (NER)
class BERTForTokenClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_labels):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_labels)
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        return self.classifier(self.dropout(sequence_output)) # Aplica classificador em cada token

# 11.2. Dataset para HAREM NER (Complexo devido ao alinhamento de labels)
# Função para alinhar labels com sub-tokens
def align_labels_with_tokens(labels, word_ids, pad_token_label_id=-100):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id is None: # Token especial (CLS, SEP, PAD do tokenizer)
            new_labels.append(pad_token_label_id)
        elif word_id != current_word: # Começo de uma nova palavra
            current_word = word_id
            new_labels.append(labels[word_id])
        else: # Sub-token da mesma palavra
            # Opção 1: Atribuir o mesmo label (se não for IOB, pode ser ok)
            # Opção 2: Atribuir um label especial como pad_token_label_id para sub-tokens
            # Para IOB/BIOES, o primeiro sub-token recebe B-TAG, os outros I-TAG.
            # Aqui, para simplificar o exemplo de alinhamento, vamos usar pad_token_label_id para sub-tokens
            # (ou seja, só o primeiro sub-token da palavra original terá o label)
            label = labels[word_id]
            if isinstance(label, int) and label_map_harem_inv.get(label,"").startswith("B-"):
                 # Se o label original era B-, os subsequentes são I-
                 # Isso requer que os labels já estejam como IDs.
                 # Por simplicidade, vamos atribuir -100. Uma implementação completa de NER faria B-/I-
                 new_labels.append(pad_token_label_id)
            else:
                 new_labels.append(pad_token_label_id) # Simplificado: só o primeiro subtoken tem label
    return new_labels


class HAREMNERDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, label_map, max_len, pad_token_label_id=-100):
        self.tokenizer, self.label_map, self.max_len, self.pad_token_label_id = tokenizer, label_map, max_len, pad_token_label_id
        self.all_tokens, self.all_labels_as_ids = [], []

        for example in hf_dataset_split:
            tokens = example["tokens"] # Lista de palavras
            ner_tags_str = example["ner_tags"] # Lista de labels (strings) como "PESSOA", "LOCAL"
            
            # Tokenizar e alinhar labels
            # Isso é complexo: tokenizer.encode() em toda a frase e depois alinhar.
            # Ou tokenizar palavra por palavra e juntar.
            # Para este exemplo, vamos tokenizar a frase inteira.
            # O tokenizer adiciona CLS/SEP
            # A função word_ids() do encoding ajuda a mapear sub-tokens para palavras originais.

            # Juntar tokens para formar a frase para o tokenizer
            text_for_tokenizer = " ".join(tokens)
            encoding = self.tokenizer.encode(text_for_tokenizer)
            
            word_ids = encoding.word_ids # Mapeia cada sub-token para o índice da palavra original
            
            current_labels_ids = [self.label_map[tag] for tag in ner_tags_str]
            
            # Alinhar labels com os sub-tokens
            aligned_label_ids = []
            previous_word_idx = None
            for word_idx in word_ids:
                if word_idx is None: # Token especial (CLS, SEP)
                    aligned_label_ids.append(self.pad_token_label_id)
                elif word_idx != previous_word_idx: # Primeiro sub-token de uma palavra
                    aligned_label_ids.append(current_labels_ids[word_idx])
                else: # Sub-token subsequente da mesma palavra (ignorar no loss)
                    aligned_label_ids.append(self.pad_token_label_id)
                previous_word_idx = word_idx
            
            # Garantir que os labels alinhados tenham o mesmo tamanho que input_ids (após padding/truncation)
            # O encoding já está no MAX_LEN com padding
            if len(aligned_label_ids) > self.max_len:
                aligned_label_ids = aligned_label_ids[:self.max_len]
            elif len(aligned_label_ids) < self.max_len:
                aligned_label_ids.extend([self.pad_token_label_id] * (self.max_len - len(aligned_label_ids)))

            self.all_tokens.append(encoding) # Salva o objeto Encoding completo
            self.all_labels_as_ids.append(aligned_label_ids)

    def __len__(self): return len(self.all_tokens)
    def __getitem__(self, idx):
        encoding = self.all_tokens[idx]
        labels = self.all_labels_as_ids[idx]
        return {"input_ids": torch.tensor(encoding.ids),
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.zeros(len(encoding.ids), dtype=torch.long), # Default para NER
                "labels": torch.tensor(labels, dtype=torch.long)}

try:
    print("Carregando HAREM ('selective' config)...")
    # 'selective' é menor e mais focado, 'total' é maior
    # Para HAREM, precisamos de um tagset consistente.
    # O dataset do Hub já fornece `ner_tags` como IDs numéricos e `ner_features.feature.names`
    harem_raw = datasets.load_dataset("harem", "selective", trust_remote_code=True) 
    
    # Criar mapeamento de labels para HAREM
    # A feature ner_tags já é uma ClassLabel, podemos pegar os nomes dela.
    harem_ner_feature = harem_raw["train"].features["ner_tags"].feature
    label_map_harem = {name: i for i, name in enumerate(harem_ner_feature.names)}
    id2label_harem = {i: name for i, name in enumerate(harem_ner_feature.names)}
    NUM_NER_TAGS = len(label_map_harem)
    PAD_TOKEN_LABEL_ID_NER = -100 # ID padrão para ignorar no loss de token classification

    print(f"Mapeamento de Labels HAREM (primeiros 5): {list(label_map_harem.items())[:5]}")
    print(f"Total de tags NER HAREM: {NUM_NER_TAGS}")

    # Usar subsets MUITO pequenos para demonstração devido à complexidade do processamento
    harem_train_hf = harem_raw["train"].select(range(min(HAREM_SUBSET_SIZE, len(harem_raw["train"]))))
    # HAREM 'selective' não tem split de validação, então vamos criar um a partir do treino
    if len(harem_train_hf) > 10: # Precisa de dados suficientes para split
        train_val_split_harem = harem_train_hf.train_test_split(test_size=0.2, shuffle=True, seed=42)
        harem_train_hf_final = train_val_split_harem["train"]
        harem_val_hf_final = train_val_split_harem["test"]
    else:
        harem_train_hf_final = harem_train_hf
        harem_val_hf_final = None # Sem validação se muito pequeno

    ft_harem_train_dataset = HAREMNERDataset(harem_train_hf_final, tokenizer, label_map_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
    ft_harem_train_loader = DataLoader(ft_harem_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
    
    ft_harem_val_loader = None
    if harem_val_hf_final and len(harem_val_hf_final) > 0:
        ft_harem_val_dataset = HAREMNERDataset(harem_val_hf_final, tokenizer, label_map_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
        ft_harem_val_loader = DataLoader(ft_harem_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)


    # Carregar backbone pré-treinado e criar modelo NER
    bert_base_for_ner = BERTBaseModel(
        vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
        num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
        max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)
    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        bert_base_state_dict = {k.replace("bert_base.", ""): v for k, v in bertlm_state_dict.items() if k.startswith("bert_base.")}
        if bert_base_state_dict: bert_base_for_ner.load_state_dict(bert_base_state_dict)
        else: print("AVISO (NER): Pesos do backbone não encontrados no arquivo salvo.")
    else: print(f"AVISO (NER): {PRETRAINED_BERTLM_SAVE_FILENAME} não encontrado. Usando backbone aleatório.")

    harem_ner_model = BERTForTokenClassification(bert_base_for_ner, MODEL_HIDDEN_SIZE, NUM_NER_TAGS).to(DEVICE)
    optimizer_ner = torch.optim.AdamW(harem_ner_model.parameters(), lr=FINETUNE_LR)
    criterion_ner = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_LABEL_ID_NER) # Ignorar -100

    trainer_ner = GenericTrainer(
        model=harem_ner_model, train_dataloader=ft_harem_train_loader, val_dataloader=ft_harem_val_loader,
        optimizer=optimizer_ner, criterion=criterion_ner, device=DEVICE,
        model_save_path=HAREM_MODEL_SAVE_FILENAME, task_type="ner", vocab_size=NUM_NER_TAGS, id2label=id2label_harem
    )
    trainer_ner.train(num_epochs=FINETUNE_EPOCHS)

except Exception as e: print(f"Erro no Bloco 11 (HAREM NER): {e}"); import traceback; traceback.print_exc()

print("\n--- Script Completo com Fine-tuning e Avaliação (ASSIN2 NLI, HAREM NER) Finalizado ---")



//////////////



# --- Bloco 6: Definição do Modelo BERT (Backbone e Modelo MLM) ---
print("\n--- Bloco 6: Definindo o Modelo BERT ---")
# BERTEmbedding (mantido como antes)
class BERTEmbedding(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_len, dropout_prob, pad_token_id):
        super().__init__(); self.tok_emb = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)
        self.seg_emb = nn.Embedding(2, hidden_size); self.pos_emb = nn.Embedding(max_len, hidden_size)
        self.norm = nn.LayerNorm(hidden_size, eps=1e-12); self.drop = nn.Dropout(dropout_prob)
        self.register_buffer("pos_ids", torch.arange(max_len).expand((1, -1)))
    def forward(self, input_ids, segment_ids):
        seq_len = input_ids.size(1)
        tok_e, seg_e = self.tok_emb(input_ids), self.seg_emb(segment_ids)
        pos_e = self.pos_emb(self.pos_ids[:, :seq_len])
        return self.drop(self.norm(tok_e + pos_e + seg_e))

class BERTBaseModel(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, num_heads,
                 intermediate_size, # <<< NOME DO PARÂMETRO CORRIGIDO AQUI
                 max_len_config, pad_token_id, dropout_prob=0.1):
        super().__init__()
        self.emb = BERTEmbedding(vocab_size, hidden_size, max_len_config, dropout_prob, pad_token_id)
        # nn.TransformerEncoderLayer espera d_model, nhead, dim_feedforward
        enc_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=intermediate_size, # <<< PASSANDO PARA O ARGUMENTO CORRETO
            dropout=dropout_prob,
            activation='gelu',
            batch_first=True
        )
        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)

    def forward(self, input_ids, attention_mask, segment_ids):
        x = self.emb(input_ids, segment_ids)
        padding_mask = (attention_mask == 0) # True onde deve ser ignorado
        return self.enc(x, src_key_padding_mask=padding_mask)

class BERTLM(nn.Module): # (mantido como antes)
    def __init__(self, bert_base: BERTBaseModel, vocab_size, hidden_size):
        super().__init__(); self.bert_base = bert_base
        self.mlm_head = nn.Linear(hidden_size, vocab_size)
    def forward(self, input_ids, attention_mask, segment_ids):
        return self.mlm_head(self.bert_base(input_ids, attention_mask, segment_ids))

/////////////

# --- Bloco 5: Definição do BERTMLMDataset ---
print("\n--- Bloco 5: Definindo o Dataset para MLM ---")
class BERTMLMDataset(Dataset):
    def __init__(self, sentences, tokenizer, max_len):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.vocab_size = tokenizer.get_vocab_size()
        self.mask_token_id = tokenizer.token_to_id("<mask>")
        self.cls_token_id = tokenizer.token_to_id("<s>")
        self.sep_token_id = tokenizer.token_to_id("</s>")
        self.pad_token_id = PAD_TOKEN_ID # Usando o ID global

    def __len__(self):
        return len(self.sentences)

    def _mask_tokens(self, token_ids_original):
        inputs = list(token_ids_original) # Cópia
        labels = list(token_ids_original) # Cópia para labels

        for i, token_id in enumerate(inputs):
            # Não mascarar tokens especiais
            if token_id in [self.cls_token_id, self.sep_token_id, self.pad_token_id]:
                labels[i] = self.pad_token_id # Ignorar no loss
                continue

            prob = random.random()
            if prob < 0.15: # 15% de chance de mascarar
                action_prob = random.random() # Probabilidade para decidir a ação de mascaramento

                if action_prob < 0.8: # 80% -> [MASK]
                    inputs[i] = self.mask_token_id
                elif action_prob < 0.9: # 10% -> Palavra Aleatória
                    inputs[i] = random.randrange(self.vocab_size)
                # else: 10% -> Manter Original (inputs[i] já é o original)
                # labels[i] já é o token original, então não precisa mudar
            else: # Não mascara este token
                labels[i] = self.pad_token_id # Ignorar no loss
        return torch.tensor(inputs), torch.tensor(labels)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        encoding = self.tokenizer.encode(sentence) # Já aplica CLS, SEP, padding, truncation

        input_ids_original = encoding.ids
        masked_input_ids, mlm_labels = self._mask_tokens(input_ids_original)
        
        attention_mask = torch.tensor(encoding.attention_mask, dtype=torch.long)
        # Segment IDs (todos zeros para MLM com sentenças únicas)
        segment_ids = torch.zeros_like(masked_input_ids, dtype=torch.long)

        return {
            "input_ids": masked_input_ids,
            "attention_mask": attention_mask,
            "segment_ids": segment_ids,
            "labels": mlm_labels, # <<< CHAVE CORRIGIDA AQUI
        }

///////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN 2 (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN 2 (NLI/RTE) ---")

# Constantes para Fine-tuning (mantidas)
ASSIN2_NLI_MODEL_SAVE_FILENAME = "aroeira_assin2_nli_model.pth"
ASSIN2_SUBSET_SIZE_TRAIN = 500 # Usar subset pequeno do treino para demonstração
ASSIN2_SUBSET_SIZE_VAL = 100  # Usar subset pequeno da validação para demonstração
FINETUNE_EPOCHS_NLI = 2 # Mesmo que FINETUNE_EPOCHS global, mas pode ser específico
FINETUNE_BATCH_SIZE_NLI = 8 # Mesmo que FINETUNE_BATCH_SIZE global
FINETUNE_LR_NLI = 3e-5 # Mesmo que FINETUNE_LR global

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
# (Definição da classe BERTForSequencePairClassification mantida como antes)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes)
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :]
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN 2 NLI
# (Definição da classe ASSIN2NLIDataset mantida como antes)
class ASSIN2NLIDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, max_len):
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        label_map = {"Entailment": 0, "Contradiction": 1, "None": 2}
        for example in hf_dataset_split: # hf_dataset_split agora é um datasets.Dataset
            self.premises.append(example["premise"])
            self.hypotheses.append(example["hypothesis"])
            self.labels.append(label_map[example["entailment_judgment"]])
    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx])
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list)
        try:
            sep_id = self.tokenizer.token_to_id("</s>")
            sep_idx = input_ids_list.index(sep_id, 1)
            for i in range(sep_idx + 1, len(input_ids_list)):
                if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"):
                    segment_ids_list[i] = 1
        except ValueError: pass
        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

# Início da lógica de carregamento e fine-tuning
try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    # Carrega toda a configuração 'ptbr' que inclui ASSIN1, ASSIN2, RTE, STS
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)

    # Agora, selecione os splits específicos para ASSIN 2 RTE dentro da configuração 'ptbr'
    # Os nomes dos splits no Hub são como "train_assin2_rte", "validation_assin2_rte", "test_assin2_rte"
    print("Selecionando splits para ASSIN 2 RTE (ptbr)...")
    assin2_nli_train_full_hf = assin_ptbr_full["train_assin2_rte"]
    assin2_nli_val_full_hf = assin_ptbr_full["validation_assin2_rte"]
    # Se precisar do conjunto de teste para avaliação final:
    # assin2_nli_test_full_hf = assin_ptbr_full["test_assin2_rte"]

    # Aplicar subsetting para demonstração
    assin2_nli_train_hf = assin2_nli_train_full_hf.select(range(min(ASSIN2_SUBSET_SIZE_TRAIN, len(assin2_nli_train_full_hf))))
    assin2_nli_val_hf = assin2_nli_val_full_hf.select(range(min(ASSIN2_SUBSET_SIZE_VAL, len(assin2_nli_val_full_hf))))

    print(f"Dataset ASSIN 2 NLI para treino: {len(assin2_nli_train_hf)} exemplos.")
    print(f"Dataset ASSIN 2 NLI para validação: {len(assin2_nli_val_hf)} exemplos.")
    
    ft_assin2_nli_train_dataset = ASSIN2NLIDataset(assin2_nli_train_hf, tokenizer, max_len=MAX_LEN)
    ft_assin2_nli_train_loader = DataLoader(ft_assin2_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE_NLI, shuffle=True, num_workers=0)
    
    ft_assin2_nli_val_loader = None
    if len(assin2_nli_val_hf) > 0:
        ft_assin2_nli_val_dataset = ASSIN2NLIDataset(assin2_nli_val_hf, tokenizer, max_len=MAX_LEN)
        ft_assin2_nli_val_loader = DataLoader(ft_assin2_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE_NLI, shuffle=False, num_workers=0)

    # Carregar backbone pré-treinado e criar modelo NLI
    bert_base_for_nli = BERTBaseModel(
        vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
        num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
        max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID) # Reusa PAD_TOKEN_ID global

    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
        bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        bert_base_state_dict = {
            k.replace("bert_base.", ""): v 
            for k, v in bertlm_state_dict.items() 
            if k.startswith("bert_base.")
        }
        if bert_base_state_dict:
            bert_base_for_nli.load_state_dict(bert_base_state_dict)
            print("Pesos do backbone BERT carregados para fine-tuning NLI.")
        else:
            print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
    else:
        print(f"AVISO (NLI): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

    assin2_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=3).to(DEVICE) # 3 classes para NLI
    optimizer_nli = torch.optim.AdamW(assin2_nli_model.parameters(), lr=FINETUNE_LR_NLI)
    criterion_nli = nn.CrossEntropyLoss()

    trainer_nli = GenericTrainer(
        model=assin2_nli_model, train_dataloader=ft_assin2_nli_train_loader, val_dataloader=ft_assin2_nli_val_loader,
        optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
        model_save_path=ASSIN2_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        # vocab_size não é necessário para GenericTrainer quando task_type é "nli"
    )
    trainer_nli.train(num_epochs=FINETUNE_EPOCHS_NLI)

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN 2 NLI): {e}")
    import traceback
    traceback.print_exc()
    # Se o erro for de dataset não encontrado ou split, pode ser necessário
    # instalar o 'gdown' se o dataset depender dele, ou verificar nomes exatos no Hugging Face Hub.


//////////

# --- Início do Bloco 10 (Diagnóstico Primeiro) ---
import datasets # Certifique-se que está importado

print("Verificando splits disponíveis para o dataset 'assin', configuração 'ptbr'...")
try:
    # Carrega TODA a configuração 'ptbr'
    assin_config_ptbr_info = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    
    print("\nSplits disponíveis na configuração 'ptbr' do dataset 'assin':")
    available_splits = list(assin_config_ptbr_info.keys())
    print(available_splits)
    
    # Sugestões de nomes que você pode procurar na lista acima:
    # Para ASSIN 2 RTE (NLI):
    # - 'train_assin2_rte'
    # - 'validation_assin2_rte'
    # - 'test_assin2_rte'
    # Ou talvez nomes mais genéricos como 'train', 'validation', 'test',
    # e você precisaria filtrar o conteúdo depois (mais complexo).
    # É provável que os nomes específicos como 'train_assin2_rte' estejam lá.

except Exception as e:
    print(f"Erro ao tentar carregar informações do dataset 'assin' com config 'ptbr': {e}")
    print("Verifique o nome do dataset, a configuração 'ptbr' e sua conexão com a internet.")
    raise # Para a execução para você poder analisar
//////////////////////////////////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN 2 (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN 2 (NLI/RTE) ---")

# Constantes para Fine-tuning (mantidas)
ASSIN2_NLI_MODEL_SAVE_FILENAME = "aroeira_assin2_nli_model.pth"
ASSIN2_SUBSET_SIZE_TRAIN = 500 # Usar subset pequeno do treino para demonstração
ASSIN2_SUBSET_SIZE_VAL = 100  # Usar subset pequeno da validação para demonstração
# FINETUNE_EPOCHS_NLI, FINETUNE_BATCH_SIZE_NLI, FINETUNE_LR_NLI são definidos no Bloco 2 como FINETUNE_EPOCHS, etc.
# Se quiser valores específicos para NLI, defina-os aqui ou no Bloco 2 com sufixo _NLI

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
# (Definição da classe BERTForSequencePairClassification mantida como antes)
# Certifique-se que esta classe está definida no seu notebook (provavelmente no Bloco 10 mesmo, antes do try-except)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes) # hidden_size deve ser o de bert_base
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :]
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN 2 NLI
# (Definição da classe ASSIN2NLIDataset mantida como antes)
class ASSIN2NLIDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, max_len): # hf_dataset_split é um datasets.Dataset
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        # Mapeamento padrão para ASSIN 2 RTE (verifique se o seu dataset usa exatamente estes)
        label_map = {"Entailment": 0, "None": 1, "Contradiction": 2} # Ajustado: None(Neutral) como 1, Contra como 2
        
        processed_count = 0
        for example in tqdm(hf_dataset_split, desc="Processando exemplos ASSIN2"):
            premise = example.get("premise")
            hypothesis = example.get("hypothesis")
            judgment_str = example.get("entailment_judgment")

            if isinstance(premise, str) and isinstance(hypothesis, str) and judgment_str in label_map:
                self.premises.append(premise)
                self.hypotheses.append(hypothesis)
                self.labels.append(label_map[judgment_str])
                processed_count += 1
            # else:
            #     print(f"Aviso: Exemplo ASSIN2 ignorado devido a dados ausentes ou label inválido: {example}")

        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO: Nenhum exemplo do dataset ASSIN2 foi processado. Verifique as colunas 'premise', 'hypothesis', 'entailment_judgment' e os valores dos labels.")

    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx])
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list) # Default para o primeiro segmento
        try:
            sep_id = self.tokenizer.token_to_id("</s>")
            # Acha o primeiro </s> (que separa premissa da hipótese)
            # Inicia a busca *após* o primeiro token (que é <s>, o CLS)
            first_sep_index = -1
            for i in range(1, len(input_ids_list)):
                if input_ids_list[i] == sep_id:
                    first_sep_index = i
                    break
            
            if first_sep_index != -1:
                for i in range(first_sep_index + 1, len(input_ids_list)):
                    # Marca como segmento 1 tudo após o primeiro SEP até o próximo PAD
                    if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"):
                        segment_ids_list[i] = 1
                    else: # Se encontrou PAD, o resto também é PAD (ou deveria ser)
                        break 
        except ValueError:
            # Pode acontecer se a sentença for muito curta ou não tiver o formato esperado
            # print(f"Aviso: Token SEP não encontrado no formato esperado para o par: {self.premises[idx]}")
            pass # Mantém todos os segment_ids como 0

        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

# Início da lógica de carregamento e fine-tuning
try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    print("Splits disponíveis na config 'ptbr':", list(assin_ptbr_full.keys()))

    # Função para filtrar exemplos do ASSIN 2 RTE
    def filter_assin2_rte(example):
        # IDs no ASSIN podem ter o formato "A2-RTE-train-ID" ou "A2-RTE-val-ID" etc.
        # Também verificamos se 'entailment_judgment' não é nulo (necessário para RTE)
        return example['id'].startswith('A2-RTE') and example['entailment_judgment'] is not None

    # Filtrar os splits genéricos 'train' e 'validation'
    print("Filtrando split 'train' para obter ASSIN 2 RTE...")
    assin2_nli_train_filtered_hf = assin_ptbr_full["train"].filter(filter_assin2_rte)
    print(f"Número de exemplos de TREINO ASSIN 2 RTE após filtro: {len(assin2_nli_train_filtered_hf)}")

    print("Filtrando split 'validation' para obter ASSIN 2 RTE...")
    assin2_nli_val_filtered_hf = assin_ptbr_full["validation"].filter(filter_assin2_rte)
    print(f"Número de exemplos de VALIDAÇÃO ASSIN 2 RTE após filtro: {len(assin2_nli_val_filtered_hf)}")

    # Aplicar subsetting para demonstração aos datasets já filtrados
    if len(assin2_nli_train_filtered_hf) > 0:
        assin2_nli_train_hf = assin2_nli_train_filtered_hf.select(
            range(min(ASSIN2_SUBSET_SIZE_TRAIN, len(assin2_nli_train_filtered_hf)))
        )
    else:
        assin2_nli_train_hf = assin2_nli_train_filtered_hf # Mantém vazio se o filtro não retornou nada
        print("AVISO: Nenhum dado de treino ASSIN 2 RTE após filtro.")


    if len(assin2_nli_val_filtered_hf) > 0:
        assin2_nli_val_hf = assin2_nli_val_filtered_hf.select(
            range(min(ASSIN2_SUBSET_SIZE_VAL, len(assin2_nli_val_filtered_hf)))
        )
    else:
        assin2_nli_val_hf = assin2_nli_val_filtered_hf # Mantém vazio
        print("AVISO: Nenhum dado de validação ASSIN 2 RTE após filtro.")


    print(f"Dataset ASSIN 2 NLI para fine-tuning (treino): {len(assin2_nli_train_hf)} exemplos.")
    print(f"Dataset ASSIN 2 NLI para fine-tuning (validação): {len(assin2_nli_val_hf)} exemplos.")
    
    if len(assin2_nli_train_hf) == 0:
        print("Nenhum dado de treino para ASSIN 2 NLI. Pulando fine-tuning e avaliação para este benchmark.")
    else:
        ft_assin2_nli_train_dataset = ASSIN2NLIDataset(assin2_nli_train_hf, tokenizer, max_len=MAX_LEN)
        ft_assin2_nli_train_loader = DataLoader(ft_assin2_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        
        ft_assin2_nli_val_loader = None
        if len(assin2_nli_val_hf) > 0:
            ft_assin2_nli_val_dataset = ASSIN2NLIDataset(assin2_nli_val_hf, tokenizer, max_len=MAX_LEN)
            ft_assin2_nli_val_loader = DataLoader(ft_assin2_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)

        # Carregar backbone pré-treinado e criar modelo NLI
        bert_base_for_nli = BERTBaseModel(
            vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
            num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
            max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
            bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            bert_base_state_dict = {
                k.replace("bert_base.", ""): v 
                for k, v in bertlm_state_dict.items() 
                if k.startswith("bert_base.")
            }
            if bert_base_state_dict:
                bert_base_for_nli.load_state_dict(bert_base_state_dict)
                print("Pesos do backbone BERT carregados para fine-tuning NLI.")
            else:
                print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
        else:
            print(f"AVISO (NLI): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

        # 3 classes para NLI: Entailment, None (Neutral), Contradiction
        assin2_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=3).to(DEVICE)
        optimizer_nli = torch.optim.AdamW(assin2_nli_model.parameters(), lr=FINETUNE_LR) # Usando FINETUNE_LR global
        criterion_nli = nn.CrossEntropyLoss()

        trainer_nli = GenericTrainer(
            model=assin2_nli_model, train_dataloader=ft_assin2_nli_train_loader, val_dataloader=ft_assin2_nli_val_loader,
            optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
            model_save_path=ASSIN2_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        )
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS) # Usando FINETUNE_EPOCHS global

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN 2 NLI): {e}")
    import traceback
    traceback.print_exc()
    # Se o erro for de dataset não encontrado ou split, pode ser necessário
    # instalar o 'gdown' se o dataset depender dele, ou verificar nomes exatos no Hugging Face Hub.

# ... (Bloco 11 para HAREM continua como antes) ...
//////////////////////////////////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN 2 (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN 2 (NLI/RTE) ---")

# ... (definições de constantes e classes como BERTForSequencePairClassification e ASSIN2NLIDataset omitidas para brevidade, mas devem estar no seu código) ...
# Certifique-se que as classes BERTForSequencePairClassification e ASSIN2NLIDataset estão definidas acima deste ponto.

try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    print("Dataset 'assin' com config 'ptbr' carregado com sucesso.")

    # =========== DIAGNÓSTICO DAS COLUNAS E EXEMPLO ===========
    print("\n--- Informações do Dataset Carregado (split 'train') ---")
    raw_train_data_assin = assin_ptbr_full["train"] # Pega o split de treino

    if raw_train_data_assin:
        print("Features (colunas) disponíveis no split 'train':")
        print(raw_train_data_assin.features) # Mostra todas as colunas e seus tipos
        
        print("\nPrimeiro exemplo do split 'train':")
        try:
            # Para IterableDatasets (que é o caso se o pai é streaming e não foi materializado)
            # ou para Datasets normais, pegar o primeiro elemento.
            # Se raw_train_data_assin for um Dataset comum, raw_train_data_assin[0] funciona.
            # Se for IterableDataset, precisamos de um iterador.
            # datasets.load_dataset com name="ptbr" geralmente retorna um DatasetDict de Datasets, não IterableDatasets.
            if len(raw_train_data_assin) > 0:
                 print(raw_train_data_assin[0])
            else:
                print("O split 'train' do ASSIN 'ptbr' está vazio.")

        except Exception as e_inspect:
            print(f"Erro ao tentar inspecionar o primeiro exemplo: {e_inspect}")
    else:
        print("O split 'train' do ASSIN 'ptbr' não foi carregado ou está vazio.")
    # ==========================================================

    # Função para filtrar exemplos do ASSIN 2 RTE
    # Você precisará AJUSTAR esta função com base na saída do DIAGNÓSTICO acima
    def filter_assin2_rte(example):
        # PROCURE O NOME CORRETO DA COLUNA DE ID NA SAÍDA ACIMA
        # E O NOME CORRETO DA COLUNA DE JULGAMENTO DE IMPLICAÇÃO
        id_column_name_in_dataset = "id" # Suposição inicial, VERIFIQUE!
        entailment_column_name_in_dataset = "entailment_judgment" # Suposição inicial, VERIFIQUE!

        example_id = example.get(id_column_name_in_dataset)
        entailment_judgment = example.get(entailment_column_name_in_dataset)

        if example_id is None or not isinstance(example_id, str):
            # print(f"Aviso: Exemplo sem ID de string válido: {example}")
            return False # Não pode filtrar por ID se não houver ID

        return example_id.startswith('A2-RTE') and entailment_judgment is not None

    # Filtrar os splits genéricos 'train' e 'validation'
    print("\nFiltrando split 'train' para obter ASSIN 2 RTE...")
    assin2_nli_train_filtered_hf = raw_train_data_assin.filter(filter_assin2_rte)
    print(f"Número de exemplos de TREINO ASSIN 2 RTE após filtro: {len(assin2_nli_train_filtered_hf)}")

    # Faça o mesmo para o split de validação
    raw_val_data_assin = assin_ptbr_full["validation"]
    if raw_val_data_assin:
        print("\nFeatures (colunas) disponíveis no split 'validation':") # Opcional, mas bom para confirmar
        print(raw_val_data_assin.features)
        print("\nFiltrando split 'validation' para obter ASSIN 2 RTE...")
        assin2_nli_val_filtered_hf = raw_val_data_assin.filter(filter_assin2_rte)
        print(f"Número de exemplos de VALIDAÇÃO ASSIN 2 RTE após filtro: {len(assin2_nli_val_filtered_hf)}")
    else:
        print("Split 'validation' do ASSIN 'ptbr' não foi carregado ou está vazio.")
        assin2_nli_val_filtered_hf = datasets.Dataset.from_dict({}) # Cria um dataset vazio

    # ... (Restante do Bloco 10, como aplicar subsetting, criar DataLoaders, modelo, treino) ...
    # Certifique-se de que assin2_nli_train_hf e assin2_nli_val_hf são usados corretamente abaixo.
    # Por exemplo:
    if len(assin2_nli_train_filtered_hf) > 0:
        assin2_nli_train_hf = assin2_nli_train_filtered_hf.select(
            range(min(ASSIN2_SUBSET_SIZE_TRAIN, len(assin2_nli_train_filtered_hf)))
        )
    else:
        assin2_nli_train_hf = assin2_nli_train_filtered_hf
        print("AVISO: Nenhum dado de treino ASSIN 2 RTE após filtro.")

    if len(assin2_nli_val_filtered_hf) > 0:
        assin2_nli_val_hf = assin2_nli_val_filtered_hf.select(
            range(min(ASSIN2_SUBSET_SIZE_VAL, len(assin2_nli_val_filtered_hf)))
        )
    else:
        assin2_nli_val_hf = assin2_nli_val_filtered_hf
        print("AVISO: Nenhum dado de validação ASSIN 2 RTE após filtro.")
        
    print(f"Dataset ASSIN 2 NLI para fine-tuning (treino): {len(assin2_nli_train_hf)} exemplos.")
    print(f"Dataset ASSIN 2 NLI para fine-tuning (validação): {len(assin2_nli_val_hf)} exemplos.")

    if len(assin2_nli_train_hf) == 0:
        print("Nenhum dado de treino para ASSIN 2 NLI. Pulando fine-tuning e avaliação para este benchmark.")
    else:
        # ... (Restante da lógica de fine-tuning com ft_assin2_nli_train_dataset, etc.)
        # (Cole o restante do Bloco 10 aqui, garantindo que as classes estejam definidas)
        ft_assin2_nli_train_dataset = ASSIN2NLIDataset(assin2_nli_train_hf, tokenizer, max_len=MAX_LEN)
        ft_assin2_nli_train_loader = DataLoader(ft_assin2_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        
        ft_assin2_nli_val_loader = None
        if len(assin2_nli_val_hf) > 0:
            ft_assin2_nli_val_dataset = ASSIN2NLIDataset(assin2_nli_val_hf, tokenizer, max_len=MAX_LEN)
            ft_assin2_nli_val_loader = DataLoader(ft_assin2_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)

        bert_base_for_nli = BERTBaseModel(
            vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
            num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
            max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
            bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            bert_base_state_dict = {
                k.replace("bert_base.", ""): v 
                for k, v in bertlm_state_dict.items() 
                if k.startswith("bert_base.")
            }
            if bert_base_state_dict:
                bert_base_for_nli.load_state_dict(bert_base_state_dict)
                print("Pesos do backbone BERT carregados para fine-tuning NLI.")
            else:
                print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
        else:
            print(f"AVISO (NLI): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

        assin2_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=3).to(DEVICE)
        optimizer_nli = torch.optim.AdamW(assin2_nli_model.parameters(), lr=FINETUNE_LR)
        criterion_nli = nn.CrossEntropyLoss()

        trainer_nli = GenericTrainer(
            model=assin2_nli_model, train_dataloader=ft_assin2_nli_train_loader, val_dataloader=ft_assin2_nli_val_loader,
            optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
            model_save_path=ASSIN2_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        )
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS) # Usando FINETUNE_EPOCHS global

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN 2 NLI): {e}")
    import traceback
    traceback.print_exc()
////////////////////////////////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN (NLI/RTE) ---
# (Lembre-se de que as definições das classes BERTBaseModel, BERTForSequencePairClassification
# e ASSIN2NLIDataset devem estar presentes no seu notebook antes deste bloco,
# ou no início deste bloco se preferir.)

print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN (NLI/RTE) ---")

# Constantes para Fine-tuning
ASSIN_NLI_MODEL_SAVE_FILENAME = "aroeira_assin_nli_model.pth" # Nome do arquivo pode ser mais genérico
ASSIN_SUBSET_SIZE_TRAIN = 500 # Usar subset pequeno do treino para demonstração
ASSIN_SUBSET_SIZE_VAL = 100  # Usar subset pequeno da validação para demonstração
# FINETUNE_EPOCHS, FINETUNE_BATCH_SIZE, FINETUNE_LR são usados do Bloco 2

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
# (Definição da classe BERTForSequencePairClassification - sem alterações em relação à anterior)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__()
        self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes) # hidden_size é de bert_base
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :] # Output do token [CLS] (<s>)
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN NLI
class ASSINNLIDataset(Dataset): # Nomeada de forma mais genérica
    def __init__(self, hf_dataset_split, tokenizer, max_len): # hf_dataset_split é um datasets.Dataset
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        
        # <<< AJUSTE NO LABEL_MAP E NÚMERO DE CLASSES >>>
        # Labels do dataset: ['NONE', 'ENTAILMENT', 'PARAPHRASE']
        self.label_map = {"ENTAILMENT": 0, "NONE": 1, "PARAPHRASE": 2} # Mapeamento para 3 classes
        self.num_classes = 3
        
        processed_count = 0
        for example in tqdm(hf_dataset_split, desc="Processando exemplos ASSIN NLI", file=sys.stdout):
            premise = example.get("premise")
            hypothesis = example.get("hypothesis")
            judgment_str = example.get("entailment_judgment") # Esta é a string do label

            # O judgment_str já é a string como "ENTAILMENT", "NONE", "PARAPHRASE"
            # Se for um ClassLabel, pode vir como ID numérico, mas a feature diz que é ClassLabel(names=...)
            # A biblioteca datasets pode retornar o ID ou a string. Vamos garantir que pegamos a string.
            # Se judgment_str for um int (ID do ClassLabel), precisamos convertê-lo de volta para string usando as features.
            # No entanto, ao iterar, geralmente já obtemos o valor bruto (string ou int).
            # Vamos assumir que é string aqui, conforme os nomes no ClassLabel.
            
            if isinstance(premise, str) and isinstance(hypothesis, str) and \
               isinstance(judgment_str, str) and judgment_str in self.label_map:
                self.premises.append(premise)
                self.hypotheses.append(hypothesis)
                self.labels.append(self.label_map[judgment_str])
                processed_count += 1
        
        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO: Nenhum exemplo do dataset ASSIN foi processado. Verifique as colunas e os valores dos labels.")
        elif processed_count > 0:
            print(f"Processados {processed_count} exemplos para ASSIN NLI.")


    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx])
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list)
        try:
            sep_id = self.tokenizer.token_to_id("</s>")
            first_sep_index = -1
            for i in range(1, len(input_ids_list)): # Começa de 1 para pular o CLS inicial
                if input_ids_list[i] == sep_id:
                    first_sep_index = i
                    break
            if first_sep_index != -1:
                for i in range(first_sep_index + 1, len(input_ids_list)):
                    if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"):
                        segment_ids_list[i] = 1
                    else: break 
        except ValueError: pass # Se SEP não for encontrado

        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

# Início da lógica de carregamento e fine-tuning
try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    
    # Usar os splits 'train' e 'validation' diretamente, pois o filtro por ID não é mais aplicável da forma anterior
    assin_nli_train_full_hf = assin_ptbr_full["train"]
    assin_nli_val_full_hf = assin_ptbr_full["validation"]
    # Se precisar do conjunto de teste para avaliação final:
    # assin_nli_test_full_hf = assin_ptbr_full["test"]

    print(f"Total de exemplos em 'train' (ASSIN ptbr): {len(assin_nli_train_full_hf)}")
    print(f"Total de exemplos em 'validation' (ASSIN ptbr): {len(assin_nli_val_full_hf)}")

    # Aplicar subsetting para demonstração
    # É importante que o subsetting seja feito *depois* de selecionar o split correto
    # e *antes* de passar para ASSINNLIDataset se você quiser que o dataset use apenas o subset.
    
    # Opcional: filtrar para garantir que temos apenas exemplos com entailment_judgment não nulo,
    # embora para ClassLabel isso seja menos crítico, pois sempre terá um valor.
    # Mas é uma boa prática se alguns exemplos pudessem ter esse campo ausente para NLI.
    def has_entailment_judgment(example):
        return example.get('entailment_judgment') is not None
    
    assin_nli_train_full_hf = assin_nli_train_full_hf.filter(has_entailment_judgment)
    assin_nli_val_full_hf = assin_nli_val_full_hf.filter(has_entailment_judgment)
    print(f"Exemplos de treino após filtro has_entailment_judgment: {len(assin_nli_train_full_hf)}")
    print(f"Exemplos de validação após filtro has_entailment_judgment: {len(assin_nli_val_full_hf)}")


    if len(assin_nli_train_full_hf) > 0:
        assin_nli_train_hf_subset = assin_nli_train_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_TRAIN, len(assin_nli_train_full_hf)))
        )
    else:
        assin_nli_train_hf_subset = assin_nli_train_full_hf # Mantém vazio se o filtro não retornou nada
    
    if len(assin_nli_val_full_hf) > 0:
        assin_nli_val_hf_subset = assin_nli_val_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_VAL, len(assin_nli_val_full_hf)))
        )
    else:
        assin_nli_val_hf_subset = assin_nli_val_full_hf
        
    print(f"Dataset ASSIN NLI para fine-tuning (treino subset): {len(assin_nli_train_hf_subset)} exemplos.")
    print(f"Dataset ASSIN NLI para fine-tuning (validação subset): {len(assin_nli_val_hf_subset)} exemplos.")
    
    if len(assin_nli_train_hf_subset) == 0:
        print("Nenhum dado de treino para ASSIN NLI. Pulando fine-tuning e avaliação para este benchmark.")
    else:
        ft_assin_nli_train_dataset = ASSINNLIDataset(assin_nli_train_hf_subset, tokenizer, max_len=MAX_LEN)
        ft_assin_nli_train_loader = DataLoader(ft_assin_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        
        ft_assin_nli_val_loader = None
        if len(assin_nli_val_hf_subset) > 0:
            ft_assin_nli_val_dataset = ASSINNLIDataset(assin_nli_val_hf_subset, tokenizer, max_len=MAX_LEN)
            ft_assin_nli_val_loader = DataLoader(ft_assin_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)

        # Carregar backbone pré-treinado e criar modelo NLI
        bert_base_for_nli = BERTBaseModel(
            vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
            num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
            max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
            bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            bert_base_state_dict = {
                k.replace("bert_base.", ""): v 
                for k, v in bertlm_state_dict.items() 
                if k.startswith("bert_base.")
            }
            if bert_base_state_dict:
                bert_base_for_nli.load_state_dict(bert_base_state_dict)
                print("Pesos do backbone BERT carregados para fine-tuning NLI.")
            else:
                print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
        else:
            print(f"AVISO (NLI): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

        # <<< AJUSTE: Usar o num_classes do dataset >>>
        assin_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=ft_assin_nli_train_dataset.num_classes).to(DEVICE)
        optimizer_nli = torch.optim.AdamW(assin_nli_model.parameters(), lr=FINETUNE_LR) # Usando FINETUNE_LR global
        criterion_nli = nn.CrossEntropyLoss()

        trainer_nli = GenericTrainer(
            model=assin_nli_model, train_dataloader=ft_assin_nli_train_loader, val_dataloader=ft_assin_nli_val_loader,
            optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
            model_save_path=ASSIN_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        )
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS) # Usando FINETUNE_EPOCHS global

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN NLI): {e}")
    import traceback
    traceback.print_exc()

# ... (Bloco 11 para HAREM continua como antes, mas também precisará de atenção se a estrutura dos dados mudar) ...


///////////////////////////////////////////////////////]

# No Bloco 10, após carregar assin_ptbr_full
print("Carregando dataset ASSIN (config 'ptbr')...")
assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
print("Splits disponíveis na config 'ptbr':", list(assin_ptbr_full.keys()))

raw_train_data_assin = assin_ptbr_full["train"]
raw_val_data_assin = assin_ptbr_full["validation"]

print(f"DEBUG: Tamanho inicial de raw_train_data_assin: {len(raw_train_data_assin)}")
print(f"DEBUG: Tamanho inicial de raw_val_data_assin: {len(raw_val_data_assin)}")

if len(raw_train_data_assin) > 0:
    print(f"DEBUG: Primeiro exemplo de raw_train_data_assin ANTES do filtro: {raw_train_data_assin[0]}")
////////////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN (NLI/RTE) ---")

# Constantes para Fine-tuning (mantidas)
ASSIN_NLI_MODEL_SAVE_FILENAME = "aroeira_assin_nli_model.pth"
ASSIN_SUBSET_SIZE_TRAIN = 500 # Mantenha um valor que você sabe que existe no dataset filtrado
ASSIN_SUBSET_SIZE_VAL = 100
# FINETUNE_EPOCHS, FINETUNE_BATCH_SIZE, FINETUNE_LR são do Bloco 2

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
# (Definição da classe BERTForSequencePairClassification mantida como antes)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes)
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :]
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN NLI
class ASSINNLIDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, max_len):
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        self.label_map = {"ENTAILMENT": 0, "NONE": 1, "PARAPHRASE": 2}
        self.num_classes = 3
        
        processed_count = 0
        skipped_count = 0
        
        print(f"DEBUG ASSINNLIDataset: Iniciando processamento de {len(hf_dataset_split)} exemplos brutos de entrada.")
        # hf_dataset_split é o resultado de .filter().select()
        # Precisamos do objeto de features do dataset *original* para int2str
        # Vamos assumir que hf_dataset_split ainda tem .features
        feature_ej = None
        if hasattr(hf_dataset_split, 'features') and 'entailment_judgment' in hf_dataset_split.features:
            feature_ej = hf_dataset_split.features['entailment_judgment']
        else:
            print("AVISO DEBUG: Não foi possível obter 'entailment_judgment' de hf_dataset_split.features. A conversão int->str pode falhar.")


        for i, example in enumerate(tqdm(hf_dataset_split, desc="Processando exemplos ASSIN NLI (Dataset Class)", file=sys.stdout)):
            premise = example.get("premise")
            hypothesis = example.get("hypothesis")
            judgment_val = example.get("entailment_judgment") # Valor original (provavelmente int)
            
            judgment_str_for_map = None # String do label (ex: "ENTAILMENT")

            if isinstance(judgment_val, str):
                judgment_str_for_map = judgment_val
            elif isinstance(judgment_val, int) and feature_ej:
                try:
                    judgment_str_for_map = feature_ej.int2str(judgment_val)
                except Exception as e_int2str:
                    # print(f"DEBUG ASSINNLIDataset: Falha ao converter int_to_str para judgment_val={judgment_val} (exemplo {i}): {e_int2str}")
                    judgment_str_for_map = None 
            
            condition_premise_ok = isinstance(premise, str) and premise.strip()
            condition_hypothesis_ok = isinstance(hypothesis, str) and hypothesis.strip()
            condition_judgment_str_ok = judgment_str_for_map is not None
            condition_judgment_in_map = judgment_str_for_map in self.label_map if condition_judgment_str_ok else False
            
            condition_met = (condition_premise_ok and condition_hypothesis_ok and 
                             condition_judgment_str_ok and condition_judgment_in_map)

            if condition_met:
                self.premises.append(premise.strip())
                self.hypotheses.append(hypothesis.strip())
                self.labels.append(self.label_map[judgment_str_for_map])
                processed_count += 1
            else:
                skipped_count += 1
                if skipped_count < 10 or i % (len(hf_dataset_split)//10 + 1) == 0 : # Imprime alguns exemplos pulados
                    print(f"DEBUG ASSINNLIDataset: Exemplo {i} IGNORADO. "
                          f"P_ok: {condition_premise_ok}, H_ok: {condition_hypothesis_ok}, "
                          f"Val: {judgment_val}({type(judgment_val)}), Str: {judgment_str_for_map}, InMap: {condition_judgment_in_map}")
        
        print(f"DEBUG ASSINNLIDataset: Processamento concluído. {processed_count} exemplos adicionados, {skipped_count} ignorados.")
        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO EM ASSINNLIDataset: Nenhum exemplo do dataset de entrada ({len(hf_dataset_split)} exs) foi processado. Verifique as condições de filtro e os dados.")

    def __len__(self): return len(self.labels) # Se self.labels estiver vazio, len será 0
    def __getitem__(self, idx):
        # ... (código __getitem__ mantido como antes)
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx])
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list)
        try:
            sep_id = self.tokenizer.token_to_id("</s>")
            first_sep_index = -1
            for i in range(1, len(input_ids_list)):
                if input_ids_list[i] == sep_id: first_sep_index = i; break
            if first_sep_index != -1:
                for i in range(first_sep_index + 1, len(input_ids_list)):
                    if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"): segment_ids_list[i] = 1
                    else: break 
        except ValueError: pass
        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

# Início da lógica de carregamento e fine-tuning
try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    
    raw_train_data_assin = assin_ptbr_full["train"]
    raw_val_data_assin = assin_ptbr_full["validation"]
    print(f"DEBUG: Tamanho inicial raw_train_data_assin: {len(raw_train_data_assin)}") # Já tínhamos este
    print(f"DEBUG: Tamanho inicial raw_val_data_assin: {len(raw_val_data_assin)}")   # Já tínhamos este

    # Função para filtrar exemplos que possuem 'entailment_judgment'
    def has_entailment_judgment(example):
        ej_value = example.get('entailment_judgment')
        # A ClassLabel pode retornar int. Se for int, é um label válido. Se for string, também.
        # O importante é que a CHAVE exista e o valor não seja explicitamente None (o que não deveria acontecer com ClassLabel)
        return ej_value is not None # Este filtro deve ser bem permissivo

    print("\nFiltrando split 'train' para reter exemplos com 'entailment_judgment'...")
    assin_nli_train_full_hf = raw_train_data_assin.filter(has_entailment_judgment)
    print(f"DEBUG: Tamanho de assin_nli_train_full_hf APÓS filtro has_entailment_judgment: {len(assin_nli_train_full_hf)}")
    if len(assin_nli_train_full_hf) > 0:
        print(f"DEBUG: Primeiro exemplo de TREINO APÓS filtro has_entailment_judgment: {assin_nli_train_full_hf[0]}")


    print("\nFiltrando split 'validation' para reter exemplos com 'entailment_judgment'...")
    assin_nli_val_full_hf = raw_val_data_assin.filter(has_entailment_judgment)
    print(f"DEBUG: Tamanho de assin_nli_val_full_hf APÓS filtro has_entailment_judgment: {len(assin_nli_val_full_hf)}")
    if len(assin_nli_val_full_hf) > 0:
        print(f"DEBUG: Primeiro exemplo de VALIDAÇÃO APÓS filtro has_entailment_judgment: {assin_nli_val_full_hf[0]}")


    # Aplicar subsetting para demonstração aos datasets já filtrados
    if len(assin_nli_train_full_hf) > 0:
        assin_nli_train_hf_subset = assin_nli_train_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_TRAIN, len(assin_nli_train_full_hf)))
        )
    else:
        assin_nli_train_hf_subset = assin_nli_train_full_hf 
        print("AVISO: Nenhum dado de treino ASSIN após filtro 'has_entailment_judgment'.")

    if len(assin_nli_val_full_hf) > 0:
        assin_nli_val_hf_subset = assin_nli_val_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_VAL, len(assin_nli_val_full_hf)))
        )
    else:
        assin_nli_val_hf_subset = assin_nli_val_full_hf
        print("AVISO: Nenhum dado de validação ASSIN após filtro 'has_entailment_judgment'.")
        
    print(f"\nDEBUG: Tamanho de assin_nli_train_hf_subset (input para Dataset final): {len(assin_nli_train_hf_subset)}")
    print(f"DEBUG: Tamanho de assin_nli_val_hf_subset (input para Dataset final): {len(assin_nli_val_hf_subset)}")

    if len(assin_nli_train_hf_subset) == 0:
        print("Nenhum dado de treino para ASSIN NLI após subsetting. Pulando fine-tuning e avaliação para este benchmark.")
    else:
        # Instanciação do Dataset e DataLoader de Treino
        ft_assin_nli_train_dataset = ASSINNLIDataset(assin_nli_train_hf_subset, tokenizer, max_len=MAX_LEN)
        if len(ft_assin_nli_train_dataset) == 0 : # Checagem crucial
            raise ValueError("ft_assin_nli_train_dataset está vazio APÓS instanciação. Verifique a lógica de processamento em ASSINNLIDataset.")
        ft_assin_nli_train_loader = DataLoader(ft_assin_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        
        # Instanciação do Dataset e DataLoader de Validação
        ft_assin_nli_val_loader = None
        if len(assin_nli_val_hf_subset) > 0:
            ft_assin_nli_val_dataset = ASSINNLIDataset(assin_nli_val_hf_subset, tokenizer, max_len=MAX_LEN)
            if len(ft_assin_nli_val_dataset) > 0: # Checagem crucial
                ft_assin_nli_val_loader = DataLoader(ft_assin_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)
            else:
                print("AVISO: ft_assin_nli_val_dataset ficou vazio após instanciação.")
        else:
            print("Nenhum dado de validação para ASSIN NLI após subsetting.")


        # Carregar backbone pré-treinado e criar modelo NLI
        # (Lógica de carregamento do modelo e instanciação do trainer mantida como antes)
        bert_base_for_nli = BERTBaseModel(
            vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
            num_attention_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
            max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
            bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            bert_base_state_dict = { k.replace("bert_base.", ""): v for k, v in bertlm_state_dict.items() if k.startswith("bert_base.") }
            if bert_base_state_dict: bert_base_for_nli.load_state_dict(bert_base_state_dict); print("Pesos do backbone BERT carregados para NLI.")
            else: print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado.")
        else: print(f"AVISO (NLI): {PRETRAINED_BERTLM_SAVE_FILENAME} não encontrado.")

        assin_nli_model = BERTForSequencePairClassification(bert_base_for_nli, MODEL_HIDDEN_SIZE, num_classes=3).to(DEVICE) # num_classes=3 para ENTAILMENT, NONE, PARAPHRASE
        optimizer_nli = torch.optim.AdamW(assin_nli_model.parameters(), lr=FINETUNE_LR)
        criterion_nli = nn.CrossEntropyLoss()

        trainer_nli = GenericTrainer(
            model=assin_nli_model, train_dataloader=ft_assin_nli_train_loader, val_dataloader=ft_assin_nli_val_loader,
            optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
            model_save_path=ASSIN_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        )
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS)

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN NLI): {e}")
    import traceback
    traceback.print_exc()

# ... (Bloco 11 para HAREM continua como antes) ...
////////////////////////////////////////////////////

# --- Bloco 10: Fine-tuning e Avaliação no ASSIN (NLI/RTE) ---
print("\n--- Bloco 10: Fine-tuning e Avaliação ASSIN (NLI/RTE) ---")

# Constantes para Fine-tuning (ajuste se necessário, ou use as globais do Bloco 2)
ASSIN_NLI_MODEL_SAVE_FILENAME = "aroeira_assin_nli_model.pth"
ASSIN_SUBSET_SIZE_TRAIN = 500 # Usar subset pequeno do treino para demonstração
ASSIN_SUBSET_SIZE_VAL = 100  # Usar subset pequeno da validação para demonstração
# FINETUNE_EPOCHS, FINETUNE_BATCH_SIZE, FINETUNE_LR são usados do Bloco 2

# 10.1. Modelo para Classificação de Pares de Sentenças (NLI)
class BERTForSequencePairClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_classes):
        super().__init__()
        self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_classes) # hidden_size deve ser o de bert_base
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        cls_output = sequence_output[:, 0, :] # Output do token [CLS] (<s>)
        return self.classifier(self.dropout(cls_output))

# 10.2. Dataset para ASSIN NLI
class ASSINNLIDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, max_len): # hf_dataset_split é um datasets.Dataset
        self.tokenizer, self.max_len = tokenizer, max_len
        self.premises, self.hypotheses, self.labels = [], [], []
        self.label_map = {"ENTAILMENT": 0, "NONE": 1, "PARAPHRASE": 2} # Mapeamento para 3 classes
        self.num_classes = 3 # Definindo o número de classes
        
        processed_count = 0
        skipped_count = 0
        
        print(f"DEBUG ASSINNLIDataset: Iniciando processamento de {len(hf_dataset_split)} exemplos brutos de entrada.")
        feature_ej = None
        if hasattr(hf_dataset_split, 'features') and 'entailment_judgment' in hf_dataset_split.features:
            feature_ej = hf_dataset_split.features['entailment_judgment']
        else:
            print("AVISO DEBUG: Não foi possível obter 'entailment_judgment' de hf_dataset_split.features.")

        for i, example in enumerate(tqdm(hf_dataset_split, desc="Processando exemplos ASSIN NLI (Dataset Class)", file=sys.stdout)):
            premise = example.get("premise")
            hypothesis = example.get("hypothesis")
            judgment_val = example.get("entailment_judgment")
            
            judgment_str_for_map = None

            if isinstance(judgment_val, str):
                judgment_str_for_map = judgment_val
            elif isinstance(judgment_val, int) and feature_ej:
                try:
                    judgment_str_for_map = feature_ej.int2str(judgment_val)
                except Exception: judgment_str_for_map = None 
            
            condition_premise_ok = isinstance(premise, str) and premise.strip()
            condition_hypothesis_ok = isinstance(hypothesis, str) and hypothesis.strip()
            condition_judgment_str_ok = judgment_str_for_map is not None
            condition_judgment_in_map = judgment_str_for_map in self.label_map if condition_judgment_str_ok else False
            condition_met = (condition_premise_ok and condition_hypothesis_ok and 
                             condition_judgment_str_ok and condition_judgment_in_map)

            if condition_met:
                self.premises.append(premise.strip())
                self.hypotheses.append(hypothesis.strip())
                self.labels.append(self.label_map[judgment_str_for_map])
                processed_count += 1
            else:
                skipped_count += 1
                # Descomente a linha abaixo para depuração intensa de exemplos ignorados
                # if skipped_count < 10 or i % (len(hf_dataset_split)//10 + 1) == 0 :
                #     print(f"DEBUG ASSINNLIDataset: Exemplo {i} IGNORADO. Val: {judgment_val}({type(judgment_val)}), Str: {judgment_str_for_map}")
        
        print(f"DEBUG ASSINNLIDataset: Processamento concluído. {processed_count} exemplos adicionados, {skipped_count} ignorados.")
        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO EM ASSINNLIDataset: Nenhum exemplo do dataset de entrada ({len(hf_dataset_split)} exs) foi processado.")

    def __len__(self): return len(self.labels)
    def __getitem__(self, idx):
        encoding = self.tokenizer.encode(self.premises[idx], self.hypotheses[idx])
        input_ids_list = encoding.ids
        segment_ids_list = [0] * len(input_ids_list)
        try:
            sep_id = self.tokenizer.token_to_id("</s>")
            first_sep_index = -1
            for i in range(1, len(input_ids_list)):
                if input_ids_list[i] == sep_id: first_sep_index = i; break
            if first_sep_index != -1:
                for i in range(first_sep_index + 1, len(input_ids_list)):
                    if input_ids_list[i] != self.tokenizer.token_to_id("<pad>"): segment_ids_list[i] = 1
                    else: break 
        except ValueError: pass
        return {"input_ids": torch.tensor(input_ids_list), 
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.tensor(segment_ids_list, dtype=torch.long),
                "labels": torch.tensor(self.labels[idx], dtype=torch.long)}

# Início da lógica de carregamento e fine-tuning
try:
    print("Carregando dataset ASSIN (config 'ptbr')...")
    assin_ptbr_full = datasets.load_dataset("assin", name="ptbr", trust_remote_code=True)
    
    raw_train_data_assin = assin_ptbr_full["train"]
    raw_val_data_assin = assin_ptbr_full["validation"]

    def has_entailment_judgment(example):
        return example.get('entailment_judgment') is not None

    assin_nli_train_full_hf = raw_train_data_assin.filter(has_entailment_judgment)
    assin_nli_val_full_hf = raw_val_data_assin.filter(has_entailment_judgment)
    print(f"Tamanho de treino ASSIN após filtro 'has_entailment_judgment': {len(assin_nli_train_full_hf)}")
    print(f"Tamanho de validação ASSIN após filtro 'has_entailment_judgment': {len(assin_nli_val_full_hf)}")

    if len(assin_nli_train_full_hf) > 0:
        assin_nli_train_hf_subset = assin_nli_train_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_TRAIN, len(assin_nli_train_full_hf)))
        )
    else: assin_nli_train_hf_subset = assin_nli_train_full_hf
    
    if len(assin_nli_val_full_hf) > 0:
        assin_nli_val_hf_subset = assin_nli_val_full_hf.select(
            range(min(ASSIN_SUBSET_SIZE_VAL, len(assin_nli_val_full_hf)))
        )
    else: assin_nli_val_hf_subset = assin_nli_val_full_hf
        
    print(f"Dataset ASSIN NLI para fine-tuning (treino subset): {len(assin_nli_train_hf_subset)} exemplos.")
    print(f"Dataset ASSIN NLI para fine-tuning (validação subset): {len(assin_nli_val_hf_subset)} exemplos.")
    
    if len(assin_nli_train_hf_subset) == 0:
        print("Nenhum dado de treino para ASSIN NLI. Pulando fine-tuning e avaliação para este benchmark.")
    else:
        ft_assin_nli_train_dataset = ASSINNLIDataset(assin_nli_train_hf_subset, tokenizer, max_len=MAX_LEN)
        if len(ft_assin_nli_train_dataset) == 0 :
            raise ValueError("ft_assin_nli_train_dataset está vazio APÓS instanciação. Verifique a lógica em ASSINNLIDataset.")
        ft_assin_nli_train_loader = DataLoader(ft_assin_nli_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
        
        ft_assin_nli_val_loader = None
        if len(assin_nli_val_hf_subset) > 0:
            ft_assin_nli_val_dataset = ASSINNLIDataset(assin_nli_val_hf_subset, tokenizer, max_len=MAX_LEN)
            if len(ft_assin_nli_val_dataset) > 0:
                ft_assin_nli_val_loader = DataLoader(ft_assin_nli_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)
            else: print("AVISO: ft_assin_nli_val_dataset ficou vazio após instanciação.")
        else: print("Nenhum dado de validação para ASSIN NLI após subsetting.")

        # Carregar backbone pré-treinado e criar modelo NLI
        bert_base_for_nli = BERTBaseModel(
            vocab_size=tokenizer.get_vocab_size(),
            hidden_size=MODEL_HIDDEN_SIZE,
            num_layers=MODEL_NUM_LAYERS,
            num_heads=MODEL_NUM_ATTENTION_HEADS, # <<< CORREÇÃO APLICADA AQUI
            intermediate_size=MODEL_INTERMEDIATE_SIZE,
            max_len_config=MAX_LEN,
            pad_token_id=PAD_TOKEN_ID
        )

        if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
            print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
            bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
            bert_base_state_dict = {
                k.replace("bert_base.", ""): v 
                for k, v in bertlm_state_dict.items() 
                if k.startswith("bert_base.")
            }
            if bert_base_state_dict:
                bert_base_for_nli.load_state_dict(bert_base_state_dict)
                print("Pesos do backbone BERT carregados para fine-tuning NLI.")
            else:
                print("AVISO (NLI): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
        else:
            print(f"AVISO (NLI): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

        assin_nli_model = BERTForSequencePairClassification(
            bert_base_for_nli, 
            MODEL_HIDDEN_SIZE, 
            num_classes=ft_assin_nli_train_dataset.num_classes # Usa num_classes do dataset
        ).to(DEVICE)
        
        optimizer_nli = torch.optim.AdamW(assin_nli_model.parameters(), lr=FINETUNE_LR)
        criterion_nli = nn.CrossEntropyLoss()

        # Usa as constantes globais FINETUNE_EPOCHS e FINETUNE_LR (definidas no Bloco 2)
        trainer_nli = GenericTrainer(
            model=assin_nli_model, train_dataloader=ft_assin_nli_train_loader, val_dataloader=ft_assin_nli_val_loader,
            optimizer=optimizer_nli, criterion=criterion_nli, device=DEVICE,
            model_save_path=ASSIN_NLI_MODEL_SAVE_FILENAME, task_type="nli" 
        )
        trainer_nli.train(num_epochs=FINETUNE_EPOCHS)

except Exception as e: 
    print(f"Erro no Bloco 10 (ASSIN NLI): {e}")
    import traceback
    traceback.print_exc()

# ... (Bloco 11 para HAREM continua como antes) ...

//////////////////////

# --- Bloco 11: Fine-tuning e Avaliação no HAREM (NER) ---
print("\n--- Bloco 11: Fine-tuning e Avaliação HAREM (NER) ---")
HAREM_MODEL_SAVE_FILENAME = "aroeira_harem_ner_model.pth"
HAREM_SUBSET_SIZE = 200 
# FINETUNE_EPOCHS, FINETUNE_BATCH_SIZE, FINETUNE_LR são do Bloco 2

# 11.1. Modelo para Classificação de Tokens (NER)
# (Definição da classe BERTForTokenClassification mantida como antes)
class BERTForTokenClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_labels):
        super().__init__(); self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_size, num_labels) # hidden_size de bert_base
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        return self.classifier(self.dropout(sequence_output))

# 11.2. Dataset para HAREM NER (COM CORREÇÃO)
class HAREMNERDataset(Dataset):
    # label_map (str->id) não é usado para processar os inputs, mas id2label (id->str) será útil para o Trainer
    def __init__(self, hf_dataset_split, tokenizer, id2label_map, max_len, pad_token_label_id=-100):
        self.tokenizer = tokenizer
        self.id2label = id2label_map # Mantemos para referência e para o Trainer
        self.max_len = max_len
        self.pad_token_label_id = pad_token_label_id

        self.all_tokens_encodings = [] # Armazenará os objetos Encoding completos
        self.all_aligned_labels_as_ids = [] # Armazenará as listas de IDs de label alinhados

        processed_count = 0
        skipped_count = 0
        
        print(f"DEBUG HAREM Dataset: Iniciando processamento de {len(hf_dataset_split)} exemplos brutos de entrada.")

        for i_example, example in enumerate(tqdm(hf_dataset_split, desc="Processando HAREM (Dataset Class)", file=sys.stdout)):
            original_tokens = example.get("tokens")       # Lista de strings (palavras)
            # ner_tag_ids_from_dataset já é uma lista de INTEIROS (IDs das NER tags)
            ner_tag_ids_from_dataset = example.get("ner_tags") 

            if not original_tokens or ner_tag_ids_from_dataset is None or len(original_tokens) != len(ner_tag_ids_from_dataset):
                skipped_count += 1
                if skipped_count < 5 or (len(hf_dataset_split) > 0 and i_example % (len(hf_dataset_split)//10 + 1) == 0):
                    print(f"DEBUG HAREM Dataset: Exemplo {i_example} IGNORADO (tokens/ner_tags ausentes ou tamanhos incompatíveis).")
                continue

            text_to_tokenize = " ".join(original_tokens)
            encoding = self.tokenizer.encode(text_to_tokenize)
            
            word_ids = encoding.word_ids # Mapeia cada sub-token para o índice da palavra original
            
            aligned_label_ids = []
            previous_word_idx = None
            for subtoken_idx, word_idx_in_original_tokens in enumerate(word_ids):
                if word_idx_in_original_tokens is None: 
                    aligned_label_ids.append(self.pad_token_label_id)
                elif word_idx_in_original_tokens != previous_word_idx: 
                    # Usa diretamente o ID da tag NER da palavra original
                    aligned_label_ids.append(ner_tag_ids_from_dataset[word_idx_in_original_tokens])
                else: 
                    # Estratégia simples: marcar subsequentes para serem ignorados.
                    aligned_label_ids.append(self.pad_token_label_id)
                previous_word_idx = word_idx_in_original_tokens
            
            if len(encoding.ids) != len(aligned_label_ids): # Verificação de sanidade
                print(f"ALERTA DEBUG HAREM: Exemplo {i_example} - Mismatch de tamanho! Input IDs: {len(encoding.ids)}, Aligned Labels: {len(aligned_label_ids)}")
                skipped_count += 1
                continue

            self.all_tokens_encodings.append(encoding)
            self.all_aligned_labels_as_ids.append(aligned_label_ids)
            processed_count += 1
        
        print(f"DEBUG HAREM Dataset: Processamento concluído. {processed_count} exemplos adicionados, {skipped_count} ignorados.")
        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO EM HAREMNERDataset: Nenhum exemplo do dataset de entrada ({len(hf_dataset_split)} exs) foi processado.")

    def __len__(self): return len(self.all_tokens_encodings)
    def __getitem__(self, idx):
        encoding = self.all_tokens_encodings[idx]
        labels = self.all_aligned_labels_as_ids[idx]
        return {"input_ids": torch.tensor(encoding.ids),
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.zeros(len(encoding.ids), dtype=torch.long),
                "labels": torch.tensor(labels, dtype=torch.long)}

try:
    print("Carregando HAREM ('selective' config)...")
    # A configuração 'selective' é menor. 'total' contém o Primeiro HAREM completo.
    harem_raw = datasets.load_dataset("harem", "selective", trust_remote_code=True) 
    
    # Criar mapeamento de labels para HAREM
    harem_ner_feature = harem_raw["train"].features["ner_tags"].feature
    id2label_harem = {i: name for i, name in enumerate(harem_ner_feature.names)}
    # label2id_harem não é estritamente necessário para HAREMNERDataset se os dados já são IDs,
    # mas é bom ter para consistência ou se precisasse converter strings.
    label2id_harem = {name: i for i, name in enumerate(harem_ner_feature.names)} 
    NUM_NER_TAGS = len(id2label_harem)
    PAD_TOKEN_LABEL_ID_NER = -100 

    print(f"Mapeamento de Labels HAREM (id2label, primeiros 5): {list(id2label_harem.items())[:5]}")
    print(f"Total de tags NER HAREM: {NUM_NER_TAGS}")

    harem_train_full_hf = harem_raw["train"]
    harem_train_hf_subset = harem_train_full_hf # Usar o subset 'selective' inteiro por enquanto
    if HAREM_SUBSET_SIZE is not None and len(harem_train_full_hf) > HAREM_SUBSET_SIZE:
        harem_train_hf_subset = harem_train_full_hf.select(range(HAREM_SUBSET_SIZE))
    
    print(f"DEBUG HAREM: Tamanho do subset de treino HAREM para processamento: {len(harem_train_hf_subset)}")

    # HAREM 'selective' não tem split de validação oficial. Pode-se criar um do 'train' ou usar o 'test' se disponível.
    # Para este exemplo, vamos simplificar e usar apenas o de treino, e o val_loader será None para o Trainer.
    # Se quiser validação, divida harem_train_hf_subset:
    # train_val_harem = harem_train_hf_subset.train_test_split(test_size=0.1, seed=42)
    # ft_harem_train_dataset_hf = train_val_harem['train']
    # ft_harem_val_dataset_hf = train_val_harem['test']
    
    ft_harem_train_dataset = HAREMNERDataset(harem_train_hf_subset, tokenizer, label2id_harem, id2label_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
    
    if len(ft_harem_train_dataset) == 0:
        raise ValueError("Dataset de treino HAREM (ft_harem_train_dataset) está vazio após processamento.")
        
    ft_harem_train_loader = DataLoader(ft_harem_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
    ft_harem_val_loader = None # Sem validação para HAREM neste exemplo simplificado

    bert_base_for_ner = BERTBaseModel(
        vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
        num_heads=MODEL_NUM_ATTENTION_HEADS, intermediate_size=MODEL_INTERMEDIATE_SIZE,
        max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
        bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        bert_base_state_dict = {k.replace("bert_base.", ""): v for k, v in bertlm_state_dict.items() if k.startswith("bert_base.")}
        if bert_base_state_dict: bert_base_for_ner.load_state_dict(bert_base_state_dict); print("Pesos do backbone BERT carregados para NER.")
        else: print("AVISO (NER): Nenhum peso para 'bert_base.' encontrado.")
    else: print(f"AVISO (NER): {PRETRAINED_BERTLM_SAVE_FILENAME} não encontrado.")

    harem_ner_model = BERTForTokenClassification(bert_base_for_ner, MODEL_HIDDEN_SIZE, NUM_NER_TAGS).to(DEVICE)
    optimizer_ner = torch.optim.AdamW(harem_ner_model.parameters(), lr=FINETUNE_LR)
    criterion_ner = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_LABEL_ID_NER)

    trainer_ner = GenericTrainer(
        model=harem_ner_model, train_dataloader=ft_harem_train_loader, val_dataloader=ft_harem_val_loader, # val_loader é None
        optimizer=optimizer_ner, criterion=criterion_ner, device=DEVICE,
        model_save_path=HAREM_MODEL_SAVE_FILENAME, task_type="ner", 
        vocab_size=NUM_NER_TAGS, # Para NER, este é o número de tags NER
        id2label=id2label_harem # Passando o mapeamento id->string label para o trainer
    )
    trainer_ner.train(num_epochs=FINETUNE_EPOCHS)

except Exception as e: 
    print(f"Erro no Bloco 11 (HAREM NER): {e}")
    import traceback
    traceback.print_exc()
    raise

# ... (final do script)


////////////////////////////////////////////

# --- Bloco 11: Fine-tuning e Avaliação no HAREM (NER) ---
print("\n--- Bloco 11: Fine-tuning e Avaliação HAREM (NER) ---")
HAREM_MODEL_SAVE_FILENAME = "aroeira_harem_ner_model.pth"
HAREM_SUBSET_SIZE = 250 # 'selective' HAREM já é pequeno, mas limite para teste rápido. None para usar o split carregado.
PAD_TOKEN_LABEL_ID_NER = -100 # ID padrão para ignorar tokens no loss de NER (CrossEntropyLoss)
# FINETUNE_EPOCHS, FINETUNE_BATCH_SIZE, FINETUNE_LR são do Bloco 2

# 11.1. Modelo para Classificação de Tokens (NER)
class BERTForTokenClassification(nn.Module):
    def __init__(self, bert_base: BERTBaseModel, hidden_size, num_labels):
        super().__init__()
        self.bert_base = bert_base
        self.dropout = nn.Dropout(0.1) # Dropout típico antes da camada final
        self.classifier = nn.Linear(hidden_size, num_labels) # hidden_size de bert_base
    def forward(self, input_ids, attention_mask, segment_ids):
        sequence_output = self.bert_base(input_ids, attention_mask, segment_ids)
        # Aplica o classificador em cada token da sequência
        return self.classifier(self.dropout(sequence_output))

# 11.2. Dataset para HAREM NER
class HAREMNERDataset(Dataset):
    def __init__(self, hf_dataset_split, tokenizer, id2label_map, max_len, pad_token_label_id):
        self.tokenizer = tokenizer
        self.id2label = id2label_map # Usado pelo GenericTrainer para avaliação com seqeval
        self.max_len = max_len
        self.pad_token_label_id = pad_token_label_id

        self.all_tokens_encodings = [] # Armazenará os objetos Encoding completos
        self.all_aligned_labels_as_ids = [] # Armazenará as listas de IDs de label alinhados

        processed_count = 0
        skipped_count = 0
        
        print(f"DEBUG HAREM Dataset: Iniciando processamento de {len(hf_dataset_split)} exemplos brutos de entrada.")

        for i_example, example in enumerate(tqdm(hf_dataset_split, desc="Processando HAREM (Dataset Class)", file=sys.stdout)):
            original_tokens = example.get("tokens")       # Lista de strings (palavras)
            ner_tag_ids_from_dataset = example.get("ner_tags") # Lista de INTEIROS (IDs das tags NER)

            if not original_tokens or ner_tag_ids_from_dataset is None or len(original_tokens) != len(ner_tag_ids_from_dataset):
                skipped_count += 1
                # Imprime com menos frequência para não poluir muito
                if skipped_count < 5 or (len(hf_dataset_split) > 0 and i_example > 0 and i_example % (len(hf_dataset_split)//10 + 1) == 0):
                    print(f"DEBUG HAREM Dataset: Exemplo {i_example} IGNORADO (tokens/ner_tags ausentes ou tamanhos incompatíveis). "
                          f"Tokens: {len(original_tokens) if original_tokens else 0}, Tags: {len(ner_tag_ids_from_dataset) if ner_tag_ids_from_dataset else 0}")
                continue

            text_to_tokenize = " ".join(original_tokens)
            # O tokenizador já está configurado para adicionar <s>, </s>, truncar e aplicar padding para MAX_LEN
            encoding = self.tokenizer.encode(text_to_tokenize)
            
            word_ids = encoding.word_ids # Mapeia cada sub-token para o índice da palavra original
            
            aligned_label_ids = []
            previous_word_idx = None
            for subtoken_idx, word_idx_in_original_tokens in enumerate(word_ids):
                if word_idx_in_original_tokens is None: # Token especial (<s>, </s>, <pad> do tokenizer)
                    aligned_label_ids.append(self.pad_token_label_id)
                elif word_idx_in_original_tokens != previous_word_idx: # Primeiro sub-token de uma nova palavra
                    # Usa diretamente o ID da tag NER da palavra original correspondente
                    aligned_label_ids.append(ner_tag_ids_from_dataset[word_idx_in_original_tokens])
                else: # Sub-token subsequente da mesma palavra
                    # Estratégia simples: marcar subsequentes para serem ignorados no loss (-100).
                    # Para IOB2 completo, a lógica para gerar I-TAGs seria mais complexa aqui.
                    aligned_label_ids.append(self.pad_token_label_id)
                previous_word_idx = word_idx_in_original_tokens
            
            # Verificação de sanidade (opcional, mas útil para depurar alinhamento)
            if len(encoding.ids) != len(aligned_label_ids):
                if i_example < 5 : # Imprime para os primeiros 5 casos de mismatch
                    print(f"ALERTA DEBUG HAREM: Exemplo {i_example} - Mismatch de tamanho! "
                          f"Input IDs: {len(encoding.ids)}, Aligned Labels: {len(aligned_label_ids)}")
                    print(f"  Original Tokens: {original_tokens}")
                    print(f"  Encoded Tokens: {encoding.tokens}")
                    print(f"  Word IDs: {word_ids}")
                skipped_count += 1
                continue # Pula este exemplo se o alinhamento falhar

            self.all_tokens_encodings.append(encoding)
            self.all_aligned_labels_as_ids.append(aligned_label_ids)
            processed_count += 1
        
        print(f"DEBUG HAREM Dataset: Processamento concluído. {processed_count} exemplos adicionados, {skipped_count} ignorados.")
        if processed_count == 0 and len(hf_dataset_split) > 0:
             print(f"AVISO SÉRIO EM HAREMNERDataset: Nenhum exemplo do dataset de entrada ({len(hf_dataset_split)} exs) foi processado.")

    def __len__(self): return len(self.all_tokens_encodings)

    def __getitem__(self, idx):
        encoding = self.all_tokens_encodings[idx]
        labels = self.all_aligned_labels_as_ids[idx]
        return {"input_ids": torch.tensor(encoding.ids, dtype=torch.long),
                "attention_mask": torch.tensor(encoding.attention_mask, dtype=torch.long),
                "segment_ids": torch.zeros(len(encoding.ids), dtype=torch.long), # Default para NER
                "labels": torch.tensor(labels, dtype=torch.long)}

try:
    print("Carregando HAREM ('selective' config)...")
    # A configuração 'selective' é menor. 'total' contém o Primeiro HAREM completo.
    # O HAREM já vem com 'ner_tags' como IDs numéricos.
    harem_raw = datasets.load_dataset("harem", "selective", trust_remote_code=True) 
    
    # Criar mapeamento de labels para HAREM
    harem_ner_feature = harem_raw["train"].features["ner_tags"].feature
    id2label_harem = {i: name for i, name in enumerate(harem_ner_feature.names)}
    label2id_harem = {name: i for i, name in enumerate(harem_ner_feature.names)} # Útil para referência
    NUM_NER_TAGS = len(id2label_harem)

    print(f"Mapeamento de Labels HAREM (id2label, primeiros 5 de {NUM_NER_TAGS}): {list(id2label_harem.items())[:5]}")

    harem_train_full_hf = harem_raw["train"] # HAREM selective só tem split 'train'
    
    # Aplicar HAREM_SUBSET_SIZE
    if HAREM_SUBSET_SIZE is not None and len(harem_train_full_hf) > HAREM_SUBSET_SIZE:
        harem_train_hf_subset = harem_train_full_hf.select(range(HAREM_SUBSET_SIZE))
    else:
        harem_train_hf_subset = harem_train_full_hf # Usa o que foi carregado (todo o 'selective train')
    
    print(f"DEBUG HAREM: Tamanho do subset de treino HAREM para processamento: {len(harem_train_hf_subset)}")

    # Para HAREM, 'selective' não tem um split de validação dedicado.
    # Vamos criar um a partir do conjunto de treino para demonstração.
    # Em um cenário real, você poderia usar o dataset 'total' e seus splits, ou um split de teste dedicado se disponível.
    if len(harem_train_hf_subset) > 10: # Precisa de exemplos suficientes para dividir
        train_val_harem_split = harem_train_hf_subset.train_test_split(test_size=0.2, shuffle=True, seed=42)
        ft_harem_train_dataset_hf = train_val_harem_split['train']
        ft_harem_val_dataset_hf = train_val_harem_split['test'] # Usando 'test' como validação
        print(f"HAREM treino dividido em: {len(ft_harem_train_dataset_hf)} para treino, {len(ft_harem_val_dataset_hf)} para validação.")
    else:
        ft_harem_train_dataset_hf = harem_train_hf_subset # Usa tudo para treino se for muito pequeno
        ft_harem_val_dataset_hf = None
        print("HAREM subset muito pequeno para divisão treino/validação. Usando tudo para treino.")

    ft_harem_train_dataset = HAREMNERDataset(ft_harem_train_dataset_hf, tokenizer, label2id_harem, id2label_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
    
    if len(ft_harem_train_dataset) == 0: # Checagem importante
        raise ValueError("Dataset de treino HAREM (ft_harem_train_dataset) está vazio após processamento. Verifique HAREMNERDataset.")
        
    ft_harem_train_loader = DataLoader(ft_harem_train_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=True, num_workers=0)
    
    ft_harem_val_loader = None
    if ft_harem_val_dataset_hf and len(ft_harem_val_dataset_hf) > 0:
        ft_harem_val_dataset = HAREMNERDataset(ft_harem_val_dataset_hf, tokenizer, label2id_harem, id2label_harem, MAX_LEN, PAD_TOKEN_LABEL_ID_NER)
        if len(ft_harem_val_dataset) > 0:
            ft_harem_val_loader = DataLoader(ft_harem_val_dataset, batch_size=FINETUNE_BATCH_SIZE, shuffle=False, num_workers=0)
        else:
            print("AVISO: Dataset de validação HAREM ficou vazio após processamento na classe HAREMNERDataset.")
    else:
        print("Nenhum dataset de validação para HAREM será usado.")


    # Carregar backbone pré-treinado e criar modelo NER
    bert_base_for_ner = BERTBaseModel(
        vocab_size=tokenizer.get_vocab_size(), hidden_size=MODEL_HIDDEN_SIZE, num_layers=MODEL_NUM_LAYERS,
        num_heads=MODEL_NUM_ATTENTION_HEADS, # CORRIGIDO
        intermediate_size=MODEL_INTERMEDIATE_SIZE,
        max_len_config=MAX_LEN, pad_token_id=PAD_TOKEN_ID)

    if Path(PRETRAINED_BERTLM_SAVE_FILENAME).exists():
        print(f"Carregando state_dict do BERTLM de '{PRETRAINED_BERTLM_SAVE_FILENAME}'...")
        bertlm_state_dict = torch.load(PRETRAINED_BERTLM_SAVE_FILENAME, map_location=DEVICE)
        bert_base_state_dict = {k.replace("bert_base.", ""): v for k, v in bertlm_state_dict.items() if k.startswith("bert_base.")}
        if bert_base_state_dict: 
            bert_base_for_ner.load_state_dict(bert_base_state_dict)
            print("Pesos do backbone BERT carregados para NER.")
        else: 
            print("AVISO (NER): Nenhum peso para 'bert_base.' encontrado no arquivo salvo do pré-treino.")
    else: 
        print(f"AVISO (NER): Modelo pré-treinado '{PRETRAINED_BERTLM_SAVE_FILENAME}' não encontrado. Usando backbone com pesos aleatórios.")

    harem_ner_model = BERTForTokenClassification(bert_base_for_ner, MODEL_HIDDEN_SIZE, NUM_NER_TAGS).to(DEVICE)
    optimizer_ner = torch.optim.AdamW(harem_ner_model.parameters(), lr=FINETUNE_LR)
    # PAD_TOKEN_LABEL_ID_NER (-100) é o ignore_index para CrossEntropyLoss em tarefas de token classification
    criterion_ner = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_LABEL_ID_NER) 

    trainer_ner = GenericTrainer(
        model=harem_ner_model, train_dataloader=ft_harem_train_loader, val_dataloader=ft_harem_val_loader,
        optimizer=optimizer_ner, criterion=criterion_ner, device=DEVICE,
        model_save_path=HAREM_MODEL_SAVE_FILENAME, task_type="ner", 
        vocab_size=NUM_NER_TAGS, # Para NER, este é o número de tags NER (para o reshape do loss)
        id2label=id2label_harem # Passando o mapeamento id->string label para o trainer usar com seqeval
    )
    trainer_ner.train(num_epochs=FINETUNE_EPOCHS)

except Exception as e: 
    print(f"Erro no Bloco 11 (HAREM NER): {e}")
    import traceback
    traceback.print_exc()
    # Se o erro for de dataset não encontrado, pode ser necessário instalar dependências extras:
    # !pip install pyarrow # ou outras que o HAREM possa precisar
    raise

print("\n--- Script Completo com Fine-tuning e Avaliação (ASSIN2 NLI, HAREM NER) Finalizado ---")
